[
{"type":"header","version":"4.9.4","comment":"Export to JSON plugin for PHPMyAdmin"},
{"type":"database","name":"parafras_dataset_corpus"},
{"type":"table","name":"view_hasil_yoga","database":"parafras_dataset_corpus","data":
[
{"id_case":"1","class":"2","sentence1":"Most of existing lexical-semantic networks have been built by hand (like for instance WordNet ( Miller et al., 1990) ) and, despite that assisting tools are generally designed for consistency checking"},
{"id_case":"2","class":"2","sentence1":"( Petrov and McDonald, 2012 ), which includes the top ranked system, this indicates that self-training is already an established technique to improve the accuracy of constituency parsing on English."},
{"id_case":"3","class":"2","sentence1":"For example, Traxler et al. (2002) combine metonymic and non-metonymic verbs with entity-denoting and event-denoting nouns (The boy [started\/saw] V [the puzzle\/fight] N P ) and report significantly."},
{"id_case":"4","class":"3","sentence1":"Our system uses the architecture from ( Lee et al., 2016 ) where a character-level neural MT model maps the source character sequence to the target character sequence."},
{"id_case":"5","class":"2","sentence1":"Finite state morphology ( Beesley and Karttunen, 2003 ) is the state-of-the-art in writing morphological analysers for natural languages of the whole range of typologically varying morphological feature."},
{"id_case":"6","class":"2","sentence1":"A framework for human error analysis and error classification has been proposed in ( Vilar et al., 2006 ) and a detailed analysis of the obtained results has been carried out."},
{"id_case":"9","class":"3","sentence1":"Training is done with the Adam optimisation algorithm ( Kingma and Ba, 2014 ) with learning rate of 10 ?4 ."},
{"id_case":"10","class":"3","sentence1":"We use the Support Vector Machines implementation in the scikit-learn toolkit ( Pedregosa et al., 2011 ) to perform regression (SVR) on each feature set with either RBF kernels and parameters optimise"},
{"id_case":"12","class":"3","sentence1":"In order to assess the reliability of such results, we computed pairwise improvement intervals as described in ( Koehn, 2004 ), by means of bootstrapping with 1000 bootstrap iterations."},
{"id_case":"13","class":"3","sentence1":"It is used to support semantic analyses in HPSG English grammar ERG ( Copestake and Flickinger, 2000 ), but also in other grammar formalisms like LFG."},
{"id_case":"15","class":"3","sentence1":"Significance is marked withfor 95% confidence and for 99% confidence, and is measured with the bootstrap resampling method as described in ( Koehn, 2004 )."},
{"id_case":"16","class":"2","sentence1":"More recently, ( Pasha et al., 2014 ) created MADAMIRA, a system for morphological analysis and disambiguation of Arabic, this system can be used to improve the accuracy of spelling checking system."},
{"id_case":"17","class":"3","sentence1":"We optimize the feature weights using a modified version of averaged perceptron learning as described by Collins (2002) ."},
{"id_case":"18","class":"2","sentence1":"Another approach that uses the polarity of the local context for computing word polarity is the one presented by ( Popescu and Etzioni, 2005 ), who use a weighting function of the words around."},
{"id_case":"21","class":"3","sentence1":"It was built with the Moses toolkit ( Koehn et al., 2007 ) using the 14 standard core features including a 5-gram language model."},
{"id_case":"22","class":"0","sentence1":"The common phrasebased translation systems, such as ( Och et al., 1999; Koehn, 2004 ), do not use an explicit sentence length model."},
{"id_case":"23","class":"0","sentence1":"The common phrasebased translation systems, such as ( Och et al., 1999; Koehn, 2004 ), do not use an explicit sentence length model."},
{"id_case":"24","class":"3","sentence1":"After creating the training instances, we train a 14-class classifier on them using SVM multiclass ( Tsochantaridis et al., 2004 )."},
{"id_case":"25","class":"3","sentence1":"POS tagging was performed with TreeTagger ( Schmid, 1994 )."},
{"id_case":"26","class":"2","sentence1":"The first one is the WS- 353 3 dataset ( Finkelstein et al., 2001 ) containing 353 pairs of English words that have been assigned similarity ratings by humans."},
{"id_case":"27","class":"2","sentence1":"Web-based methods, instead, extract text snippets from Web pages which match pre-defined lexical patterns, such as X is a Y, along the lines of Hearst (1992) ."},
{"id_case":"28","class":"3","sentence1":"Given the semantic objects defined in the previous section, we design a convolution kernel in a way similar to the parse-tree kernel proposed in ( Collins and Duffy, 2002 )."},
{"id_case":"29","class":"2","sentence1":"The modeling of Process as a kind of event that is continuous and homogeneous in nature, follows the frame semantic analysis used for generating the FRAMENET data ( Baker et al., 1998 )."},
{"id_case":"31","class":"3","sentence1":"The SSN uses standard methods ( Bishop, 1995 ) to estimate a probability distribution over the set of possible next decisions d i given these representations ."},
{"id_case":"32","class":"2","sentence1":"Recently, there has been a successful attempt to harmonize the linguistic principles behind the coding systems MSD and KR ( Farkas et al., 2010 )."},
{"id_case":"33","class":"2","sentence1":"The second algorithm, denoted GloTr, is the Chu-Liu- Edmonds algorithm for maximal spanning tree implemented in the MSTParser ( McDonald, 2006 )."},
{"id_case":"34","class":"2","sentence1":"The first one is the WS-353 dataset ( Finkelstein et al., 2001 ) containing 353 pairs of English words that have been assigned similarity ratings by humans."},
{"id_case":"35","class":"2","sentence1":"Abstract Meaning Representation (AMR) ( Banarescu et al., 2013 ) is a semantic formalism where the meaning of a sentence is encoded as a rooted, directed graph."},
{"id_case":"36","class":"3","sentence1":"We perform bootstrap resampling with bounds estimation as described in ( Koehn, 2004 )."},
{"id_case":"37","class":"3","sentence1":"It is used to support semantic analyses in HPSG English Resource  grammar ERG ( Copestake and Flickinger, 2000 ), but also in other grammar formalisms like LFG."},
{"id_case":"38","class":"3","sentence1":"It is used to support semantic analyses in the English HPSG grammar ERG ( Copestake and Flickinger, 2000 ), but also in other grammar formalisms like LFG."},
{"id_case":"39","class":"3","sentence1":"It is used to support semantic analyses in HPSG English grammar ERG ( Copestake and Flickinger, 2000 ), but also in other grammar formalisms like LFG."},
{"id_case":"40","class":"3","sentence1":"It is used to support semantic analyses in the English HPSG grammar ERG ( Copestake and Flickinger, 2000 ), but also in other grammar formalisms like LFG."},
{"id_case":"41","class":"0","sentence1":"Table 2 : The size of annotated data from CoNLL ( Buchholz and Marsi, 2006 ), and the number of tags included and missing for 8 languages ."},
{"id_case":"43","class":"3","sentence1":"We build upon our previous Markov Logic based approach for joint concept disambiguation and clustering ( Fahrni and Strube, 2012 )."},
{"id_case":"44","class":"2","sentence1":"Details about SVM and KFD can be found in ( Taylor and Cristianini, 2004 )."},
{"id_case":"45","class":"1","sentence1":"Therefore, POS taggers for English tweets have been developed such as ARK, T-Pos and GATE TwitIE which reaches 92.8%, 88.4% and 89.37% accuracy respectively ( Derczynski et al., 2013 )."},
{"id_case":"46","class":"3","sentence1":"We learn the parameters using a quasi-Newton procedure with L 1 (lasso) regularization ( Andrew and Gao, 2007 )."},
{"id_case":"47","class":"3","sentence1":"We use the SCFG decoder cdec ( Dyer et al., 2010 ) 4 and build grammars using its implementation of the suffix array extraction method described in Lopez (2007)."},
{"id_case":"48","class":"2","sentence1":"This is known as the Distributional Hypothesis ( Harris, 1968 )."},
{"id_case":"49","class":"3","sentence1":"All our models , as well as the parser described in ( Henderson, 2003 ), are run only once."},
{"id_case":"50","class":"2","sentence1":"Word alignment is performed by GIZA++ ( Och and Ney, 2000 ) in both directions with the default setting."},
{"id_case":"51","class":"2","sentence1":"For strings, many such kernel functions exist with various applications in computational biology and computational linguistics ( Taylor and Cristianini, 2004 )."},
{"id_case":"52","class":"2","sentence1":"is an example of SRL annotation from the Prop- Bank corpus ( Palmer et al., 2005 ), where the subscripted information maps the semantic roles A0, A1 and A2 to arguments for the predicate sell as def"},
{"id_case":"53","class":"2","sentence1":"The estimation of the semantically Smoothed Partial Tree Kernel (SPTK) is made available by an extended version of SVM-LightTK software 5 ( Moschitti, 2006 )."},
{"id_case":"54","class":"2","sentence1":"In the i2b2 2012 temporal challenge, all top performing teams used a combination of supervised classification and rule-based methods for extracting temporal information and relations ( Sun et al., 2013 )."},
{"id_case":"55","class":"3","sentence1":"We train with the Adam optimizer ( Kingma and Ba, 2015 ), a learning rate of 0.0001, batch size of 50, and dropout with probability 0.2 applied to the hidden layer."},
{"id_case":"56","class":"3","sentence1":"In our experimental study, we use the freely available implementations in Weka ( Witten and Frank, 2005 )."},
{"id_case":"57","class":"3","sentence1":"The phrase translation probabilities are smoothed with Good-Turing smoothing ( Foster et al., 2006 )."},
{"id_case":"58","class":"2","sentence1":"More recently, ( Carpineto and Romano, 2010 ) showed that the characteristics of the outputs returned by SRC algorithms suggest the adoption of a meta clustering approach."},
{"id_case":"59","class":"3","sentence1":"They are based on the distributional hypothesis ( Harris, 1968 ) and by looking at a set of event expressions whose argument fillers have a similar distribution, they try to recognize synonymous event."},
{"id_case":"60","class":"2","sentence1":"Word alignment is performed using GIZA++ ( Och and Ney, 2003 )."},
{"id_case":"61","class":"2","sentence1":"Word alignment is performed using GIZA++ ( Och and Ney, 2003 )."},
{"id_case":"62","class":"3","sentence1":"We used Mallet software ( McCallum, 2002 ) for CRF experiments."},
{"id_case":"63","class":"2","sentence1":"The thesaurus consists of a hierarchy of 2,710 semantic classes, defined for over 264,312 nouns, with a maximum depth of twelve ( Ikehara et al., 1997 )."},
{"id_case":"64","class":"2","sentence1":"The detailed discussion is provided in the longer version of the paper ( Kim et al., 2013 )."},
{"id_case":"65","class":"2","sentence1":"can be evaluated by maximizing the pseudo-likelihood on a training corpus see ( Malouf, 2002 )."},
{"id_case":"66","class":"2","sentence1":"The first one is the WS- 353 3 dataset ( Finkelstein et al., 2001 ) containing 353 pairs of English words that have been assigned similarity ratings by humans."},
{"id_case":"67","class":"3","sentence1":"Following Blitzer et al. (2006) , we consider pivot features that appear more than 50 times in all the domains for SCL and mDA."},
{"id_case":"68","class":"3","sentence1":"A framework for human error analysis and error classification has been proposed in ( Vilar et al., 2006 ), but like human evaluation , this is also a time consuming task."},
{"id_case":"69","class":"3","sentence1":"MaxEnt classifier is a good example of this group ( Mani et al., 2006 )."},
{"id_case":"70","class":"2","sentence1":"Among these media, blog is one of the communicative and informative repository of text based emotional contents in the Web 2.0 ( Lin et al., 2007 )."},
{"id_case":"71","class":"3","sentence1":"For the gold preprocessing and all 5k settings, we refer the reader to the Shared Task overview paper ( Seddah et al., 2013 )."},
{"id_case":"73","class":"3","sentence1":"For preprocessing, we used MADA (Morphological Analysis and Disambiguation for Arabic) ( Habash et al., 2009 ) which is one of the most accurate Arabic preprocessing toolkits."},
{"id_case":"74","class":"3","sentence1":"We trained a 5-gram language model on the Xinhua section of the English Gigaword corpus (306 million words) using the SRILM toolkit ( Stolcke, 2002) with the modified Kneser-Ney smoothing."},
{"id_case":"75","class":"3","sentence1":"To determine semantic type and subtype, we train two SVM multiclass classifiers using SVM multiclass ( Tsochantaridis et al., 2004 )."},
{"id_case":"76","class":"3","sentence1":"We use the scikit implementation of Random Forest ( Pedregosa et al., 2011 )."},
{"id_case":"78","class":"3","sentence1":"Each term in the input text will be represented by its stem and POS tag, in the following format (stem:POS) using Buckwalter transliteration ( Buckwalter, 2002 )."},
{"id_case":"79","class":"3","sentence1":"An algorithm, the Kuhn-Munkres method ( Kuhn, 1955 ), can find solutions to the optimum assignment problem in polynomial time."},
{"id_case":"81","class":"3","sentence1":"We use Collapsed Gibbs Sampling ( Griffiths and Steyvers, 2004 ) to infer the parameters of the model and the latent violent categories and topics assignments for tweets, given observed data D. Gibbs."},
{"id_case":"82","class":"3","sentence1":"We use the Stanford dependency parser ( Marneffe et al., 2006 )."},
{"id_case":"83","class":"3","sentence1":"Filter weights are initialized using Glorot-Bengio strategy ( Glorot and Bengio, 2010 )."},
{"id_case":"85","class":"2","sentence1":"Automatic sentence alignment of the training data was provided by Ulrich German, and the hand alignments of the words in the test data were created by Franz Och and Hermann Ney ( Och and Ney, 2003 )."},
{"id_case":"86","class":"3","sentence1":"We use the AdaGrad optimizer ( Duchi et al., 2011)  with initial learning rate set to 0.1."},
{"id_case":"87","class":"3","sentence1":"Then we did word alignment using GIZA++ ( Och and Ney, 2003 ) with the default grow-diagfinal-and alignment symmetrization method."},
{"id_case":"88","class":"3","sentence1":"For example, DIRT ( Lin and Pantel, 2001 ) aims to discover different representations of the same semantic relation, i.e. similar dependency paths."},
{"id_case":"89","class":"3","sentence1":"The annotation was performed manually using the brat annotation tool ( Stenetorp et al., 2012 )."},
{"id_case":"90","class":"2","sentence1":"As explained in more detail in ( Henderson, 2003b )."},
{"id_case":"91","class":"2","sentence1":"System proposed by ( Li et al., 2006 ), uses a semantic-vector approach to measure sentence similarity."},
{"id_case":"93","class":"2","sentence1":"All modules take as input the corpus documents preprocessed with a part-of-speech tagger 4 and shallow parser 5 ( Punyakanok and Roth, 2001 )."},
{"id_case":"94","class":"1","sentence1":"It is worth noting that even if the above equations define a kernel function similar to the one proposed in ( Collins and Duffy, 2002 )"},
{"id_case":"95","class":"3","sentence1":"From the pioneering work of ( Rapp, 1995 ), contextual similarity has been used for BLE for a long time."},
{"id_case":"96","class":"2","sentence1":"Europarl ( Koehn, 2005 ) is a multilingual parallel corpus extracted from the proceedings of the European Parliament."},
{"id_case":"97","class":"2","sentence1":"The four test sets are: WS-353 ( Finkelstein et al., 2002 ) is a set of 353 word pairs."},
{"id_case":"98","class":"3","sentence1":"We use the F1 measure according to the Parseval metric ( Black et al., 1991 ) for the evaluation."},
{"id_case":"99","class":"2","sentence1":"TESLA (Translation Evaluation of Sentences with Linear-programming-based Analysis) was first proposed in Liu et al. (2010) ."},
{"id_case":"100","class":"3","sentence1":"For building the baseline SMT system, we used the open-source SMT toolkit Moses ( Koehn et al., 2007 ), in its standard setup."},
{"id_case":"102","class":"2","sentence1":"Rhetorical Structure Theory (RST) ( Mann and Thompson, 1988 ), one of the most influential theories of discourse, represents texts by labeled hierarchical structures, called Discourse Trees (DTs)."},
{"id_case":"103","class":"3","sentence1":"In addition, the fix-discount method in ( Foster et al., 2006 ) for phrase table smoothing is also used."},
{"id_case":"104","class":"2","sentence1":"Memory-based language processing (Daelemans and van den Bosch, 2005 ) is based on the idea that NLP problems can be solved by reuse of solved examples of the problem stored in memory."},
{"id_case":"105","class":"3","sentence1":"We calculate statistical significance of performance differences using stratified shuffling ( Yeh, 2000 )."},
{"id_case":"106","class":"2","sentence1":"For instance, machine translation (MT) systems can benefit from training on sentences extracted from parallel or comparable documents retrieved from the Web ( Munteanu and Marcu, 2005 )."},
{"id_case":"108","class":"2","sentence1":"In PB-SMT, the posterior probability P(e I 1 |f J 1 ) is directly modelled as a (log-linear) combination of features ( Och and Ney, 2002 ), that usually comprise M translational features"},
{"id_case":"109","class":"3","sentence1":"1 with 2 -regularization using AdaGrad ( Duchi et al., 2011 )."},
{"id_case":"111","class":"1","sentence1":"Why does the lr model outperform Berkeley 13 The MUC ( Vilain et al., 1995 ) score is the minimum number of links between mentions to be inserted or deleted when mapping the output to a gold standard."},
{"id_case":"112","class":"3","sentence1":"In the context of this paper we will be focusing on the SubSet Tree (SST) kernel described in ( Collins and Duffy, 2002 ), which relies on a fragment definition that does not allow to break production."},
{"id_case":"113","class":"2","sentence1":"Europarl ( Koehn, 2005 ) is a multilingual parallel corpus extracted from the proceedings of the European Parliament."},
{"id_case":"114","class":"3","sentence1":"We have used Foma, a free software tool to specify finite-state automata and transducers ( Hulden, 2009 )."},
{"id_case":"116","class":"3","sentence1":"We used the same test set used in Li et al. (2004) for our testing 5 ."},
{"id_case":"117","class":"2","sentence1":"The Penn Discourse Treebank (PDTB, Prasad et al., 2008 ) is a large corpus annotated with discourse relations, covering the Wall Street Journal part of the Penn Treebank."},
{"id_case":"118","class":"3","sentence1":"We used the implementation of the scikit-learn 2 module ( Pedregosa et al., 2011 )."},
{"id_case":"119","class":"3","sentence1":"Since the commonly used word similarity datasets contain a small number of word pairs we also use the MEN dataset ( Bruni et al., 2012 ) of 3,000 word pairs sampled from words that occur at least 700 times in a large web corpus."},
{"id_case":"120","class":"2","sentence1":"The training data of the shared task is the NUCLE corpus ( Dahlmeier et al., 2013 ), which contains essays written by learners of English ."},
{"id_case":"121","class":"2","sentence1":"RTE is instead the task of deciding whether a long text T entails a shorter text, typically a single sentence, called hypothesis H. It has been often seen as a classification task  ( Dagan et al., 2013 )."},
{"id_case":"122","class":"2","sentence1":"All system implementation was done using Python and the open-source machine learning toolkit scikit-learn ( Pedregosa et al., 2011 )."},
{"id_case":"123","class":"2","sentence1":"It has been shown that a diverse set of predictions can be used to help improve decoder accuracy for various problems in NLP ( Henderson and Brill, 1999 )."},
{"id_case":"124","class":"3","sentence1":"In the 2013 system, we had used SentiStrength lexicon ( Thelwall et al., 2010 )."},
{"id_case":"125","class":"1","sentence1":"Finally, we also compare the quality of the candidate phrase embeddings with word embeddings ( Dhillon et al., 2011 ) by adding them as features in a CRF based sequence tagger."},
{"id_case":"126","class":"2","sentence1":"These methods are based on the distributional hypothesis which states that words appearing in the same contexts tend to have similar meaning ( Harris, 1954 )."},
{"id_case":"127","class":"2","sentence1":"All annotations were done using the BRAT rapid annotation tool ( Stenetorp et al., 2012 )."},
{"id_case":"128","class":"0","sentence1":"Compared to WordNet (Fellbaum, 1998 ), there are similarities but also significant differences."},
{"id_case":"131","class":"3","sentence1":"For training, we use Adam ( Kingma and Ba, 2015 ) for optimization with an initial learning rate of 0.001."},
{"id_case":"132","class":"3","sentence1":"For our classifier, we use SVMs, specifically the LIBLINEAR SVM software package ( Fan et al., 2008 ), which is well-suited to text classification tasks with large numbers of features and large number"},
{"id_case":"133","class":"2","sentence1":"The first two experiments concern the prediction of the sentiment of movie reviews in the Stanford Sentiment Treebank ( Socher et al., 2013)."},
{"id_case":"134","class":"2","sentence1":"Concept-to-text generation broadly refers to the task of automatically producing textual output from nonlinguistic input ( Reiter and Dale, 2000 )."},
{"id_case":"136","class":"3","sentence1":"We used only the non-ensembled left-to-right run (i.e. no right-to-left rescoring as done by Sennrich et al., 2016a ) with beam size of 5, 5 taking just the single-best output."},
{"id_case":"137","class":"3","sentence1":"We used only the non-ensembled left-to-right run (Sennrich et al., 2016a ) with beam size of 5, 5 taking just the single-best output."},
{"id_case":"138","class":"2","sentence1":"The MSD morphological coding system was developed for a bunch of languages including Hungarian ( Erjavec, 2004 )."},
{"id_case":"139","class":"2","sentence1":"The phrase tables were generated by means of symmetrised word alignments obtained with GIZA++ ( Och and Ney, 2003 )."},
{"id_case":"140","class":"2","sentence1":"Word alignment is performed using GIZA++ ( Och and Ney, 2003 )."},
{"id_case":"141","class":"3","sentence1":"We experimented with several levels of cluster granularity using development data, and following Koo et al. (2008)."},
{"id_case":"143","class":"2","sentence1":"(Raghavan et al. (2007) ) measure the benefit from feature feedback as the gain in the learning speed with feature feedback."},
{"id_case":"144","class":"3","sentence1":"We built a modified Kneser-Ney smoothed 5-gram language model using the English side of the training data and performed querying with KenLM ( Heafield, 2011 )."},
{"id_case":"145","class":"2","sentence1":"( Kneser and Ney, 1995 )."},
{"id_case":"146","class":"2","sentence1":"A formal PAC-style analysis can be found in ( Ando and Zhang, 2004 )."},
{"id_case":"147","class":"3","sentence1":"The first model we introduce is based on the recurrent neural network language model of Mikolov et al. (2010) ."},
{"id_case":"148","class":"2","sentence1":"The SMT systems were built using the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"149","class":"2","sentence1":"The classifier experiments were carried out using the SVM-light software ( Joachims, 1999 ) available at http:\/\/svmlight.joachims.org\/ with a polynomial kernel 2 (degree=3)."},
{"id_case":"150","class":"2","sentence1":"The training data of the shared task is the NUCLE corpus ( Dahlmeier et al., 2013 ), which contains essays written by learners of English."},
{"id_case":"151","class":"2","sentence1":"SALDO ( Borin et al., 2013 ) is the largest freely available lexical resource for Swedish."},
{"id_case":"152","class":"3","sentence1":"For the phrase-based SMT system, we adopted the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"153","class":"2","sentence1":"However, those string-to-tree systems run slowly in cubic time ( Huang et al., 2006 )."},
{"id_case":"154","class":"2","sentence1":"The 5-gram target language model was trained using KenLM ( Heafield, 2011 )."},
{"id_case":"155","class":"3","sentence1":"For all experiments, we used the Moses SMT system ( Koehn et al., 2007 )."},
{"id_case":"156","class":"3","sentence1":"We evaluate our method on the following data sets: @BULLET OntoNotes-Dev: Development set of the OntoNotes data provided by the CoNLL2012 shared task ( Pradhan et al., 2012 )."},
{"id_case":"157","class":"3","sentence1":"We use the Moses phrase-based translation system ( Koehn et al., 2007 ) to implement our models."},
{"id_case":"158","class":"3","sentence1":"But we randomly selected 90% of the training data used in Li et al. (2004) as our training data and the remainder as the development data, as shown in Table 5 ."},
{"id_case":"159","class":"0","sentence1":"The BLEU score measures the precision of n-grams (over all n to 4 in our case) with respect to a reference translation with a penalty for short translations ( Papineni et al., 2001 )."},
{"id_case":"160","class":"2","sentence1":"The training data of the shared task is the NUCLE corpus ( Dahlmeier et al., 2013 ), which contains essays written by learners of English."},
{"id_case":"161","class":"2","sentence1":"Test data was drawn from the Open American National Corpus ( Ide and Suderman, 2004 , OANC) across a variety of genres and from both the spoken and written portions of the corpus."},
{"id_case":"162","class":"3","sentence1":"We measure statistical significance using 95% confidence intervals computed with paired bootstrap resampling ( Koehn, 2004 )."},
{"id_case":"163","class":"3","sentence1":"We use the English portion of the ACE 2005 relation extraction dataset ( Walker et al., 2006 )."},
{"id_case":"164","class":"2","sentence1":"This data was collected for the 2014 SemEval competition ( Marelli et al., 2014 ) and consists of 9,927 sentence pairs, with 4,500 for training, 500 as a development set."},
{"id_case":"165","class":"2","sentence1":"The parsing model used for intra-sentential parsing is a Dynamic Conditional Random Field (DCRF) ( Sutton et al., 2007 ) shown in Figure 7 ."},
{"id_case":"166","class":"2","sentence1":"Latent Dirichlet Allocation (LDA) is a generative model which considers a document model ( Salton, 1989 ) as a mixture probability of latent topics ."},
{"id_case":"167","class":"3","sentence1":"Distributional hypothesis theory ( Harris, 1954 ) indicates that words that occur in the same context tend to have similar meanings."},
{"id_case":"168","class":"3","sentence1":"Distributional hypothesis theory ( Harris, 1954 ) indicates that words that occur in the same context tend to have similar meanings."},
{"id_case":"169","class":"2","sentence1":"In 2009, Yefang Wang ( Wang et al., 2009 ) used cascading classifiers on manually annotated data which fetched F-score of 0.832."},
{"id_case":"170","class":"3","sentence1":"We built a 5-gram language model on the English side of QCA-train using KenLM ( Heafield, 2011 )."},
{"id_case":"171","class":"2","sentence1":"The remaining three models are all Naive Bayes classifiers trained on the Google Web 1T 5-gram corpus (henceforth, Google corpus , ( Brants and Franz, 2006 ))."},
{"id_case":"172","class":"3","sentence1":"We apply bootstrapping ( Kozareva et al., 2008 ) on the word graphs by manually selecting 10 seeds for concrete and abstract words (see Table 10 )."},
{"id_case":"173","class":"2","sentence1":"These methods are based on the distributional hypothesis which states that words appearing in the same contexts tend to have similar meaning ( Harris, 1954 )."},
{"id_case":"174","class":"3","sentence1":"These results verify the benefit of using LTAG based features and confirm the hypothesis that LTAG based features provide a novel set of abstract features that complement the hand selected features ( Collins, 2000 )."},
{"id_case":"175","class":"2","sentence1":"RG-65: ( Rubenstein and Goodenough, 1965 ) has 65 word pairs."},
{"id_case":"176","class":"3","sentence1":"The significance tests were performed using the bootstrap resampling method ( Koehn, 2004 )."},
{"id_case":"177","class":"2","sentence1":"The default Phrasal search algorithm is cube pruning ( Huang and Chiang, 2007 )."},
{"id_case":"178","class":"2","sentence1":"In-domain data is mainly used to solve the problem of data sparseness ( Sun and Xu, 2011 )."},
{"id_case":"179","class":"3","sentence1":"All experiments were carried out using the open-source SMT toolkit Moses ( Koehn et al., 2007 )."},
{"id_case":"180","class":"3","sentence1":"First, we apply heuristics to determine number and gender based on word lists, WordNet ( Miller, 1990 ) and part-of-speech tags."},
{"id_case":"181","class":"3","sentence1":"Rank SVM ( Joachims, 2002 ) is a method based on Support Vector Machines (SVMs) for which we use only linear kernels to keep complexity low."},
{"id_case":"182","class":"2","sentence1":"A more detailed description of the task can be found in ( Nakov et al., 2017 )."},
{"id_case":"184","class":"2","sentence1":"A Tree Kernel function is a convolution kernel ( Haussler, 1999 ) defined over pairs of trees."},
{"id_case":"185","class":"3","sentence1":"We build upon our previous approach for joint concept disambiguation and clustering ( Fahrni and Strube, 2012 )."},
{"id_case":"186","class":"2","sentence1":"ROUGE-2 metric ( Lin, 2004 ) is used for the evaluation ."},
{"id_case":"187","class":"3","sentence1":"We used Mallet software ( McCallum, 2002 ) for CRF experiments."},
{"id_case":"188","class":"3","sentence1":"We exploit a transition-based framework with global learning and beam-search decoding to implement the joint model ( Zhang and Clark, 2011 )."},
{"id_case":"189","class":"2","sentence1":"The annotation was performed using the BRAT 2 tool ( Stenetorp et al., 2012 )."},
{"id_case":"190","class":"3","sentence1":"For building the word alignment models we use MGIZA++ ( Gao and Vogel, 2008 )."},
{"id_case":"191","class":"2","sentence1":"The reliability of the annotation was evaluated using the kappa statistic ( Carletta, 1996 )."},
{"id_case":"192","class":"3","sentence1":"It therefore follows the distributional hypothesis ( Harris, 1954 ) which states that words that occur in the same contexts tend to have similar meanings."},
{"id_case":"194","class":"2","sentence1":"The significance tests were performed using the bootstrap resampling method ( Koehn, 2004 )."},
{"id_case":"195","class":"3","sentence1":"We use the standard alignment tool Giza++ ( Och and Ney, 2003 ) to word align the parallel data."},
{"id_case":"196","class":"3","sentence1":"We use the standard alignment tool Giza++ ( Och and Ney, 2003 ) to word align the parallel data."},
{"id_case":"198","class":"2","sentence1":"The kernels are combined using Gaussian process regression (GPR) ( Rasmussen and Williams, 2006 )."},
{"id_case":"200","class":"3","sentence1":"We measure statistical significance using 95% confidence intervals computed with paired bootstrap resampling ( Koehn, 2004 )."},
{"id_case":"701","class":"3","sentence1":"We use the Stanford dependency parser ( Chen and Manning, 2014 ) at this stage, and have not experimented with alternatives ."},
{"id_case":"702","class":"2","sentence1":"Next, a tweet was tokenized and fed into MADAMIRA ( Pasha et al., 2014 ), a morphological analysis tool for Arabic text."},
{"id_case":"703","class":"2","sentence1":"(Yarowsky ,(1995) ) has proposed a bootstrapping method for word sense disambiguation ."},
{"id_case":"704","class":"3","sentence1":"We also list the previous state-of-the-art performance from a conventional SMT system ( Durrani et al., 2014) with the BLEU of 37.0."},
{"id_case":"705","class":"3","sentence1":"We used Adam ( Kingma and Ba, 2014 ) with a learning rate of 0.0002."},
{"id_case":"706","class":"2","sentence1":"Distributional semantics is based on the idea that ( Firth, 1957) in other words, the meaning of a word is related to the contexts it appears in."},
{"id_case":"707","class":"2","sentence1":"In order to estimate the basic lexical similarity function employed in the SUM, SSC and SPTK operators, a co-occurrence Word Space is acquired through the distributional analysis of the UkWaC corpus (Baroni et al., 2009 )."},
{"id_case":"709","class":"3","sentence1":"We used the implementation of the scikit-learn 2 module ( Pedregosa et al., 2011 )."},
{"id_case":"710","class":"2","sentence1":"The translation model was trained by GIZA++ ( Och and Ney, 2003 ), and the trigram was trained by the CMU-Cambridge Statistical Language Modeling Toolkit v2 ( Clarkson and Rosenfeld, 1997 )."},
{"id_case":"711","class":"2","sentence1":"This is a generalization of the operator Id in ( Kaplan and Kay, 1994 )."},
{"id_case":"712","class":"3","sentence1":"We used standard classifiers available in scikit-learn package ( Pedregosa et al., 2011 )."},
{"id_case":"714","class":"3","sentence1":"We used the implementation of the scikit-learn 2 module ( Pedregosa et al., 2011 )."},
{"id_case":"715","class":"3","sentence1":"For the phrase-based SMT system, we adopted the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"716","class":"2","sentence1":"4 Word alignments are created by aligning the data in both directions with GIZA++ 5 and symmetrizing the two trained alignments ( Och and Ney, 2003 )."},
{"id_case":"717","class":"0","sentence1":"The perplexity achieved by the 6- gram NN LM in the Spanish news-test08 development set was 116, versus 94 obtained with a standard 6-gram language model with interpolation and Kneser-Ney smoothing ( Kneser and Ney, 1995 )."},
{"id_case":"718","class":"3","sentence1":"We used standard classifiers available in scikit-learn package ( Pedregosa et al., 2011 )."},
{"id_case":"719","class":"2","sentence1":"Statistical machine translation is typically performed using phrase-based systems ( Koehn et al., 2007 )."},
{"id_case":"720","class":"2","sentence1":"WordNet ( Miller et al., 1990 ) is an on-line hierarchical lexical database which contains semantic information about English words."},
{"id_case":"721","class":"3","sentence1":"We use the scikit implementation of Random Forest ( Pedregosa et al., 2011 )."},
{"id_case":"724","class":"3","sentence1":"We lemmatise the head of each constituent with TreeTagger ( Schmid, 1994 )."},
{"id_case":"725","class":"3","sentence1":"Our text processing uses the Natural Language Toolkit (NLTK) ( Bird et al., 2009 )."},
{"id_case":"726","class":"2","sentence1":"They used the Web-based annotation tool brat ( Stenetorp et al., 2012 ) for the annotation ."},
{"id_case":"728","class":"1","sentence1":"The log-likelihood ratio test ( Lin and Hovy, 2000 ) compares the distribution of a word in the input with that in a large background corpus to identify topic words."},
{"id_case":"729","class":"2","sentence1":"Our system participated in SemEval-2013 Task 2: Sentiment Analysis in Twitter ( Wilson et al., 2013 )."},
{"id_case":"730","class":"2","sentence1":"The English text was tokenized using the word tokenize routine from NLTK ( Bird et al., 2009 )."},
{"id_case":"731","class":"2","sentence1":"The webpages were parsed using the Stanford CoreNLP software ( Manning et al., 2014 )."},
{"id_case":"732","class":"2","sentence1":"Statistical machine translation is typically performed using phrase-based systems ( Koehn et al., 2007 )."},
{"id_case":"733","class":"2","sentence1":"The English side was tokenized using the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"734","class":"3","sentence1":"We trained an English 5-gram language model using KenLM ( Heafield, 2011 )."},
{"id_case":"735","class":"2","sentence1":"All of the text data from Reddit was tokenized using the NLTK tokenizer ( Bird et al., 2009 )."},
{"id_case":"736","class":"3","sentence1":"Our machine translation systems are trained using Moses 3 ( Koehn et al., 2007 )."},
{"id_case":"737","class":"3","sentence1":"For the phrase-based SMT system, we adopted the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"738","class":"1","sentence1":"We build upon our previous approach for joint concept disambiguation and clustering ( Fahrni and Strube, 2012 )."},
{"id_case":"739","class":"3","sentence1":"We used Adam ( Kingma and Ba, 2014 ) with a learning rate of 0.0002 ()."},
{"id_case":"740","class":"3","sentence1":"For all experiments, we used the Moses SMT system ( Koehn et al., 2007 )."},
{"id_case":"741","class":"2","sentence1":"The SMT systems were built using the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"742","class":"3","sentence1":"For training the translation model and for decoding we used the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"743","class":"3","sentence1":"For training the translation model and for decoding we used the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"744","class":"3","sentence1":"For the phrase-based SMT system, we adopted the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"745","class":"3","sentence1":"We develop translation models using the phrase-based Moses ( Koehn et al., 2007 ) SMT system."},
{"id_case":"746","class":"3","sentence1":"We used TnT ( Brants, 2000 ), trained on the Negra training set."},
{"id_case":"747","class":"2","sentence1":"The webpages were parsed using the Stanford CoreNLP software ( Manning et al., 2014 )."},
{"id_case":"748","class":"3","sentence1":"We develop translation models using the phrase-based Moses ( Koehn et al., 2007 ) SMT system."},
{"id_case":"749","class":"1","sentence1":"POS Features: ( Pak and Paroubek, 2010 ) found that subjective texts often contain more adjectives or adverbs and less nouns than objective texts."},
{"id_case":"750","class":"1","sentence1":"The term frequency count is normalized with the inverse document frequency in the test collection ( Salton and Buckley, 1988 )."},
{"id_case":"751","class":"1","sentence1":"We assessed the statistical significance of differences in score with an approximate randomization test 8 ( Noreen, 1989 ), indicating a significant impact in bold font."},
{"id_case":"752","class":"2","sentence1":"A framework for human error analysis and error classification has been proposed in ( Vilar et al., 2006 ) and a detailed analysis of the obtained results has been carried out."},
{"id_case":"753","class":"0","sentence1":"For example, Chang et al. (2009) found that the probability of held-out documents is not always a good predictor of human judgments."},
{"id_case":"754","class":"3","sentence1":"On the Chinese side, we used the morphological analyzer described in ( Kruengkrai et al., 2009 ) trained on the training data of CTB tp to perform word segmentation and POS tagging and used the first"},
{"id_case":"755","class":"2","sentence1":"conducted using the Moses phrase-based decoder ( Koehn et al., 2007 )."},
{"id_case":"756","class":"2","sentence1":"conducted using the Moses phrase-based decoder ( Koehn et al., 2007 )."},
{"id_case":"757","class":"3","sentence1":"We conducted baseline experiments for phrasebased machine translation using the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"759","class":"2","sentence1":"Then the processed data was performed for tokenization, POS tagging, parsing, stemming and lemmatization using Stanford CoreNLP ( Manning et al., 2014 )."},
{"id_case":"760","class":"3","sentence1":"We applied bootstrap resampling ( Koehn, 2004 ) to measure statistical significance , p < 0.05, of our models compared to a baseline."},
{"id_case":"761","class":"2","sentence1":"This system uses the attentional encoder-decoder architecture described by (Bahdanau et al. (2015)) , building on work by (Sutskever et al. (2014)) ."},
{"id_case":"762","class":"2","sentence1":"The language model is a 5-gram KenLM ( Heafield, 2011 ) model, trained using lmplz, with modified Kneser-Ney smoothing and no pruning."},
{"id_case":"763","class":"3","sentence1":"Weighted Finite State Transducers (FSTs) used in our model are constructed with OpenFst ( Allauzen et al., 2007 )."},
{"id_case":"764","class":"3","sentence1":"Weighted Finite State Transducers (FSTs) used in our model are constructed with OpenFst ( Allauzen et al., 2007 )."},
{"id_case":"765","class":"2","sentence1":"Then the processed data was performed for tokenization, POS tagging, parsing, stemming and lemmatization using Stanford CoreNLP ( Manning et al., 2014 )."},
{"id_case":"766","class":"2","sentence1":"It is a modification of the model proposed by Mintz et al. (2009) ."},
{"id_case":"767","class":"3","sentence1":"We use Scikit-learn ( Pedregosa et al., 2011 ), the machine learning library for Python, for implementing the different approaches."},
{"id_case":"768","class":"3","sentence1":"We use the Universal POS Tagset (UPOS) of Petrov et al. (2012) ."},
{"id_case":"770","class":"3","sentence1":"For language modeling, we trained a separate 5-gram Kneser-Ney smoothed LM model on the target (i.e. English) side of the training bi-text using KenLM ( Heafield, 2011 )."},
{"id_case":"771","class":"2","sentence1":"with the training script of the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"772","class":"2","sentence1":"The word alignment was trained using GIZA++ ( Och and Ney, 2003 ) with the configuration grow-diag-final-and alignment symmetrization method."},
{"id_case":"773","class":"3","sentence1":"We use linear SVMs from LIBLINEAR and SVMs with RBF kernel from LIBSVM ( Chang and Lin, 2011 )."},
{"id_case":"774","class":"2","sentence1":"We specify the hierarchical aligner in terms of a deduction system ( Shieber et al., 1995 )."},
{"id_case":"775","class":"3","sentence1":"We use Stanford parser ( de Marneffe et al., 2006 ) to obtain parse trees and dependency relations."},
{"id_case":"776","class":"3","sentence1":"In this work, we use the Stanford neural dependency parser ( Chen and Manning, 2014 )."},
{"id_case":"778","class":"3","sentence1":"We use Stanford parser ( de Marneffe et al., 2006 ) to obtain parse trees and dependency relations."},
{"id_case":"779","class":"3","sentence1":"The learning algorithm used in our coreference engine is C4.5 ( Quinlan, 1993 )."},
{"id_case":"780","class":"3","sentence1":"We use Stanford parser ( de Marneffe et al., 2006 ) to obtain parse trees and dependency relations."},
{"id_case":"781","class":"2","sentence1":"Distributional semantics (see Cohen and Widdows (2009) for an overview is based on the observation that words that occur in similar contexts tend to be semantically related ( Harris, 1954 )."},
{"id_case":"782","class":"2","sentence1":"Their work is part of the state-of-the-art Arabic morphological tagger MADAMIRA ( Pasha et al., 2014 )."},
{"id_case":"783","class":"3","sentence1":"We use Stanford parser ( de Marneffe et al., 2006 ) to obtain parse trees and dependency relations."},
{"id_case":"784","class":"2","sentence1":"Phrasal follows the log-linear approach to phrase-based translation ( Och and Ney, 2004 ) in which the decision rule has the familiar linear form = arg max e w ?(e, f ) (1)"},
{"id_case":"785","class":"2","sentence1":"Training data are based on a concatenation of 18 POS-tagged English corpora 2 from the CHILDES database ( MacWhinney, 2000 )."},
{"id_case":"786","class":"3","sentence1":"In-domain SMT: we used the parallel corpus (Section 3) to train an en-pt phrase-based SMT system using the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"787","class":"2","sentence1":"5-gram language models of Turkish and English were trained using KenLM ( Heafield, 2011 )."},
{"id_case":"788","class":"3","sentence1":"We used the relation classification dataset of the SemEval 2010 task 8 ( Hendrickx et al., 2010 )."},
{"id_case":"789","class":"3","sentence1":"We then run word alignment with GIZA++ ( Och and Ney, 2003 ) in both directions, with the default parameters used in Moses."},
{"id_case":"790","class":"3","sentence1":"In-domain SMT: we used the parallel corpus (Section 3) to train an en-pt phrase-based SMT system using the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"791","class":"2","sentence1":"The significance tests were performed using the bootstrap resampling method ( Koehn, 2004 )."},
{"id_case":"792","class":"2","sentence1":"All experiments were carried out using the open-source SMT toolkit Moses ( Koehn et al., 2007 ), in its standard non-monotonic configuration ."},
{"id_case":"793","class":"2","sentence1":"These sentences have then be fed into an efficient HPSG parser (PET; ( Callmeier, 2000 )) with ERG loaded."},
{"id_case":"794","class":"2","sentence1":"The language model is a 5-gram KenLM ( Heafield, 2011 ) model, trained using lmplz, with modified Kneser-Ney smoothing and no pruning."},
{"id_case":"795","class":"0","sentence1":"Cohen et al. (2012)  present a spectral algorithm for L-PCFG estimation, but the transformation of the L-PCFG model and its spectral algorithm to R-HMMs is awkward and opaque."},
{"id_case":"796","class":"2","sentence1":"Distributional hypothesis theory ( Harris, 1954 ) indicates that words that occur in the same context tend to have similar meanings."},
{"id_case":"797","class":"2","sentence1":"Distributional hypothesis theory ( Harris, 1954 ) indicates that words that occur in the same context tend to have similar meanings."},
{"id_case":"799","class":"3","sentence1":"We use the Stanford dependency parser ( Marneffe et al., 2006 )."},
{"id_case":"800","class":"3","sentence1":"We also replicated the experiment of Holmqvist et al. (2012) on this dataset."},
{"id_case":"1301","class":"3","sentence1":"We develop translation models using the phrase-based Moses ( Koehn et al., 2007 ) SMT system."},
{"id_case":"1302","class":"2","sentence1":"POS tagging was performed with TreeTagger ( Schmid, 1994 )."},
{"id_case":"1303","class":"3","sentence1":"We used the MaltParser ( Nivre et al., 2007 ) for parsing experiments."},
{"id_case":"1304","class":"2","sentence1":"Classification uses the scikit-learn Python package ( Pedregosa et al., 2011 )."},
{"id_case":"1305","class":"3","sentence1":"We use the Stanford dependency parser ( Marneffe et al., 2006 )."},
{"id_case":"1306","class":"3","sentence1":"We used Adam ( Kingma and Ba, 2014 ) with a learning rate of 0.0002."},
{"id_case":"1307","class":"2","sentence1":"Word alignment is performed using GIZA++ ( Och and Ney, 2003 )."},
{"id_case":"1308","class":"2","sentence1":"The test set was tagged with the French TreeTagger ( Schmid, 1994 )."},
{"id_case":"1309","class":"3","sentence1":"For the phrase-based SMT system, we adopted the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"1310","class":"2","sentence1":"POS tagging was performed with TreeTagger ( Schmid, 1994 )."},
{"id_case":"1311","class":"3","sentence1":"For building the word alignment models we use MGIZA++ ( Gao and Vogel, 2008 )."},
{"id_case":"1313","class":"2","sentence1":"The Polish data is taken from the EUROPARL corpus ( Koehn, 2005 )."},
{"id_case":"1314","class":"2","sentence1":"The German-to-English corpus is Europarl v7 ( Koehn, 2005 )."},
{"id_case":"1315","class":"2","sentence1":"Distributional models of meaning follow the distributional hypothesis ( Harris, 1954 ), which states that two words that occur in similar contexts have similar meanings."},
{"id_case":"1316","class":"2","sentence1":"conducted using the Moses phrase-based decoder ( Koehn et al., 2007 )."},
{"id_case":"1317","class":"2","sentence1":"Europarl 2 ( Koehn, 2005 ): it is a corpus of parallel texts in 11 languages from the proceedings of the European Parliament ."},
{"id_case":"1318","class":"3","sentence1":"We conducted statistical significance tests for BLEU between our best domain-adapted system, the baseline and the three third-party systems using paired bootstrap resampling ( Koehn, 2004) with 1,000 "},
{"id_case":"1319","class":"3","sentence1":"The phrase-based SMT framework which we used is based on the log-linear model ( Och and Ney, 2002 ), where the decision rule is expressed as follow: argmax e P (e|f ) = argmax e M ? m=1 ? m h m (e, f "},
{"id_case":"1320","class":"2","sentence1":"The English side was tokenized using the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"1321","class":"3","sentence1":"We develop translation models using the phrase-based Moses ( Koehn et al., 2007 ) SMT system."},
{"id_case":"1322","class":"2","sentence1":"The baseline will be created by the Moses SMT toolkit ( Koehn et al., 2007 )."},
{"id_case":"1323","class":"2","sentence1":"The resulting matrix is weighted using pointwise mutual information ( Church and Hanks, 1990 )."},
{"id_case":"1324","class":"2","sentence1":"The SMT systems were built using the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"1325","class":"2","sentence1":"The morpho-syntactic tagging has been made with the Tree Tagger ( Schmid, 1994 )."},
{"id_case":"1326","class":"3","sentence1":"For the determination of POS tags we use the Stuttgart TreeTagger ( Schmid, 1994 )."},
{"id_case":"1327","class":"2","sentence1":"The Polish data is taken from the EUROPARL corpus ( Koehn, 2005 )."},
{"id_case":"1328","class":"2","sentence1":"All annotations were done using the brat rapid annotation tool ( Stenetorp et al., 2012 )."},
{"id_case":"1329","class":"2","sentence1":"The Spanish-English (S2E) training corpus was drawn from the Europarl collection ( Koehn, 2005 )."},
{"id_case":"1330","class":"3","sentence1":"We used TnT ( Brants, 2000 ), trained on the Negra training set."},
{"id_case":"1331","class":"3","sentence1":"For all experiments, we used the Moses SMT system ( Koehn et al., 2007 )."},
{"id_case":"1332","class":"2","sentence1":"Two baseNP data sets have been put forward by ( Ramshaw and Marcus, 1995 )."},
{"id_case":"1333","class":"3","sentence1":"Both of our systems were based on the Moses decoder ( Koehn et al., 2007 )."},
{"id_case":"1334","class":"3","sentence1":"For the phrase-based SMT system, we adopted the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"1335","class":"2","sentence1":"Corpus-based meaning representations rely on the distributional hypothesis, which assumes that words occurring in a similar set of contexts are also similar in meaning ( Harris, 1954 )."},
{"id_case":"1336","class":"2","sentence1":"Recently, Naim et al. (2014)  proposed an unsupervised learning algorithm for automatically aligning sentences in a document with corresponding video segments."},
{"id_case":"1337","class":"2","sentence1":"A distributional similarity model is constructed based on the distributional hypothesis ( Harris, 1954 ): words that occur in the same contexts tend to share similar meanings."},
{"id_case":"1338","class":"2","sentence1":"This paper describes the details of our system that participated in the subtask A of Semeval-2014 Task 9: Sentiment Analysis in Twitter ( Rosenthal et al., 2014 )."},
{"id_case":"1339","class":"3","sentence1":"We used the phrase-based model Moses ( Koehn et al., 2007 ) for the experiments with all the standard settings, including a lexicalized reordering model, and a 5-gram language model."},
{"id_case":"1340","class":"2","sentence1":"Combinatory Categorial grammar (CCG) is a linguistic formalism that represents both the syntax and semantics of language ( Steedman, 1996 )."},
{"id_case":"1341","class":"3","sentence1":"We used the Random Forests implementation of scikit-learn toolkit ( Pedregosa et al., 2011 ) with 50 estimators."},
{"id_case":"1342","class":"2","sentence1":"The proposed model extends the LDA framework of Blei et al. (2003) ."},
{"id_case":"1343","class":"3","sentence1":"We used the training section of the dataset from Gimpel et al. (2011) ."},
{"id_case":"1344","class":"2","sentence1":"5-gram language models of Turkish and English were trained using KenLM ( Heafield, 2011 )."},
{"id_case":"1345","class":"2","sentence1":"The corpora are first tokenized and lowercased using the Moses scripts, then lemmatized and tagged by part-of-speech (PoS) using the TreeTagger ( Schmid, 1994 )."},
{"id_case":"1346","class":"2","sentence1":"The statistical significance test is also carried out with paired bootstrap resampling method with p < 0.001 intervals ( Koehn, 2004 )."},
{"id_case":"1347","class":"2","sentence1":"Li et al. (2012) used the dictionary from Wiktionary, 1 a crowd-sourced dictionary."},
{"id_case":"1348","class":"2","sentence1":"The realisation ranking component is an SVM ranking model implemented with SVMrank, a Support Vector Machine-based learning tool ( Joachims, 2006 )."},
{"id_case":"1351","class":"2","sentence1":"The statistical significance test is also carried out with paired bootstrap resampling method with p < 0.001 intervals ( Koehn, 2004 )."},
{"id_case":"1352","class":"3","sentence1":"We use the NLTK toolkit ( Loper and Bird, 2002 ) to extract the numerical quantity from each sentence."},
{"id_case":"1357","class":"3","sentence1":"We mark the source tokens to which each target unk symbol is most aligned with the method of (Luong et al. (2015)) ."},
{"id_case":"1358","class":"3","sentence1":"We rely on the hybrid aligned lexical semantic resource proposed by ( Faralli et al. 2016 ) to perform WSD."},
{"id_case":"1359","class":"2","sentence1":"To help improve the information extraction tools, a corpus , called BioScope, has been annotated for speculation, negation and its linguistic scopes in biomedical texts ( Szarvas et al., 2008 )."},
{"id_case":"1362","class":"2","sentence1":"The module of coreference resolution included in the IXA pipeline is loosely based on the Stanford Multi Sieve Pass system ( Lee et al., 2013 )."},
{"id_case":"1363","class":"3","sentence1":"We run our experiments on Europarl ( Koehn, 2005 ), a multilingual parallel corpus extracted from the proceedings of the European Parliament."},
{"id_case":"1364","class":"3","sentence1":"We used the scikit-learn ( Pedregosa et al., 2011 ) implementation of these classifiers with their default parameter settings for our experiments."},
{"id_case":"1365","class":"3","sentence1":"We used the scikit-learn ( Pedregosa et al., 2011 ) implementation of these classifiers with their default parameter settings for our experiments."},
{"id_case":"1366","class":"2","sentence1":"Automatic Multi-Document Summarization (MDS) aims at selecting the relevant information from multiple documents on the same topic to produce a summary ( Mani, 2001 )."},
{"id_case":"1367","class":"3","sentence1":"Additionally, we train phrase-based machine translation models with our alignments using the popular Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"1368","class":"3","sentence1":"We used the scikit-learn ( Pedregosa et al., 2011 ) implementation of these classifiers with their default parameter settings for our experiments."},
{"id_case":"1369","class":"3","sentence1":"Additionally, we train phrase-based machine translation models with our alignments using the popular Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"1370","class":"3","sentence1":"We train classifiers for each of the above feature types and for the full feature set on the training set of each corpus using the default configuration of the naive Bayes implementation of Weka  (Hall et al., 2009 )."},
{"id_case":"1371","class":"2","sentence1":"In addition, the corpus was lemmatised using the TreeTagger lemmatizer ( Schmid, 1994 )."},
{"id_case":"1372","class":"2","sentence1":"They are based on Distributional Hypothesis which works under the assumption that similar words occur in similar contexts ( Harris, 1954 )."},
{"id_case":"1373","class":"3","sentence1":"We used the same test set used in (Li et al. (2004)) for our testing 5 ."},
{"id_case":"1374","class":"3","sentence1":"We develop translation models using the phrase-based Moses ( Koehn et al., 2007 ) SMT system."},
{"id_case":"1375","class":"3","sentence1":"We used non-local features based on Finkel et al. (2005) ."},
{"id_case":"1376","class":"3","sentence1":"For building the baseline SMT system, we used the open-source SMT toolkit Moses ( Koehn et al., 2007 ), in its standard setup."},
{"id_case":"1377","class":"3","sentence1":"For translation, we use Moses ( Koehn et al., 2007 ) with lexicalized reordering (step), and the proposed model with latent derivations (lader)."},
{"id_case":"1378","class":"3","sentence1":"To recognize explicit connectives, we construct a list of existing connectives labeled in the Penn Discourse Treebank ( Prasad et al., 2008a )."},
{"id_case":"1379","class":"3","sentence1":"To construct language models and measure perplexity , we use SRILM (Stolcke, 2002) with interpolated modified Kneser-Ney discounting ( Chen and Goodman, 1996 ) and with a fixed vocabulary ."},
{"id_case":"1380","class":"3","sentence1":"We use Gibbs sampling to estimate the distributions of N and M , integrating out the multinomial parameters ( Griffiths and Steyvers, 2004 )."},
{"id_case":"1381","class":"1","sentence1":"Arabizi is not a letter-based transliteration from the Arabic script as is, for example, the Buckwalter transliteration ( Buckwalter, 2004 )."},
{"id_case":"1382","class":"3","sentence1":"To construct language models and measure perplexity , we use SRILM (Stolcke, 2002) with interpolated modified Kneser-Ney discounting ( Chen and Goodman, 1996 ) and with a fixed vocabulary ."},
{"id_case":"1384","class":"3","sentence1":"We build upon our previous Markov Logic based approach for joint concept disambiguation and clustering ( Fahrni and Strube, 2012 )."},
{"id_case":"1385","class":"3","sentence1":"We use the feedforward neural probabilistic language model architecture of (Vaswani et al. (2013)) , as shown in Figure 4 ."},
{"id_case":"1386","class":"2","sentence1":"The SMT systems were built using the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"1387","class":"3","sentence1":"For building the baseline SMT system, we used the open-source SMT toolkit Moses ( Koehn et al., 2007 ), in its standard setup."},
{"id_case":"1388","class":"3","sentence1":"In-domain SMT: we used the parallel corpus (Section 3) to train an en-pt phrase-based SMT system using the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"1389","class":"2","sentence1":"Following resource collection and construction a SMT model for English\/Brazilian-Portuguese was trained using the Moses toolkit ( Koehn et al., 2007 ) using its baseline settings."},
{"id_case":"1390","class":"2","sentence1":"One semiautomatic approach to evaluation is ROUGE ( Lin and Hovy, 2003 ), which is primarily based on ngram co-occurrence between automatic and human summaries."},
{"id_case":"1391","class":"3","sentence1":"We use the Web 1T 5-gram corpus ( Brants and Franz, 2006 ) to compute the language model score for a sentence."},
{"id_case":"1392","class":"2","sentence1":"The SemEval-2015 Aspect Based Sentiment Analysis task is a continuation of SemEval-2014 Task 4 ( Pontiki et al., 2014 )."},
{"id_case":"1393","class":"2","sentence1":"The 2009 Bio NLP shared task ( Kim et al., 2009 ) aimed at extracting biological \"events\", where one of the event types was gene expression."},
{"id_case":"1394","class":"2","sentence1":"The constituent context model (CCM) for inducing constituency parses ( Klein and Manning, 2002 ) was the first unsupervised approach to surpass a right-branching baseline ."},
{"id_case":"1395","class":"0","sentence1":"The CCM is a generative model for the unsupervised induction of binary constituency parses over sequences of part-of-speech (POS) tags ( Klein and Manning, 2002 )."},
{"id_case":"1397","class":"3","sentence1":"We use the implementation provided by Tai et al. (2015) , changing only the dependency parses that are fed to their model."},
{"id_case":"1398","class":"2","sentence1":"Europarl ( Koehn, 2005 ) is a multilingual parallel corpus extracted from the proceedings of the European Parliament."},
{"id_case":"1399","class":"2","sentence1":"The Stanford dependency parser ( De Marneffe et al., 2006 ) is used for extracting features from the dependency parse trees."},
{"id_case":"1400","class":"2","sentence1":"The Stanford dependency parser ( De Marneffe et al., 2006 ) is used for extracting features from the dependency parse trees."},
{"id_case":"1901","class":"2","sentence1":"The major part of data comes from current and upcoming full releases of the Europarl data set ( Koehn, 2005 )."},
{"id_case":"1902","class":"2","sentence1":"This is an extension of UBK algorithm ( Agirre and Soroa, 2009 ), the first application of personalized PageRank to the WSD problem."},
{"id_case":"1903","class":"1","sentence1":"We compare the proposed model to our implementation of the IOBES-based model described in Collobert et al. (2011) , applied to MWE tagging ."},
{"id_case":"1904","class":"2","sentence1":"It is a phrase-based system built using the Moses toolkit ( Koehn et al., 2007 ) and trained\/tuned using only the preprocessed (tokenised, lower-cased) parallel data provided for the shared task."},
{"id_case":"1905","class":"3","sentence1":"In this section, we first discuss the hybrid tree model of (Lu et al. (2008)) , and introduce a novel extension."},
{"id_case":"1906","class":"2","sentence1":"The Europarl corpus ( Koehn, 2005 ) is built from the proceedings of the European Parliament."},
{"id_case":"1907","class":"2","sentence1":"The Europarl corpus ( Koehn, 2005 ) is built from the proceedings of the European Parliament."},
{"id_case":"1908","class":"3","sentence1":"We use Adam ( Kingma and Ba, 2015 ) for optimisation with initial learning rate of 0.001."},
{"id_case":"1909","class":"3","sentence1":"We use the open-source Moses toolkit ( Koehn et al., 2007 ) to build a standard phrase-based SMT system which extracts up to 8 words phrases in the Moses phrase table."},
{"id_case":"1910","class":"3","sentence1":"We used the mkcls tool in GIZA++ ( Och and Ney, 2003 ) to learn the word classes."},
{"id_case":"1911","class":"3","sentence1":"We then use the phrase extraction utility in the Moses statistical machine translation system ( Koehn et al., 2007 ) to extract a phrase table which operates over characters ."},
{"id_case":"1912","class":"2","sentence1":"Phrase pairs are extracted from IBM4 alignments obtained with GIZA++( Och and Ney, 2003 )."},
{"id_case":"1914","class":"3","sentence1":"Specifically, we build off the Bayesian block HMMs used by (Ritter et al. (2010)) for modeling Twitter conversations, which will be our primary baseline."},
{"id_case":"1915","class":"3","sentence1":"Like (Cho & Chai (2000)) , our analysis also provides the same explanation for various scrambled sentences such as the Double Accusative Construction (DAC) in Korean."},
{"id_case":"1916","class":"2","sentence1":"In PB-SMT, the posterior probability P(e I 1 |f J 1 ) is directly modelled as a (log-linear) combination of features ( Och and Ney, 2002 ), that usually comprise M translational features, and the lang"},
{"id_case":"1917","class":"3","sentence1":"We propose a relatively simple and nuanced unsupervised model inspired by the monolingual weighted matrix factorization (WMF) model proposed in ( Guo and Diab, 2012 ), which we extend to the cross-lin"},
{"id_case":"1918","class":"3","sentence1":"Our work is also related to ( Bunescu and Mooney, 2005 ), where the similarity between the words on the path connecting two entities in the dependency graph is used to devise a Kernel function."},
{"id_case":"1919","class":"3","sentence1":"Motivated by previous work, we include a frequency count of 17 discourse markers which were found to be the most common across the ARGUE corpus ( Abbott et al., 2011 )."},
{"id_case":"1920","class":"2","sentence1":"The highest performance levels were achieved using a sequential minimal optimization algorithm for training a support vector classifier using polynomial kernels ( Platt, 1998 )."},
{"id_case":"1922","class":"2","sentence1":"In principle, classifiers trained on PDTB data can be applied directly to label connectives over the English side of the Europarl corpus ( Koehn, 2005 ) used for training and testing SMT."},
{"id_case":"1923","class":"2","sentence1":"The Shared Task on Language Identification in Code- Switched Data held in 2014 ( Solorio et al., 2014 ) is another related competition, where the focus was on tweets in which users were mixing two or "},
{"id_case":"1924","class":"2","sentence1":"The corpora are tokenised and truecased using scripts from the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"1925","class":"2","sentence1":"The HMM classifier is similar to the one described in ( Bikel et al., 1999 )."},
{"id_case":"1926","class":"3","sentence1":"In the next section we briefly review modeling of transition probabilities in a conventional HMM alignment model ( Vogel et al., 1996, Och and Ney, 2000a )."},
{"id_case":"1927","class":"2","sentence1":"This result is statistically significant at p = 0.05 according to Bootstrap Resampling Test ( Koehn, 2004 )."},
{"id_case":"1928","class":"2","sentence1":"Starting with TextRank ( Mihalcea and Tarau, 2004 ), graph-based ranking methods are becoming the most widely used unsupervised approach for keyphrase extraction."},
{"id_case":"1929","class":"3","sentence1":"Because of our experience with the Weka package ( Hall et al., 2009 ) we chose this tool for implementation ."},
{"id_case":"1930","class":"0","sentence1":"Dropout ( Srivastava et al., 2014 ) is implemented with a dropout rate of 0.2 to prevent the model from overfitting."},
{"id_case":"1931","class":"3","sentence1":"In-domain SMT: we used the parallel corpus (Section 3) to train an en-pt phrase-based SMT system using the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"1932","class":"3","sentence1":"As mentioned in Section 3, we obtained dependencies from the output of the Stanford parser ( de Marneffe and Manning, 2008 )."},
{"id_case":"1933","class":"3","sentence1":"We tokenize and truecase all of the corpora using code released with Moses ( Koehn et al., 2007 )."},
{"id_case":"1935","class":"3","sentence1":"To obtain these we use the Stanford dependency parser ( de Marneffe et al., 2006 ) and the forced alignment from Section 3.9."},
{"id_case":"1936","class":"2","sentence1":"The NMT models are trained using Adam optimizer ( Kingma and Ba, 2014 ) with an initial learning rate of 0.0001."},
{"id_case":"1937","class":"0","sentence1":"The improved alignments gave a gain of Table 8 : Hierarchical lexicalized reordering model ( Galley and Manning, 2008 )."},
{"id_case":"1938","class":"2","sentence1":"The statistical significance tests using 95% confidence interval are measured with paired bootstrap resampling ( Koehn, 2004 )."},
{"id_case":"1939","class":"3","sentence1":"To obtain these we use the Stanford dependency parser ( de Marneffe et al., 2006 ) and the forced alignment from Section 3.9."},
{"id_case":"1940","class":"3","sentence1":"For all experiments, we used the Moses SMT system ( Koehn et al., 2007 )."},
{"id_case":"1941","class":"3","sentence1":"We use the AdaGrad algorithm ( Duchi et al., 2011 ) to optimize the conditional, marginal log-likelihood of the data."},
{"id_case":"1942","class":"1","sentence1":"The other is from (Jeffrey Pennington et al. (2014)) , the dimension of word embedding is 100."},
{"id_case":"1943","class":"3","sentence1":"We use the AdaGrad optimizer ( Duchi et al., 2011)  with initial learning rate set to 0.1."},
{"id_case":"1944","class":"3","sentence1":"We experiment with the phrase-based statistical machine translation toolkit Moses ( Koehn et al., 2007 ) in order to train a Japanese -English system and to show the influence of the expanded parallel"},
{"id_case":"1945","class":"3","sentence1":"In-domain SMT: we used the parallel corpus (Section 3) to train an en-pt phrase-based SMT system using the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"1946","class":"3","sentence1":"We follow the definition in Cohen et al. (2012) of L-PCFGs."},
{"id_case":"1947","class":"2","sentence1":"The language model is a 5-gram KenLM ( Heafield, 2011 ) model, trained using lmplz, with modified Kneser-Ney smoothing and no pruning."},
{"id_case":"1949","class":"3","sentence1":"As mentioned in Section 3, we obtained dependencies from the output of the Stanford parser ( de Marneffe and Manning, 2008 )."},
{"id_case":"1950","class":"1","sentence1":"Also, we evaluate on the RTE part of the SICK dataset ( Marelli et al., 2014 ) and show that our approach leads to improvements."},
{"id_case":"1951","class":"2","sentence1":"Our model has a \" Siamese \" structure ( Bromley et al., 1993 ) with two subnetworks each processing a sentence in parallel."},
{"id_case":"1952","class":"3","sentence1":"For building the word alignment models we use MGIZA++ ( Gao and Vogel, 2008 )."},
{"id_case":"1953","class":"2","sentence1":"The dialogue act labelling of the corpus follows the DATE tagging scheme ( Walker et al., 2001 )."},
{"id_case":"1954","class":"3","sentence1":"We also obtain the dependency parse of the sentences using the Stanford parser ( De Marneffe et al., 2006 )."},
{"id_case":"1955","class":"2","sentence1":"The resulting matrix is weighted using pointwise mutual information ( Church and Hanks, 1990 )."},
{"id_case":"1956","class":"3","sentence1":"We extract structured facts using two methods: Clausie (Del Corro and Gemulla, 2013 ) and Sedona ( detailed later in Sec 4); also see Fig 1."},
{"id_case":"1957","class":"2","sentence1":"Dropout ( Srivastava et al., 2014 ) is implemented with a dropout rate of 0.2 to prevent the model from overfitting."},
{"id_case":"1958","class":"2","sentence1":"The population distribution was estimated by the bootstrap method ( Cohen, 1995 )."},
{"id_case":"1959","class":"2","sentence1":"The statistical significance test is also carried out with paired bootstrap resampling method with p < 0.001 intervals ( Koehn, 2004 )."},
{"id_case":"1960","class":"2","sentence1":"The bootstrap sampling method provides a way for artificially establishing a sampling distribution for a statistic, when the distribution is not known ( Cohen, 1995 )."},
{"id_case":"1961","class":"3","sentence1":"For medical we use the biomedical data from EMEA ( Tiedemann, 2009 )."},
{"id_case":"1962","class":"2","sentence1":"The MT experiments were carried out using the standard log-linear phrase-based SMT toolkit MOSES ( Koehn et al., 2007 )."},
{"id_case":"1963","class":"1","sentence1":"For the theory of Cho & Chai (2000) to be complete, we have proposed a new type of marker and the Adjunct LP Constraint, in conjunction with their Argument LP Constraint."},
{"id_case":"1964","class":"0","sentence1":"3 With these trees fixed, the partial derivatives with respect to parameters are computed via the backpropagation through structures algorithm ( Goller and Kuchler, 1996 )."},
{"id_case":"1965","class":"2","sentence1":"Many researchers have considered generating paraphrases by mining the Web guided by the distributional hypothesis, which states that words occurring in similar contexts tend to have similar meanings ( Harris, 1954 )."},
{"id_case":"1966","class":"3","sentence1":"We used Weka ( Hall et al., 2009 ) for all our classification experiments."},
{"id_case":"1967","class":"3","sentence1":"Bilingual Corpora The corpus used in the following experiments is the Basic Travel Expression Corpus ( Takezawa et al., 2002 )."},
{"id_case":"1968","class":"3","sentence1":"First, we used the Moses toolkit ( Koehn et al., 2007 ) for statistical machine translation."},
{"id_case":"1969","class":"1","sentence1":"Another parallel corpus is the JRC-Acquis Multilingual Parallel Corpus ( Steinberger et al., 2006 )."},
{"id_case":"1970","class":"2","sentence1":"The parallel corpus is wordaligned using GIZA++ ( Och and Ney, 2003 )."},
{"id_case":"1971","class":"2","sentence1":"It is similar to ( Och and Ney, 2000a )."},
{"id_case":"1972","class":"3","sentence1":"We thus cast MSC as a semantic sentence classification task in a CNN architecture, adopting the one-layer CNN model of Kim (2014), a variant of Collobert et al. (2011) ."},
{"id_case":"1973","class":"2","sentence1":"All these features are inherited from Moses ( Koehn et al., 2007 )."},
{"id_case":"1974","class":"3","sentence1":"For this purpose we use the Europarl corpus ( Koehn, 2005 )."},
{"id_case":"1975","class":"2","sentence1":"The texts were first automatically segmented and tokenized 10 and then they were part-of-speech tagged by TnT tagger ( Brants, 2000 ), which was trained on the respective CoNLL training data (the file"},
{"id_case":"1976","class":"1","sentence1":"Secondly, Holmqvist et al. (2012) reordered source words based on word alignment, whereas we suggest reordering source chunks."},
{"id_case":"1977","class":"3","sentence1":"For language modeling, we use the English Gigaword corpus with 5-gram LM implemented with the KenLM toolkit ( Heafield, 2011 )."},
{"id_case":"1978","class":"3","sentence1":"For our LDA implementations, we use MALLET ( McCallum, 2002 )."},
{"id_case":"1979","class":"3","sentence1":"We use the standard Stanford-style set of dependency labels ( de Marneffe et al., 2006 )."},
{"id_case":"1980","class":"3","sentence1":"Next we evaluate how well the complexity measures proposed in  (Raghavan et al. 2007 ) correlate with improvement in performance and improvement in learning rate."},
{"id_case":"1981","class":"2","sentence1":"Syntax in EPEC is annotated following the dependency based formalism used in the Prague Dependency Treebank, which was also used in the German NEGRA corpus ( Skut et al., 1997 )."},
{"id_case":"1982","class":"2","sentence1":"In addition, the corpus was lemmatised using the TreeTagger lemmatiser ( Schmid, 1994 )."},
{"id_case":"1983","class":"3","sentence1":"We used the scikit-learn ( Pedregosa et al., 2011 ) implementation of SVRs and the SKLL toolkit."},
{"id_case":"1984","class":"3","sentence1":"We used the scikit-learn toolkit to train our classifiers ( Pedregosa et al., 2011 )."},
{"id_case":"1985","class":"2","sentence1":"The training set is used to train the phrase-based translation model and language model for Moses ( Koehn et al., 2007 )."},
{"id_case":"1986","class":"2","sentence1":"These features were obtained using the Stanford parser 2 ( Marneffe et al., 2006 )."},
{"id_case":"1987","class":"3","sentence1":"We built a 5-gram language model on the English side of QCA-train using KenLM ( Heafield, 2011 )."},
{"id_case":"1988","class":"1","sentence1":"In the other side, the French corpus is part-of-speech (POS) tagged by using treetagger tool ( Schmid, 1994 ) for annotating text with part-of-speech and lemma information."},
{"id_case":"1989","class":"2","sentence1":"The COMLEX syntax dictionary ( Grishman et al., 1994 )."},
{"id_case":"1990","class":"3","sentence1":"We lemmatise the head of each constituent with TreeTagger ( Schmid, 1994 )."},
{"id_case":"1991","class":"2","sentence1":"with the KenLM toolkit ( Heafield, 2011 )."},
{"id_case":"1992","class":"3","sentence1":"We briefly review the HMM based word alignment models ( Vogel, 1996, Och and Ney, 2000 )."},
{"id_case":"1993","class":"3","sentence1":"We use the Stanford parser with Stanford dependencies ( Marneffe et al., 2006 )."},
{"id_case":"1995","class":"3","sentence1":"We use the Stanford parser with Stanford dependencies ( de Marneffe et al., 2006 )."},
{"id_case":"1996","class":"2","sentence1":"All the summaries are evaluated using ROUGE ( Lin, 2004 )."},
{"id_case":"1997","class":"2","sentence1":"The reported confidence intervals were estimated using bootstrap resampling ( Koehn, 2004 )."},
{"id_case":"1999","class":"3","sentence1":"We implemented CharWNN using the Theano library ( Bergstra et al., 2010 )."},
{"id_case":"2000","class":"3","sentence1":"For the linear logistic regression implementation we used scikit-learn ( Pedregosa et al., 2011 )."},
{"id_case":"2501","class":"3","sentence1":"The tagger we use is TnT ( Brants, 2000 ) , a hidden Markov trigram tagger, which was trained on the Spoken Dutch Corpus (CGN), Internal Release 6."},
{"id_case":"2502","class":"3","sentence1":"We have used the implementation described in ( Schapire and Singer, 1999 ) with decision trees of depth fixed to 3."},
{"id_case":"2503","class":"3","sentence1":"We built a 5-gram language model on the English side of QCA-train using KenLM ( Heafield, 2011 )."},
{"id_case":"2504","class":"2","sentence1":"DELPH-IN Minimal Recursion Semantics (DM) As part of the full HPSG sign, the ERG also makes available a logical-form representation of propositional semantics in the format of Minimal Recursion Semant"},
{"id_case":"2505","class":"1","sentence1":"Like the CoNLL-2006 shared task, the 2007 shared task focuses on dependency parsing and aims at comparing state-of-the-art machine learning algorithms applied to this task ( Nivre et al., 2007 )."},
{"id_case":"2506","class":"3","sentence1":"Like ( Cho & Chai 2000 ), our analysis also provides the same explanation for various scrambled sentences such as the Double Accusative Construction (DAC) in Korean."},
{"id_case":"2507","class":"2","sentence1":"The phrase table is extracted from a bilingual text aligned on the word level, using e.g. GIZA ++ ( Och and Ney, 2003 )."},
{"id_case":"2508","class":"2","sentence1":"Google Web 1T ( Brants and Franz, 2006 ) has been used to calculate term idf, which is used as a measure of the importance of the terms."},
{"id_case":"2509","class":"3","sentence1":"We first use a dependency parser ( de Marneffe et al., 2006 ) to parse each sentence and extract the set of dependency relations associated with the sentence ."},
{"id_case":"2510","class":"3","sentence1":"We first use a dependency parser ( de Marneffe et al., 2006 ) to parse each sentence and extract the set of dependency relations associated with the sentence ."},
{"id_case":"2511","class":"1","sentence1":"These analyses provide an alternative but theoretically more reasonable explanation to the findings of ( Liang et al. 2006 ) : while they blame \" unreasonable \" gold derivations for the failure of stand"},
{"id_case":"2512","class":"3","sentence1":"We experiment with the phrase-based statistical machine translation toolkit Moses ( Koehn et al., 2007 ) in order to train a Japanese -English system and to show the influence of the expanded parallel"},
{"id_case":"2513","class":"3","sentence1":"For our corpus, we randomly selected documents from the Washington section of the New York Times corpus ( Sandhaus, 2008 ) from the year 2007."},
{"id_case":"2514","class":"2","sentence1":"They used the Web-based annotation tool brat ( Stenetorp et al., 2012 ) for the annotation ."},
{"id_case":"2515","class":"2","sentence1":"All corpora were taken from the CHILDES database ( MacWhinney, 2000 )."},
{"id_case":"2516","class":"2","sentence1":"The system generated tweets were evaluated using ROUGE measures ( Lin, 2004 )."},
{"id_case":"2517","class":"2","sentence1":"The TOEFL synonym selection task is to select the semantically closest word to a target from a list of four candidates ( Landauer and Dumais, 1997 )."},
{"id_case":"2518","class":"3","sentence1":"We calculate our features using the KenLM toolkit ( Heafield, 2011 )."},
{"id_case":"2519","class":"3","sentence1":"We use the scikit implementation of Random Forest ( Pedregosa et al., 2011 )."},
{"id_case":"2521","class":"3","sentence1":"For this purpose we use the Europarl corpus ( Koehn, 2005 )."},
{"id_case":"2522","class":"2","sentence1":"The statistical significance tests using 95% confidence interval are measured with paired bootstrap resampling ( Koehn, 2004 )."},
{"id_case":"2523","class":"3","sentence1":"We then run word alignment with GIZA++ ( Och and Ney, 2003 ) in both directions, with the default parameters used in Moses."},
{"id_case":"2524","class":"3","sentence1":"We built a 5-gram language model on the English side of QCA-train using KenLM ( Heafield, 2011 )."},
{"id_case":"2525","class":"3","sentence1":"We used GIZA++ ( Och and Ney, 2003 ) to align the words in the corpus."},
{"id_case":"2526","class":"2","sentence1":"The phrase tables were generated by means of symmetrised word alignments obtained with GIZA++ ( Och and Ney, 2003 )."},
{"id_case":"2527","class":"2","sentence1":"Zhu et al. (2013) applied Kalman filter model to learn and estimate user intentions in their human-computer interactive word segmentation framework."},
{"id_case":"2528","class":"2","sentence1":"( Koo et al. 2008 )  have proposed to use word clusters as features to improve graph-based statistical dependency parsing for English and Czech."},
{"id_case":"2529","class":"2","sentence1":"More recently, ( Pasha et al., 2014 ) created MADAMIRA, a system for morphological analysis and disambiguation of Arabic, this system can be used to improve the accuracy of spelling checking system es"},
{"id_case":"2530","class":"3","sentence1":"We used the Moses toolkit ( Koehn et al., 2007 ) with its default settings."},
{"id_case":"2531","class":"3","sentence1":"@BULLET Naive Bayes(NB): We use Binomial variant with Laplace smoothing parameter = 1 ( Pedregosa et al., 2011 )."},
{"id_case":"2532","class":"3","sentence1":"We use AdaGrad ( Duchi et al., 2011)  with the initial learning rate set to ? = 0.5."},
{"id_case":"2533","class":"2","sentence1":"The MT experiments were carried out using the standard log-linear phrase-based SMT toolkit MOSES ( Koehn et al., 2007 )."},
{"id_case":"2534","class":"3","sentence1":"We use the AdaGrad method ( Duchi et al., 2011 ) to automatically update the learning rate for each parameter."},
{"id_case":"2535","class":"2","sentence1":"Among the existing sense-tagged corpora, the SEMCOR corpus ( Miller et al., 1994 ) is one of the most widely used."},
{"id_case":"2536","class":"3","sentence1":"We use the Liblinear Support Vector Machine (SVM) ( Chang and Lin, 2011 ) classifier for training and run 5-fold cross-validation for evaluation."},
{"id_case":"2537","class":"3","sentence1":"We build a state of the art phrase-based SMT system using Moses ( Koehn et al., 2007 )."},
{"id_case":"2538","class":"3","sentence1":"We built a source-to-target PB-SMT model from the bilingual domain corpus using the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"2539","class":"2","sentence1":"The dictionaries are automatically generated via word alignment using GIZA++ ( Och and Ney, 2000 ) on parallel corpora."},
{"id_case":"2540","class":"2","sentence1":"There are two main approaches to processing non-standard data: normalization and domain adaptation ( Eisenstein, 2013 )."},
{"id_case":"2541","class":"3","sentence1":"We conducted baseline experiments for phrasebased machine translation using the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"2542","class":"2","sentence1":"The task of identifying mentions to medical concepts in free text and mapping these mentions to a knowledge base was recently proposed in ShARe\/CLEF eHealth Evaluation Lab 2013 ( Suominen et al., 2013 )"},
{"id_case":"2543","class":"3","sentence1":"We built a source-to-target PB-SMT model from the bilingual domain corpus using the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"2545","class":"3","sentence1":"We also use MADA+TOKAN ( Habash et al., 2009 ) to preprocess and tokenize the Arabic side of the corpus."},
{"id_case":"2546","class":"1","sentence1":"Both training and testing data consist of PubMed abstracts extracted from the GENIA corpus ( Kim et al., 2008 )."},
{"id_case":"2547","class":"3","sentence1":"We conducted baseline experiments for phrasebased machine translation using the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"2548","class":"3","sentence1":"To extract our part-of-speech (POS) features, we first tag the transcripts using the NLTK POS tagger ( Bird et al., 2009 )."},
{"id_case":"2549","class":"2","sentence1":"The corpora are tokenised and truecased using scripts from the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"2550","class":"3","sentence1":"We trained a model using Moses toolkit ( Koehn et al., 2007 ) on the training data as our baseline system."},
{"id_case":"2551","class":"3","sentence1":"For seed and test paradigms we used verbal inflectional paradigms from the CELEX morphological database ( Baayen et al., 1995 )."},
{"id_case":"2552","class":"3","sentence1":"Then we did word alignment using GIZA++ ( Och and Ney, 2003 ) with the default grow-diagfinal-and alignment symmetrization method."},
{"id_case":"2553","class":"2","sentence1":"( Pang et al. 2002) have reported the effectiveness of applying machine learning techniques to the p\/n classification."},
{"id_case":"2555","class":"3","sentence1":"We train the concept identification stage using infinite ramp loss (1) with AdaGrad ( Duchi et al., 2011 )."},
{"id_case":"2556","class":"3","sentence1":"For the contextual check we use the Google Web 1T 5-gram Corpus ( Brants and Franz, 2006 ) which contains counts for n-grams from unigrams through to five-grams obtained from over 1 trillion word toke"},
{"id_case":"2557","class":"3","sentence1":"We built a source-to-target PB-SMT model from the bilingual domain corpus using the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"2558","class":"3","sentence1":"@BULLET Naive Bayes(NB): We use Binomial variant with Laplace smoothing parameter = 1 ( Pedregosa et al., 2011 )."},
{"id_case":"2559","class":"3","sentence1":"We built a source-to-target PB-SMT model from the bilingual domain corpus using the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"2560","class":"2","sentence1":"An unpruned, modified Kneser-Ney-smoothed 4-gram language model is estimated using the KenLM toolkit ( Heafield et al., 2013 )."},
{"id_case":"2561","class":"2","sentence1":"For native data, several teams make use of the Web 1T 5-gram corpus (henceforth Web1T, ( Brants and Franz, 2006 ))."},
{"id_case":"2562","class":"3","sentence1":"We measure statistical significance using 95% confidence intervals computed with paired bootstrap resampling ( Koehn, 2004 )."},
{"id_case":"2563","class":"2","sentence1":"1 The models are constructed using C4.5 decision tree classifiers as implemented within Weka ( Hall et al., 2009 ), with default parameter settings."},
{"id_case":"2564","class":"2","sentence1":"It therefore follows the distributional hypothesis ( Harris, 1954 ) which states that words that occur in the same contexts tend to have similar meanings."},
{"id_case":"2565","class":"2","sentence1":"Consider the two examples below, drawn from the Penn Discourse Treebank (PDTB) ( Prasad et al., 2008 ), of a causal and a contrast relation, respectively ."},
{"id_case":"2566","class":"2","sentence1":"BLE is based on the distributional hypothesis ( Harris, 1954 ), stating that words with similar meaning have similar distributions across languages ."},
{"id_case":"2567","class":"3","sentence1":"We use the BLEU score as primary criterion which is optimized on the development set using the Downhill Simplex algorithm ( Press et al., 2002 )."},
{"id_case":"2568","class":"1","sentence1":"The relation prediction task of Science IE is challenging and quite different from other semantic relation prediction task like SemEval-2010 Task 8 ( Hendrickx et al., 2009 )."},
{"id_case":"2569","class":"3","sentence1":"Machine translation system settings: We used a phrase-based statistical machine translation model as implemented in the Moses toolkit ( Koehn et al., 2007 ) for machine translation."},
{"id_case":"2570","class":"3","sentence1":"We use the MRS analyses that are produced by the HPSG English Resource Grammar (ERG) ( Flickinger, 2000 )."},
{"id_case":"2571","class":"2","sentence1":"In each case, the improvement of EBMT TM + SMT over the baseline SMT is statistically significant (reliability of 98%) using bootstrap resampling ( Koehn, 2004 )."},
{"id_case":"2572","class":"2","sentence1":"To capture typical part-of bridging (see Example 2), we extract a list of 45 nouns which specify building parts (e.g. room or roof ) from the General Inquirer lexicon ( Stone et al., 1966 )."},
{"id_case":"2574","class":"2","sentence1":"The surfacesyntactic representation ? p was a standard firstorder edge factorization using the same features as McDonald et al. (2005) ."},
{"id_case":"2575","class":"3","sentence1":"In our work, like Hernault et al. (2010) , we also consider the discourse segmentation task as a sequence labeling problem."},
{"id_case":"2576","class":"3","sentence1":"We use the data that were recorded and preprocessed by Mitchell et al. (2008) , available for download in their supporting online material."},
{"id_case":"2577","class":"3","sentence1":"We trained non-projective dependency parsers for 6 languages using the CoNLL-X shared task datasets ( Buchholz and Marsi, 2006 ): Arabic, Danish, Dutch, Japanese, Slovene, and Spanish."},
{"id_case":"2578","class":"3","sentence1":"Our LP constraints based on the new type marker are compatible with those of Cho & Chai (2000) on the one hand and are able to deal with the scrambling between adjuncts and arguments on the other hand"},
{"id_case":"2579","class":"2","sentence1":"The WSJ grammar covers the UPenn Wall Street Journal (WSJ) treebank sentences ( Marcus et al., 1994 )."},
{"id_case":"2580","class":"2","sentence1":"Statistically significant results, calculated with paired bootstrap resampling ( Koehn, 2004 ) for BLEU and NIST, are indicated with symbols (p = 0.01) and (p = 0.05)."},
{"id_case":"2581","class":"3","sentence1":"In testing, we used minimum Bayes risk decoding ( Kumar and Byrne, 2004 ), cube pruning, and the operation sequence model ( Durrani et al., 2011 )."},
{"id_case":"2582","class":"0","sentence1":"selects the translation with minimum Bayes risk ( Kumar and Byrne, 2004 )."},
{"id_case":"2583","class":"3","sentence1":"We use a prototype-based selectional preference model ( Erk, 2007 )."},
{"id_case":"2584","class":"3","sentence1":"We used 10-fold cross-validation, set the confidence interval to 95% and used the jackknifing procedure for multi-annotation evaluation ( Lin, 2004 )."},
{"id_case":"2585","class":"2","sentence1":"The Brown Corpus tagged with WordNet senses ( Miller et al., 1993 )."},
{"id_case":"2586","class":"3","sentence1":"As an implementation, we use SVM light ( Joachims, 1999 )."},
{"id_case":"2587","class":"2","sentence1":"For translation tables , the Moses system ( Koehn et al., 2007 ) as well as Portage offer model filtering (Moses: offline; Portage: offline and\/or at load time)."},
{"id_case":"2588","class":"2","sentence1":"The particle filter of Canini et al. (2009) rejuvenates over independent draws from the history by storing all past observations and states."},
{"id_case":"2589","class":"3","sentence1":"We describe an approximation to the BLEU score ( Papineni et al., 2001 ) that will satisfy these conditions ."},
{"id_case":"2590","class":"3","sentence1":"We applied the Naive Bayes probabilistic supervised learning algorithm from the Weka machine learning library ( Hall et al., 2009 )."},
{"id_case":"2593","class":"3","sentence1":"We minimize the cross entropy loss using gradient-based optimization and the Adam update rule ( Kingma and Ba, 2014 )."},
{"id_case":"2594","class":"2","sentence1":"Statistical significance in BLEU score difference was measured by using paired bootstrap resampling ( Koehn, 2004 )."},
{"id_case":"2595","class":"3","sentence1":"We measure statistical significance using 95% confidence intervals computed with paired bootstrap resampling ( Koehn, 2004 )."},
{"id_case":"2596","class":"2","sentence1":"3 As verbs, we take all tags that map to V in the universal tag mappings from Petrov et al. (2012) ."},
{"id_case":"2597","class":"3","sentence1":"We use the MRS analyses that are produced by the HPSG English Resource Grammar (ERG) ( Flickinger, 2000 )."},
{"id_case":"2598","class":"3","sentence1":"We measure statistical significance using 95% confidence intervals computed with paired bootstrap resampling ( Koehn, 2004 )."},
{"id_case":"2599","class":"1","sentence1":"On Semantic Role Labeling Gildea and Jurafsky (2002) first presented a system based on a statistical classifier which is trained on a hand-annotated corpora FrameNet."},
{"id_case":"2600","class":"3","sentence1":"We apply the stochastic gradient descent algorithm with mini-batches and the AdaDelta update rule ( Zeiler, 2012 )."},
{"id_case":"3101","class":"3","sentence1":"We used the Linear SVM implementation (with default parameters), and Random Forest with 50 trees, both available at Scikit-learn ( Pedregosa et al., 2011 )."},
{"id_case":"3102","class":"3","sentence1":"We test the statistical significance of differences between various MT systems using the bootstrap resampling method ( Koehn, 2004 )."},
{"id_case":"3103","class":"2","sentence1":"In particular, the parser implements the arc-standard parsing algorithm ( Nivre, 2004 )."},
{"id_case":"3104","class":"3","sentence1":"We use Adadelta ( Zeiler, 2012 ) to update the parameters during training."},
{"id_case":"3105","class":"3","sentence1":"We used the English side of the Europarl corpus ( Koehn, 2005 )."},
{"id_case":"3106","class":"2","sentence1":"CCG is a lexicalized theory of grammar ( Steedman, 2001 )."},
{"id_case":"3107","class":"2","sentence1":"The results for TESLA-M and TESLA-F have previously been reported in (Liu et al 2010) 3 ."},
{"id_case":"3108","class":"2","sentence1":"The semantic representation is Minimal Recursion Semantics ( Copestake et al., 2005 )."},
{"id_case":"3109","class":"2","sentence1":"GermaNet (GN) is the German counterpart to WN ( Hamp and Feldweg, 1997 )."},
{"id_case":"3110","class":"2","sentence1":"(Baroni et al 2002)  report that 47% of the vocabulary (types) in the APA corpus 2 were compounds ."},
{"id_case":"3111","class":"2","sentence1":"Both language models use modified Kneser-Ney smoothing ( Chen and Goodman, 1996 )."},
{"id_case":"3112","class":"3","sentence1":"We used MALLET ( McCallum, 2002 ) for this experiment and the same training\/testing partitions used in the experiment reported in Table 3 ."},
{"id_case":"3114","class":"3","sentence1":"We calculated significance using paired bootstrap resampling ( Koehn, 2004 )."},
{"id_case":"3115","class":"3","sentence1":"Additionally, we train phrase-based machine translation models with our alignments using the popular Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"3116","class":"3","sentence1":"For building the baseline SMT system, we used the open-source SMT toolkit Moses ( Koehn et al., 2007 ), in its standard setup."},
{"id_case":"3117","class":"2","sentence1":"This results in the semantic triple shot(man,bird), lemmatized to shoot(man,bird), using the Stanford CoreNLP lemmatizer ( Manning et al., 2014 )."},
{"id_case":"3118","class":"2","sentence1":"One of the most important resources for discourse connectives in English is the Penn Discourse Treebank ( Prasad et al., 2008 )."},
{"id_case":"3119","class":"3","sentence1":"We used standard classifiers available in scikit-learn package ( Pedregosa et al., 2011 )."},
{"id_case":"3120","class":"2","sentence1":"Rhetorical Structure Theory ( Mann and Thompson, 1988 ) belongs to the first sort."},
{"id_case":"3121","class":"2","sentence1":"Both language models use modified Kneser-Ney smoothing ( Chen and Goodman, 1996 )."},
{"id_case":"3123","class":"3","sentence1":"We train the concept identification stage using infinite ramp loss with AdaGrad ( Duchi et al., 2011 )."},
{"id_case":"3124","class":"3","sentence1":"For training the translation model and for decoding we used the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"3125","class":"3","sentence1":"We use ROUGE score as our evaluation metric ( Lin, 2004 ) with standard options 8 ."},
{"id_case":"3126","class":"3","sentence1":"For building the baseline SMT system, we used the open-source SMT toolkit Moses ( Koehn et al., 2007 ), in its standard setup."},
{"id_case":"3127","class":"3","sentence1":"We use the Europarl parallel corpus ( Koehn, 2005 ) as the bitext from which to extract the AUTO- MATIC bilingual lexicons."},
{"id_case":"3128","class":"3","sentence1":"We use Ridge Regression (RR) with l2-norm regularization and Support Vector Regression (SVR) with an RBF kernel from scikit-learn ( Pedregosa et al., 2011 )."},
{"id_case":"3129","class":"3","sentence1":"All data used in our experiments are sentence-split, lower-cased and tokenize using the CORENLP toolkit ( Manning et al., 2014 )."},
{"id_case":"3130","class":"2","sentence1":"Since the phrase table contains lemmas, the Wikipedia corpus was lemmatised using the TreeTagger ( Schmid, 1994 )."},
{"id_case":"3131","class":"3","sentence1":"For this purpose we used the logistic regression classifier from scikit-learn with L2 regularisation ( Pedregosa et al., 2011 ) 1 ."},
{"id_case":"3132","class":"3","sentence1":"All our systems are contrasted with a standard phrase-based system built with Moses ( Koehn et al., 2007 )."},
{"id_case":"3133","class":"2","sentence1":"We test our metrics in the setting of the WMT 2009 evaluation task ( Callison-Burch et al., 2009 )."},
{"id_case":"3134","class":"2","sentence1":"These classifiers have been used in related work by (Pang et al, 2002) ."},
{"id_case":"3135","class":"2","sentence1":"One is from (Turian et al, 2010) , the dimension of word embedding is 50."},
{"id_case":"3136","class":"2","sentence1":"Word alignment using GIZA++ toolkit ( Och and Ney, 2000 ), the default configuration as available in training scripts for Moses."},
{"id_case":"3137","class":"2","sentence1":"We found that using AdaGrad ( Duchi et al., 2011 ) to update the parameters is very effective ."},
{"id_case":"3138","class":"2","sentence1":"The Penn Discourse Treebank (PDTB, Prasad et al., 2008 ) is a large corpus annotated with discourse relations, (covering the Wall Street Journal part of the Penn Treebank)."},
{"id_case":"3139","class":"2","sentence1":"The system was trained on the English and Danish part of the Europarl corpus version 3 ( Koehn, 2005 )."},
{"id_case":"3140","class":"1","sentence1":"For example, Turian et al. (2010) compared Brown clusters, C&W embeddings and HLBL embeddings in NER and chunking tasks."},
{"id_case":"3141","class":"3","sentence1":"We use the Moses software package 5 to train a PBMT model ( Koehn et al., 2007 )."},
{"id_case":"3142","class":"1","sentence1":"Past experiences on this system have shown that the Random Forest ( Breiman, 2001 ) outperforms other regression algorithms (like Support Vector Regression or Multi-Layer Perceptron)."},
{"id_case":"3143","class":"3","sentence1":"We trained a number of French-English SMT systems using the Moses toolkit ( Koehn et al., 2007 ) in its default setting."},
{"id_case":"3144","class":"2","sentence1":"@BULLET Zhang2015: Zhang et al. (2015) proposed to use shallow convolutional neural networks to model two arguments respectively."},
{"id_case":"3145","class":"3","sentence1":"All data used in our experiments are sentence-split, lower-cased and tokenize using the CORENLP toolkit ( Manning et al., 2014 )."},
{"id_case":"3146","class":"2","sentence1":"The major part of data comes from current and upcoming film releases of the Europarl dataset ( Koehn, 2005 )."},
{"id_case":"3147","class":"0","sentence1":"We expect this restriction is more consistent with the ROUGE evaluation metric used for summarization ( Lin, 2004 )."},
{"id_case":"3148","class":"3","sentence1":"Additionally, we train phrase-based machine translation models with our alignments using the popular Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"3149","class":"3","sentence1":"Additionally, we train phrase-based machine translation models with our alignments using the popular Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"3150","class":"2","sentence1":"The decoder searches for the best translation given a set of models h m (e I 1 , s K 1 , f J 1 ) by maximizing the log-linear feature score ( Och and Ney, 2004 ): e I 1 = arg max I,e I 1 M m=1"},
{"id_case":"3151","class":"2","sentence1":"The training set is used to train the phrase-based translation model and language model for Moses ( Koehn et al., 2007 )."},
{"id_case":"3152","class":"2","sentence1":"The usages from the ukWaC are tokenised and lemmatised using TreeTagger ( Schmid, 1994 ), as provided by the corpus."},
{"id_case":"3153","class":"0","sentence1":"To quantify the redundancy of structures, we part-of speech tagger the English Gigaword corpus ( Graff and Cieri, 2003 )."},
{"id_case":"3154","class":"2","sentence1":"Baseline word alignments were obtained by running GIZA++ in both directions and symmetrizing using the grow-diag-final-and heuristic ( Och and Ney, 2003; )."},
{"id_case":"3155","class":"0","sentence1":"The biggest challenge in coreference resolution accounting for 42% of errors in the stateof-the-art Stanford system is the inability to reason effectively about background semantic knowledge ( Lee et al., 2013 )."},
{"id_case":"3156","class":"2","sentence1":"The results displayed in Table 3 are obtained with the SMO classifier trained using the WEKA library ( Hall et al., 2009 ) on our downloaded SemEval 2013 development and training corpora (7595 tweets)"},
{"id_case":"3157","class":"3","sentence1":"We trained a number of French-English SMT systems using the Moses toolkit ( Koehn et al., 2007 ) in its default setting."},
{"id_case":"3158","class":"1","sentence1":"The performance of our algorithm is compared with the disambiguation accuracy obtained with a variation of the Lesk algorithm 3 ( Lesk, 1986 ), which selects the meaning of an open-class word by findi"},
{"id_case":"3159","class":"3","sentence1":"We use the scikit implementation of SVM ( Pedregosa et al., 2011 )."},
{"id_case":"3160","class":"2","sentence1":"For example, Turian et al. (2010) showed that the optimal dimensionality for word embeddings is task specific ."},
{"id_case":"3161","class":"2","sentence1":"EASE uses NLTK ( Bird et al., 2009 ) for POS tagging and stemming, a spell for spell checking, and WordNet ( Fellbaum, 1998 ) to get the synonyms ."},
{"id_case":"3162","class":"2","sentence1":"SMT systems for Spanish English were trained from the Europarl v5 parallel corpus ( Koehn, 2005 ), collected from the proceedings of the European Parliament."},
{"id_case":"3163","class":"3","sentence1":"All our translation systems are based on Moses ( Koehn et al., 2007 ) and standard components for training and tuning the models."},
{"id_case":"3164","class":"2","sentence1":"The training set is used to train the phrase-based translation model and language model for Moses ( Koehn et al., 2007 )."},
{"id_case":"3165","class":"3","sentence1":"Additionally, we train phrase-based machine translation models with our alignments using the popular Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"3166","class":"3","sentence1":"We thank Gbor Recski (HAS Research Institute for Linguistics) for performing the PCA on the operators of the Socher et al. (2013) CVG."},
{"id_case":"3167","class":"2","sentence1":"It has been shown in previous work on relation extraction that the shortest dependency path between any two entities captures the information required to assert a relationship between them ( Bunescu and Mooney, 2005 )."},
{"id_case":"3168","class":"2","sentence1":"These algorithms were used to participate in the the expression level task (Subtask A) and message level task (Subtask B) of the SemEval-2014 Task 9: Sentiment Analysis in Twitter ( Rosenthal et al., "},
{"id_case":"3169","class":"2","sentence1":"Phrase-Based SMT system: a standard non factored phrase-based SMT system was built using the open source Moses toolkit ( Koehn et al., 2007 ) with parameters set similar to those of Neubig (2011),"},
{"id_case":"3171","class":"3","sentence1":"For the theory of Cho & Chai (2000) to be complete, we have proposed a new type of marker and the Adjunct LP Constraint, in conjunction with their Argument LP Constraint."},
{"id_case":"3172","class":"2","sentence1":"All linguistic annotations needed for features (POS, chunks 7 , parses) are from Stanford CoreNLP ( Manning et al., 2014 )."},
{"id_case":"3173","class":"2","sentence1":"Algorithm 5 shows a Passive-Aggressive algorithm for the structured output ( Crammer et al., 2006 )."},
{"id_case":"3174","class":"2","sentence1":"with the training script of the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"3175","class":"2","sentence1":"The task is part of the Semantic Evaluation 2012 Workshop ( Agirre et al., 2012 )."},
{"id_case":"3176","class":"3","sentence1":"We used the same annotation guidelines as Zaidan et al. (2007) ."},
{"id_case":"3177","class":"2","sentence1":"The Stanford parser 1 ( Marneffe et al., 2006 ) was used to produce all dependency parses."},
{"id_case":"3178","class":"3","sentence1":"1 After tokenization , we lemmatize and stem tweets and remove stopwords from each tweet using the NLTK toolkit ( Bird et al., 2009 )."},
{"id_case":"3179","class":"3","sentence1":"Machine translation system settings: We used a phrase-based statistical machine translation model as implemented in the Moses toolkit ( Koehn et al., 2007 ) for machine translation."},
{"id_case":"3180","class":"2","sentence1":"The phrase table is extracted from a bilingual text aligned on the word level, using e.g. GIZA ++ ( Och and Ney, 2003 )."},
{"id_case":"3181","class":"1","sentence1":"This architecture is very similar to the framework of UIMA ( Ferrucci and Lally, 2004 )."},
{"id_case":"3182","class":"3","sentence1":"Besides using SentiStrength, we use the lexicon approach proposed by ( Hu and Liu, 2004 )."},
{"id_case":"3183","class":"3","sentence1":"Run1: We firstly use the Stanford CoreNLP toolkit 3 ( Manning et al., 2014 ) to split each token for the sentence pairs in the evaluation data."},
{"id_case":"3184","class":"2","sentence1":"By setting (n inw and enw(n)=1 for all nodes, the generalized kernel can be converted to the kernel proposed in ( Collins and Duffy, 2001 )."},
{"id_case":"3185","class":"3","sentence1":"2 We tested the difference in performance for statistical significance using an approximate randomization procedure ( Yeh, 2000 ) with 10000 iterations ."},
{"id_case":"3186","class":"3","sentence1":"Run1: We firstly use the Stanford CoreNLP toolkit 3 ( Manning et al., 2014 ) to split each token for the sentence pairs in the evaluation data."},
{"id_case":"3187","class":"2","sentence1":"All linguistic annotations needed for features (POS, chunks 7 , parses) are from Stanford CoreNLP ( Manning et al., 2014 )."},
{"id_case":"3188","class":"2","sentence1":"In ( Erkan and Radev, 2004 ), the concept of graph based centrality was used to rank a set of sentences, in producing generic multi-document summaries."},
{"id_case":"3189","class":"2","sentence1":"Significance was tested using a paired bootstrap ( Koehn, 2004) with 1000 samples (p<0.02)."},
{"id_case":"3190","class":"2","sentence1":"This work has been developed in the context of dependency parsing exemplified by the CoNLL Shared Task on Dependency Parsing in years 2006 and 2007 ( Nivre et al., 2007b ), where several systems compe"},
{"id_case":"3191","class":"2","sentence1":"This work has been developed in the context of dependency parsing exemplified by the CoNLL Shared Task on Dependency Parsing in years 2006 and 2007 ( Nivre et al., 2007b ), where several systems compe"},
{"id_case":"3194","class":"3","sentence1":"For seed and test paradigms we used verbal inflectional paradigms from the CELEX morphological database ( Baayen et al., 1995 )."},
{"id_case":"3195","class":"3","sentence1":"We used KenLM ( Heafield, 2011 ) to create 3-gram language models with Kneser-Ney smoothing on the target side of the bilingual training corpora."},
{"id_case":"3196","class":"3","sentence1":"We built a 5-gram language model on the English side of QCA-train using KenLM ( Heafield, 2011 )."},
{"id_case":"3197","class":"2","sentence1":"An estimate of the likelihood of a verb taking a event subject was computed over the Annotated English Gigaword v.5 corpus ( Napoles et al., 2012 )."},
{"id_case":"3198","class":"3","sentence1":"We use the Web 1T 5-gram corpus ( Brants and Franz, 2006 ) to compute the language model score for a sentence."},
{"id_case":"3199","class":"3","sentence1":"Machine translation system settings: We used a phrase-based statistical machine translation model as implemented in the Moses toolkit ( Koehn et al., 2007 ) for machine translation."},
{"id_case":"3200","class":"3","sentence1":"Machine translation system settings: We used a phrase-based statistical machine translation model as implemented in the Moses toolkit ( Koehn et al., 2007 ) for machine translation."},
{"id_case":"3701","class":"0","sentence1":"A chunk is a minimal , non-recursive structure consisting of correlated groups of words ( Bharati et al., 2006 )."},
{"id_case":"3702","class":"2","sentence1":"MC-30: A subset of RG-65 dataset with 30 word pairs ( Miller and Charles, 1991 )."},
{"id_case":"3703","class":"3","sentence1":"We obtained news-peg judgments using the Brat annotation tool ( Stenetorp et al., 2012 ) from two annotators 3 ."},
{"id_case":"3704","class":"3","sentence1":"We transformed the parse trees in OntoNotes into syntactic dependencies using Stanford CoreNLP ( Manning et al., 2014 )."},
{"id_case":"3705","class":"2","sentence1":"Gradient clipping heuristic to prevent the \" exploding gradient \" problem ( Graves, 2013 )."},
{"id_case":"3706","class":"3","sentence1":"We obtained news-peg judgments using the Brat annotation tool ( Stenetorp et al., 2012 ) from two annotators 3 ."},
{"id_case":"3707","class":"3","sentence1":"We then describe in more detail a modern Chinese corpus, the Penn Chinese Treebank ( Xue et al., 2005 )."},
{"id_case":"3708","class":"3","sentence1":"In order to assess statistical significance of the obtained results, we use the paired bootstrap resampling method ( Koehn, 2004 ) which estimates the probability (p-value) that a measured difference "},
{"id_case":"3709","class":"2","sentence1":"Barrachina et al., (Barrachina et al., 2009 ) presented a prefix-based ITP approach in which the user is assumed to proof-read each automatic translation correcting each time the first error, if any, "},
{"id_case":"3710","class":"2","sentence1":"Statistical significance in BLEU score difference was measured by using paired bootstrap re-sampling ( Koehn, 2004 )."},
{"id_case":"3711","class":"2","sentence1":"The article system builds on the elements of the system described in ( Rozovskaya and Roth, 2010c )."},
{"id_case":"3712","class":"3","sentence1":"For calculating the required frequencies, we use the Web1T corpus 6 ( Brants and Franz, 2006 )."},
{"id_case":"3713","class":"2","sentence1":"Phrase pairs were extracted from symmetrized word alignments and distortions generated by GIZA++ ( Och and Ney, 2003 ) using the combination of heuristics \" grow-diagfinal-and \" and \" msd-bidirectional"},
{"id_case":"3714","class":"3","sentence1":"We ran all of our experiments in Weka ( Hall et al., 2009 ) using Logistic Regression."},
{"id_case":"3715","class":"2","sentence1":"Both corpora were extracted from the open parallel corpus OPUS ( Tiedemann, 2012 )."},
{"id_case":"3716","class":"3","sentence1":"We ran all of our experiments in Weka ( Hall et al., 2009 ) using Logistic Regression."},
{"id_case":"3717","class":"2","sentence1":"The Spanish-English (S2E) training corpus was drawn from the Europarl collection ( Koehn, 2005 )."},
{"id_case":"3718","class":"2","sentence1":"Type System extends the type system that is built into the UIMA 6 framework ( Ferrucci and Lally, 2004 )."},
{"id_case":"3719","class":"3","sentence1":"For other languages we use the corpora made available for the CoNLL-X Shared Task ( Buchholz and Marsi, 2006 )."},
{"id_case":"3720","class":"1","sentence1":"Table 5 compares our reordering model with a reimplementation of the reordering model proposed in ( Tromble and Eisner, 2009 )."},
{"id_case":"3721","class":"1","sentence1":"These results contradict those given in Zelenko et al. (2003) , where the sparse kernel achieves 2-3% better F1 performance than the contiguous kernel."},
{"id_case":"3722","class":"2","sentence1":"For example, OntoNotes ( Hovy et al., 2006 ), a large-scale annotation project, chose this option."},
{"id_case":"3723","class":"3","sentence1":"As for EJ translation, we use the Stanford parser ( de Marneffe et al., 2006 ) to obtain English abstraction trees."},
{"id_case":"3724","class":"3","sentence1":"For both English and German we used the part-of speech tagger TreeTagger ( Schmid, 1994 ) to obtain POS-tags."},
{"id_case":"3725","class":"3","sentence1":"We train our model on a subset of the WaCkypedia EN 6 corpus ( Baroni et al., 2009 )."},
{"id_case":"3726","class":"2","sentence1":"The questions are translated using a phrase-based system built using Moses ( Koehn et al., 2007 ) (the Mo set)."},
{"id_case":"3727","class":"2","sentence1":"The questions are translated using a phrase-based system built using Moses ( Koehn et al., 2007 ) (the Mo set)."},
{"id_case":"3728","class":"3","sentence1":"We measure statistical significance using 95% confidence intervals computed with paired bootstrap resampling ( Koehn, 2004 )."},
{"id_case":"3729","class":"3","sentence1":"As for EJ translation, we use the Stanford parser ( de Marneffe et al., 2006 ) to obtain English abstraction trees."},
{"id_case":"3730","class":"2","sentence1":"Words were downcased and lemmatized using the WordNet lemmatizer in the NLTK 2 toolkit ( Bird et al., 2009 )."},
{"id_case":"3731","class":"2","sentence1":"English sentences are parsed into dependency structures by Stanford parser ( Marneffe et al., 2006 )."},
{"id_case":"3732","class":"3","sentence1":"Both of our systems were based on the Moses decoder ( Koehn et al., 2007 )."},
{"id_case":"3733","class":"3","sentence1":"We also used ANEW ( Bradley and Lang, 1999 ) for bootstrapping the affective lexicon expansion process."},
{"id_case":"3734","class":"2","sentence1":"translation at the DiscoMT 2015 workshop ( Hardmeier et al., 2015 )."},
{"id_case":"3735","class":"0","sentence1":"Establishing and maintaining common ground is a complicated process, even for human interlocutors ( Clark, 1996 )."},
{"id_case":"3736","class":"3","sentence1":"SentiWordNet score (senti) We used the Senti- WordNet ( Baccianella et al., 2010 ) lexical resource to assign scores for each word based on three sentiments i.e positive, negative and objective respec"},
{"id_case":"3737","class":"2","sentence1":"Europarl 2 ( Koehn, 2005 ): it is a corpus of parallel texts in 11 languages from the proceedings of the European Parliament ."},
{"id_case":"3738","class":"3","sentence1":"We also carried out a chunk-reordering PB-SMT experiment where the chunks are reordered based on the final alignments obtained by 1-pass experiment of Holmqvist et al. (2012) ."},
{"id_case":"3739","class":"2","sentence1":"WSI is generally considered as an unsupervised clustering task under the distributional hypothesis ( Harris, 1954 ) that the word meaning is reflected by the set of contexts in which it appears."},
{"id_case":"3740","class":"2","sentence1":"The data used for the experiments described in this paper comes predominantly from Bible translations , Wikipedia, and the Europarl corpus of European parliamentary proceedings ( Koehn, 2005 )."},
{"id_case":"3741","class":"3","sentence1":"In a second step, we applied Pointwise Mutual Information ( Church and Hanks, 1990 ) as a weighting function to discover informative semantic similarity relations between words."},
{"id_case":"3743","class":"3","sentence1":"For the language model, we used the KenLM toolkit ( Heafield, 2011 ) to create a 5-gram language model on the target side of the Europarl corpus (v7) with approximately 54M tokens with Kneser-Ney smoo"},
{"id_case":"3744","class":"2","sentence1":"First used by Blitzer et al. (2007) , the MDS dataset contains 4 different types of product reviews taken from Amazon.com including books, DVDs, electronics and kitchen appliances, with 1000 positive "},
{"id_case":"3745","class":"2","sentence1":"It is based on Incremental Sigmoid Belief Networks (ISBNs), a class of directed graphical model for structure prediction problems recently proposed in ( Titov and Henderson, 2007 ), where they were de"},
{"id_case":"3746","class":"0","sentence1":"In the Penn Discourse Treebank ( Prasad et al., 2008 ), which uses multiple labels as a last resort when annotators cannot reach an agreement or feel that an instance is inherently ambiguous, 5.5% of "},
{"id_case":"3747","class":"2","sentence1":"We evaluated the validity of our model through experiments on a disambiguation task of parsing the Penn Treebank ( Marcus et al., 1994 ) with an automatically acquired LTAG grammar."},
{"id_case":"3748","class":"2","sentence1":"The most famous example would probably be the Europarl corpus ( Koehn, 2005 )."},
{"id_case":"3749","class":"3","sentence1":"For language modeling, we trained a separate 5-gram Kneser-Ney smoothed LM model on the target (i.e. English) side of the training bi-text using KenLM ( Heafield, 2011 )."},
{"id_case":"3750","class":"2","sentence1":"They used the Web-based annotation tool brat ( Stenetorp et al., 2012 ) for the annotation ."},
{"id_case":"3751","class":"2","sentence1":"The classification was conducted, using different Scikit-learn algorithms ( Pedregosa et al., 2011 )."},
{"id_case":"3752","class":"2","sentence1":"The Europarl corpus ( Koehn, 2005 ) is built from the proceedings of the European Parliament."},
{"id_case":"3753","class":"3","sentence1":"For language modeling, we trained a separate 5-gram Kneser-Ney smoothed LM model on the target (i.e. English) side of the training bi-text using KenLM ( Heafield, 2011 )."},
{"id_case":"3754","class":"2","sentence1":"The Spanish-English (S2E) training corpus was drawn from the Europarl collection ( Koehn, 2005 )."},
{"id_case":"3756","class":"3","sentence1":"We use the Stanford CoreNLP caseless tagger for part-of-speech tagging ( Manning et al., 2014 )."},
{"id_case":"3757","class":"3","sentence1":"All of our models are trained using Nematus ( Sennrich et al., 2017 )."},
{"id_case":"3758","class":"3","sentence1":"We use GIZA++ ( Och and Ney, 2003 ) with its default parameters to produce phrase alignments."},
{"id_case":"3759","class":"2","sentence1":"We are working with standard tools as DISSECT ( Dinu et al., 2013 )."},
{"id_case":"3760","class":"2","sentence1":"They are based on Distributional Hypothesis which works under the assumption that similar words occur in similar contexts ( Harris, 1954 )."},
{"id_case":"3761","class":"2","sentence1":"Its segmentation model is a class-based hidden Markov model (HMM) model ( Zhang et al., 2003 )."},
{"id_case":"3762","class":"2","sentence1":"The edit distance kernel was trained with LIBSVM ( Chang and Lin, 2011 )."},
{"id_case":"3763","class":"2","sentence1":"The corpora are first tokenized and lowercased using the Moses scripts, then lemmatized and tagged by part-of-speech (PoS) using the TreeTagger ( Schmid, 1994 )."},
{"id_case":"3764","class":"2","sentence1":"For example, OntoNotes ( Hovy et al., 2006 ), a large-scale annotation project, chose this option."},
{"id_case":"3766","class":"3","sentence1":"We use the open-source Moses toolkit ( Koehn et al., 2007 ) to build a standard phrase-based SMT system which extracts up to 8 words phrases in the Moses phrase table."},
{"id_case":"3767","class":"3","sentence1":"In our experiments, we use the LIBLINEAR package ( Fan et al., 2008 ) to solve the primal problem with L 2 -regularization and L 2 -loss."},
{"id_case":"3768","class":"2","sentence1":"The Spanish-English (S2E) training corpus was drawn from the Europarl collection ( Koehn, 2005 )."},
{"id_case":"3769","class":"2","sentence1":"The ICSI meeting corpus ( Janin et al., 2003 ) is a corpus of text transcripts of research meetings."},
{"id_case":"3770","class":"3","sentence1":"We use ROUGE ( Lin, 2004 ) for evaluating the content of summaries."},
{"id_case":"3771","class":"2","sentence1":"The measure selected is the normalised Pearson correlation ( Agirre et al., 2012 )."},
{"id_case":"3772","class":"3","sentence1":"The main corpora we use are Europarl ( Koehn, 2005 ) and the Canadian Hansard."},
{"id_case":"3773","class":"3","sentence1":"We exploit this monolingual data for training as described in ( Sennrich et al., 2016a )."},
{"id_case":"3774","class":"3","sentence1":"We use the Stanford CoreNLP caseless tagger for part-of-speech tagging ( Manning et al., 2014 )."},
{"id_case":"3775","class":"2","sentence1":"This is a corpus-based metric relying on the distributional hypothesis of meaning suggesting that similarity of context implies similarity of meaning (Z. Harris, 1954 )."},
{"id_case":"3776","class":"2","sentence1":"Similarly, (Turian et al, 2010) evaluated three different word representations on NER and Chunking, and concluded that unsupervised word representations improved NER and Chunking."},
{"id_case":"3777","class":"1","sentence1":"3.3.1 Reference System We compare a number of analogical devices to the state-of-the-art statistical translation engine Moses ( Koehn et al., 2007 )."},
{"id_case":"3778","class":"3","sentence1":"For training SVM classifiers we used LIBSVM package ( Chang and Lin, 2001 )."},
{"id_case":"3779","class":"2","sentence1":"The word alignment was trained using GIZA++ ( Och and Ney, 2003 ) with the configuration grow-diag-final-and."},
{"id_case":"3780","class":"2","sentence1":"The n-gram (2 n 5) searches are performed on the Google Web 1T corpus ( Brants and Franz, 2006 ), and the number of hits is weighted by the length of the n-gram search (such that longer sequences "},
{"id_case":"3781","class":"2","sentence1":"The corpus is first word-aligned using a word alignment heuristic ( Och and Ney, 2003 )."},
{"id_case":"3782","class":"3","sentence1":"We perform bootstrap resampling with bounds estimation as described in ( Koehn, 2004 )."},
{"id_case":"3784","class":"3","sentence1":"We perform bootstrap resampling with bounds estimation as described by ( Koehn, 2004 )."},
{"id_case":"3785","class":"2","sentence1":"Markov Logic Networks (MLN) ( Richardson and Domingos, 2006 ) is adopted for learning and predication."},
{"id_case":"3786","class":"2","sentence1":"with the training script of the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"3787","class":"3","sentence1":"We train for 15 epochs using mini-batch stochastic gradient descent, the Adadelta update rule ( Zeiler, 2012 ), and early stopping."},
{"id_case":"3788","class":"2","sentence1":"This confirms the finding of Liu et al. (2012) that language model and lexicalized reordering models only have modest effects on translation retrieval."},
{"id_case":"3789","class":"3","sentence1":"For this purpose we use the Europarl corpus ( Koehn, 2005 )."},
{"id_case":"3790","class":"3","sentence1":"We exploit this monolingual data for training as described in ( Sennrich et al., 2016a )."},
{"id_case":"3791","class":"1","sentence1":"Our decoder is a stack decoder similar to Koehn et al. (2003) ."},
{"id_case":"3792","class":"3","sentence1":"For the English- Spanish and French-English systems, we used parallel training data from the Europarl and News Commentary corpora, as well as the TED corpus ( Cettolo et al., 2012 )."},
{"id_case":"3793","class":"3","sentence1":"We translated the English side of PCEDT 5 to Czech using SMT (we chose the Moses system ( Koehn et al., 2007 ) for our experiments) and tagged the resulting translations using the More tagger ( Spous"},
{"id_case":"3794","class":"2","sentence1":"The word alignment was trained using GIZA++ ( Och and Ney, 2003 ) with the configuration grow-diag-final-and."},
{"id_case":"3795","class":"2","sentence1":"Rhetorical Structure Theory (RST) ( Mann and Thompson, 1988 ) represents the coherence structure of a text by a labeled tree, called discourse tree (DT) as shown in Figure 1 ."},
{"id_case":"3796","class":"3","sentence1":"The dataset used for the experiments reported in this paper has been prepared by Reyes et al. (2013) ."},
{"id_case":"3797","class":"2","sentence1":"A good data source for this is the Europarl Corpus ( Koehn, 2005 )."},
{"id_case":"3798","class":"2","sentence1":"The most famous example would probably be the Europarl corpus ( Koehn, 2005 )."},
{"id_case":"3800","class":"2","sentence1":"Previously (Socher et al, 2011)  used a recursive autoencoder to similarly obtain a vector representation of each sentence, again combining other lexical similarity features to improve the results."},
{"id_case":"4302","class":"2","sentence1":"The first source is the CoNLL 2003 shared task date (Tjong Kim Sang and De Meulder, 2003 ) and the second source is the 2004 NIST Automatic Content Extraction ( Weischedel, 2004 )."},
{"id_case":"4303","class":"3","sentence1":"We used the Moses toolkit ( Koehn et al., 2007 ) to build a phrase based machine translation system with a traditional 5-gram LM trained on the target side of our bitext."},
{"id_case":"4304","class":"2","sentence1":"The submitted summaries were evaluated using ROUGE metric ( Lin, 2004 (Bulgarian, German, Greek, English and Romanian) Nevertheless, the results are encouraging for this complex system (s1 is the id o"},
{"id_case":"4305","class":"2","sentence1":"We therefore propose an alternative method based on \" correlation matrices \" computed from the BLEU performance measure ( Papineni et al., 2001 )."},
{"id_case":"4306","class":"3","sentence1":"We used the implementation of the scikit-learn 2 module ( Pedregosa et al., 2011 )."},
{"id_case":"4307","class":"2","sentence1":"Finally, the CoNLL-2003 shared task (Tjong Kim Sang and De Meulder, 2003 ) is an Figure 1 : Search results for the queries Michael Jackson and Michael Jackson footballer."},
{"id_case":"4308","class":"3","sentence1":"Further, we sentence-split, tokenized, and lemmatized the text in Wikipedia and Gigaword using Stanford CoreNLP Toolkit 3.6.0 ( Manning et al., 2014 )."},
{"id_case":"4309","class":"2","sentence1":"The corpora are first tokenized and lowercased using the Moses scripts, then lemmatized and tagged by part-of-speech (PoS) using the TreeTagger ( Schmid, 1994 )."},
{"id_case":"4310","class":"3","sentence1":"We use Adadelta ( Zeiler, 2012 ) to update the parameters during training."},
{"id_case":"4311","class":"1","sentence1":"The first source is the CoNLL 2003 shared task date (Tjong Kim Sang and De Meulder, 2003 ) and the second source is the 2004 NIST Automatic Content Extraction ( Weischedel, 2004 )."},
{"id_case":"4313","class":"3","sentence1":"We used a 2009 snapshot of Wikipedia, 2 which was PoS tagged and lemmatized using the TreeTagger ( Schmid, 1994 )."},
{"id_case":"4314","class":"2","sentence1":"Recently, Hovy et al. (2013) utilized word embeddings by (Collobert et al, 2011) for capturing coherence and contextual features for supervised metaphor detection."},
{"id_case":"4316","class":"2","sentence1":"Finally, the CoNLL-2003 shared task (Tjong Kim Sang and De Meulder, 2003 ) is an Figure 1 : Search results for the queries Michael Jackson and Michael Jackson footballer."},
{"id_case":"4317","class":"3","sentence1":"We run our experiments on Europarl ( Koehn, 2005 ), a multilingual parallel corpus extracted from the proceedings of the European Parliament."},
{"id_case":"4318","class":"3","sentence1":"We applied bootstrap resampling ( Koehn, 2004 ) to measure statistical significance , p < 0.05, of our models compared to a baseline."},
{"id_case":"4319","class":"3","sentence1":"We used the Moses toolkit ( Koehn et al., 2007 ) with its default settings."},
{"id_case":"4320","class":"2","sentence1":"The statistical significance test is also carried out with paired bootstrap resampling method with p < 0.001 intervals ( Koehn, 2004 )."},
{"id_case":"4321","class":"2","sentence1":"This model is motivated by Vector Space Model ( Salton et al., 1975 )."},
{"id_case":"4322","class":"2","sentence1":"The data was segmented into baseNP parts and nonbaseNP parts in a similar fashion as the data used by ( Ramshaw and Marcus, 1995 )."},
{"id_case":"4323","class":"3","sentence1":"Parameter optimization is done using Adagrad ( Duchi et al., 2011 ) with a mini-batch size of 100 and a learning rate = 0.1, which we found to work well on held-out data."},
{"id_case":"4324","class":"2","sentence1":"The NJU-Parser is based on the state-of-the art MAT Parser ( McDonald, 2006 )."},
{"id_case":"4325","class":"2","sentence1":"One is a 3-gram language model built using KenLM ( Heafield, 2011 ) and trained over a modified version of the annotated corpus in which every it is concatenated with its type (e.g. it event)."},
{"id_case":"4326","class":"2","sentence1":"The word alignment was obtained by running Giza++ ( Och and Ney, 2003 )."},
{"id_case":"4327","class":"2","sentence1":"Translations for English words in the lexical sample are extracted from a semi-automatic word alignment of sentences from the Europarl parallel corpus ( Koehn, 2005 )."},
{"id_case":"4328","class":"2","sentence1":"Translations for English words in the lexical sample are extracted from a semi-automatic word alignment of sentences from the Europarl parallel corpus ( Koehn, 2005 )."},
{"id_case":"4330","class":"2","sentence1":"Also annotated in the PDTB is the attribution of each discourse relation and of its arguments ( Prasad et al., 2007 )."},
{"id_case":"4331","class":"3","sentence1":"All the Language Models (LM) used in our experiments are 5-grams modified Kneser-Ney smoothed LMs trained using KenLM ( Heafield et al., 2013 )."},
{"id_case":"4332","class":"3","sentence1":"In all experiments, we use the SVM classifier with sequential minimal optimization (SMO) implementation available in the Weka package ( Hall et al., 2009 )."},
{"id_case":"4333","class":"3","sentence1":"We use the Stanford parser to generate a DG for each sentence ( de Marneffe et al., 2006 )."},
{"id_case":"4334","class":"3","sentence1":"Preprocessing: We tokenized the English side of all bitexts for language modeling using the standard tokenizer of the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"4336","class":"3","sentence1":"We use the Moses software package 5 to train a PBMT model ( Koehn et al., 2007 )."},
{"id_case":"4337","class":"3","sentence1":"In-domain SMT: we used the parallel corpus (Section 3) to train an en-pt phrase-based SMT system using the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"4338","class":"3","sentence1":"In-domain SMT: we used the parallel corpus (Section 3) to train an en-pt phrase-based SMT system using the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"4339","class":"2","sentence1":"The first competitive learning based system is described in (Soon et al, 2001) ."},
{"id_case":"4340","class":"3","sentence1":"In-domain SMT: we used the parallel corpus (Section 3) to train an en-pt phrase-based SMT system using the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"4341","class":"3","sentence1":"We used predicted collapsed Stanford dependencies ( de Marneffe et al., 2006 ) to extract arguments of the verbs, and used only a subset of dependents of a verb."},
{"id_case":"4342","class":"3","sentence1":"We select as a general-purpose corpus Europarl v7 ( Koehn, 2005 ), with 1.97M parallel sentences."},
{"id_case":"4343","class":"3","sentence1":"We used the lexicalized dependency parser in the Stanford Statistical Natural Language Parser (ver.2.0.3) ( de Marneffe et al., 2006 ) to obtain parses for the data sets."},
{"id_case":"4344","class":"2","sentence1":"The result of the operation is equivalent to weighted composition of the lexicon and the weighted intersection of the rules as defined in ( Allauzen et al., 2007 )."},
{"id_case":"4345","class":"2","sentence1":"The distributional hypothesis of meaning ( Harris, 1954 ) is a widely-used approach for estimating term similarity."},
{"id_case":"4346","class":"3","sentence1":"Preprocessing: We tokenized the English side of all bitexts for language modeling using the standard tokenizer of the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"4347","class":"2","sentence1":"One is from (Turian et al, 2010) , the dimension of word embedding is 50."},
{"id_case":"4348","class":"3","sentence1":"The thresholds were thoroughly selected depending on our analysis for the WordNet hierarchy and semantic similarity measures ( Pedersen et al., 2004 )."},
{"id_case":"4349","class":"3","sentence1":"In-domain SMT: we used the parallel corpus (Section 3) to train an en-pt phrase-based SMT system using the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"4350","class":"2","sentence1":"The statistical significance test is also carried out with paired bootstrap resampling method with p < 0.001 intervals ( Koehn, 2004 )."},
{"id_case":"4351","class":"3","sentence1":"We use the Moses software package 5 to train a PBMT model ( Koehn et al., 2007 )."},
{"id_case":"4352","class":"3","sentence1":"We used a 2009 snapshot of Wikipedia, 2 which was PoS tagged and lemmatized using the TreeTagger ( Schmid, 1994 )."},
{"id_case":"4353","class":"2","sentence1":"The text was pre-processed using wp2txt 6 to remove markup, and then tokenized with the Stanford tokenizer ( Manning et al., 2014 )."},
{"id_case":"4354","class":"3","sentence1":"We run our experiments on Europarl ( Koehn, 2005 ), a multilingual parallel corpus which is described in detail in Section 3.1."},
{"id_case":"4355","class":"3","sentence1":"We use Scikit-learn ( Pedregosa et al., 2011 ), the machine learning library for Python, for implementing the different approaches."},
{"id_case":"4356","class":"2","sentence1":"The way they were added is similar to incorporating the negation effect described by Pang et al. (2002) ."},
{"id_case":"4358","class":"3","sentence1":"We used the lexicalized dependency parser in the Stanford Statistical Natural Language Parser (ver.2.0.3) ( de Marneffe et al., 2006 ) to obtain parses for the data sets."},
{"id_case":"4359","class":"3","sentence1":"We modified the implementation of the SWELL Java package 4 of (Dhillon et al. 2015) ."},
{"id_case":"4360","class":"2","sentence1":"The Stanford dependency parser ( De Marneffe et al., 2006 ) is used for extracting features from the dependency parse trees."},
{"id_case":"4361","class":"2","sentence1":"The first work on this topic was done back in the eighties ( Church, 1988 )."},
{"id_case":"4362","class":"2","sentence1":"The OpenFst library is used to perform all of the WFST operations ( Allauzen et al., 2007 )."},
{"id_case":"4363","class":"3","sentence1":"For our experiments, we used translated movie subtitles from the OPUS corpus ( Tiedemann, 2009b )."},
{"id_case":"4364","class":"2","sentence1":"The Moses15 result is obtained by applying the SMT toolkit Moses ( Koehn et al., 2007 ) over letter strings with 15-character context windows."},
{"id_case":"4365","class":"2","sentence1":"The OpenFst library is used to perform all of the WFST operations ( Allauzen et al., 2007 )."},
{"id_case":"4366","class":"3","sentence1":"We select as a general-purpose corpus Europarl v7 ( Koehn, 2005 ), with 1.97M parallel sentences."},
{"id_case":"4367","class":"1","sentence1":"We also compare our word embeddings with the Eigen word embeddings of Dhillon et al. (2015)  without any prior knowl- edge."},
{"id_case":"4368","class":"2","sentence1":"The relationship between language and sentiment is an active area of investigation ( Pang and Lee, 2008 )."},
{"id_case":"4371","class":"2","sentence1":"Support vector machines (SVM) are one of the binary classifiers based on maximum margin strategy introduced by Vapnik ( Vapnik, 1995 )."},
{"id_case":"4372","class":"3","sentence1":"In Barankov and Tamchyna (2014), we experimented with targeted paraphrasing using the freely available SMT system Moses ( Koehn et al., 2007 )."},
{"id_case":"4373","class":"3","sentence1":"To learn the parameters of the model we minimize the cross-entropy loss as the training objective using the Adam Optimization algorithm ( Kingma and Ba, 2014 )."},
{"id_case":"4374","class":"3","sentence1":"For training the translation model and for decoding we used the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"4375","class":"2","sentence1":"The text was pre-processed using wp2txt 6 to remove markup, and then tokenized with the Stanford tokenizer ( Manning et al., 2014 )."},
{"id_case":"4376","class":"2","sentence1":"The task reported on here is to produce PropBank ( Kingsbury and Palmer, 2002 ) labels, given the features provided for the CoNLL-2005 closed task (Carreras and M` arquez, 2005)."},
{"id_case":"4377","class":"2","sentence1":"For both systems, the used training data is from the 4th version of the Europarl Corpus ( Koehn, 2005 ) and the News Commentary corpus."},
{"id_case":"4378","class":"3","sentence1":"We use the Stanford parser to generate a DG for each sentence ( de Marneffe et al., 2006 )."},
{"id_case":"4379","class":"3","sentence1":"We used predicted collapsed Stanford dependencies ( de Marneffe et al., 2006 ) to extract arguments of the verbs, and used only a subset of dependents of a verb."},
{"id_case":"4380","class":"3","sentence1":"We tokenize English data and segment Chinese data using the Stanford CoreNLP toolkit ( Manning et al., 2014 )."},
{"id_case":"4381","class":"2","sentence1":"The evaluation emphasis in multi-document summarization has been on evaluating content (not readability ), using manual () as well as automatic ( Lin and Hovy, 2003 ) methods."},
{"id_case":"4382","class":"3","sentence1":"We conducted baseline experiments for phrase based machine translation using the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"4383","class":"2","sentence1":"The similarity function is here the Smoothed Partial Tree Kernel (SPTK) proposed in ( Croce et al., 2011 )."},
{"id_case":"4384","class":"3","sentence1":"We tokenize English data and segment Chinese data using the Stanford CoreNLP toolkit ( Manning et al., 2014 )."},
{"id_case":"4385","class":"2","sentence1":"The Stanford dependency parser ( De Marneffe et al., 2006 ) is used for extracting features from the dependency parse trees."},
{"id_case":"4386","class":"3","sentence1":"To demonstrate the effect of the proposed method, we use the state-of-the-art phrase-based system and hierarchical phrase-based system implemented in Moses ( Koehn et al., 2007 )."},
{"id_case":"4387","class":"2","sentence1":"The baseline systems are built with the opensource phrase-based SMT toolkit Moses ( Koehn et al., 2007 )."},
{"id_case":"4388","class":"1","sentence1":"3.3.1 Reference System We compare a number of analogical devices to the state-of-the-art statistical translation engine Moses ( Koehn et al., 2007 )."},
{"id_case":"4389","class":"3","sentence1":"Preprocessing: We tokenized the English side of all bitexts for language modeling using the standard tokenizer of the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"4391","class":"1","sentence1":"Finally, it is also noticeable that the percentage of compounds detected in the training set is similar to the one reported by Baroni et al. (2002) and referenced to in Section 2."},
{"id_case":"4392","class":"3","sentence1":"As a learning algorithm we adopt a ranking SVM ( Joachims, 2002 ), which is an instance of preference learning."},
{"id_case":"4393","class":"3","sentence1":"Brain images are quite noisy, so we used the methodology from Mitchell et al. (2008) to select the most stable brain image features for each of the 18 participants."},
{"id_case":"4394","class":"2","sentence1":"The training set is used to train the phrase-based translation model and language model for Moses ( Koehn et al., 2007 )."},
{"id_case":"4396","class":"2","sentence1":"The work presented in Berger et al. (1996) that is based on the A * concept, however, introduces word reordering restrictions in order to reduce the overall search space."},
{"id_case":"4397","class":"2","sentence1":"Word alignment was performed using GIZA++ ( Och and Ney, 2003 ) in both directions, the grow-diag-final-and heuristic was used to symmetrize the alignments, and a max phrase length of 7 was used for p"},
{"id_case":"4398","class":"3","sentence1":"Our second method is based on the recurrent neural network language model (RNNLM) approach to learning word embeddings of Mikolov et al. (2013a) and Mikolov et al. (2013b) , using the WORD2VEC package"},
{"id_case":"4399","class":"0","sentence1":"The SUSANNE Corpus is a modified and condensed version of Brown Corpus ( Francis and Kucera, 1979 )In order to avoid the errors introduced by tagger, the SUSANNE corpus is used as the training and tes"},
{"id_case":"4400","class":"2","sentence1":"The corpora are tokenised and truecased using scripts from the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"4901","class":"2","sentence1":"English annotations were all produced using the Stanford Core- NLP toolkit ( Manning et al., 2014 )."},
{"id_case":"4903","class":"2","sentence1":"The parameters are estimated by Gibbs sampling, using the Mallet implementation ( McCallum, 2002 )."},
{"id_case":"4904","class":"2","sentence1":"The meta-classifier is a linear SVM ( Fan et al., 2008 ) implemented with KeLP."},
{"id_case":"4905","class":"3","sentence1":"We use Adadelta ( Zeiler, 2012 ) to update the parameters during training."},
{"id_case":"4907","class":"2","sentence1":"We train a Support Vector Machine (SVM) ( Cortes and Vapnik, 1995 ) on the tweets provided for training."},
{"id_case":"4908","class":"2","sentence1":"Parameter tuning is carried out using Z- MERT ( Zaidan, 2009 )."},
{"id_case":"4909","class":"2","sentence1":"Brin identifies the use of patterns in the discovery of relations on the web ( Brin, 1998 )."},
{"id_case":"4910","class":"3","sentence1":"We adopt the setting of Socher et al. (2012) ."},
{"id_case":"4912","class":"2","sentence1":"The publicly available tool GIZA++ was used to align the letters ( Och and Ney, 2003 )."},
{"id_case":"4913","class":"2","sentence1":"CRFSuite implementation ( Okazaki, 2007 )."},
{"id_case":"4914","class":"3","sentence1":"The HMM classifier used in our experiments follows the algorithm described in ( Bikel et al., 1999 )."},
{"id_case":"4915","class":"3","sentence1":"We computed 4-gram LMs with modified Kneser-Ney smoothing ( Kneser and Ney, 1995 )."},
{"id_case":"4916","class":"3","sentence1":"Socher et al. (2011) come closest to our target problem."},
{"id_case":"4917","class":"3","sentence1":"We use Ridge Regression (RR) with l2-norm regularization and Support Vector Regression (SVR) with an RBF kernel from scikit-learn ( Pedregosa et al., 2011 )."},
{"id_case":"4918","class":"3","sentence1":"As for the former (hereafter it is referred to synPth), we continue to use a dependency version of the pruning algorithm of ( Xue and Palmer, 2004 )."},
{"id_case":"4920","class":"2","sentence1":"15 The significance tests were performed using the bootstrap resampling method ( Koehn, 2004 )."},
{"id_case":"4921","class":"3","sentence1":"In this work, we focus on learning with Support Vector Machines (SVMs) ( Vapnik, 1995 )."},
{"id_case":"4922","class":"3","sentence1":"We used TreeTagger ( Schmid, 1994 ) to obtain a lemma-tag pair for each Russian word."},
{"id_case":"4923","class":"0","sentence1":"To account for this constraint,  include information from latent semantic analysis ( Deerwester et al., 1990 )."},
{"id_case":"4924","class":"2","sentence1":"The system was trained on the English and Danish part of the Europarl corpus version 3 ( Koehn, 2005 )."},
{"id_case":"4925","class":"3","sentence1":"We use linear SVMs from LIBLINEAR and SVMs with RBF kernel from LIBSVM ( Chang and Lin, 2011 )."},
{"id_case":"4926","class":"2","sentence1":"In current phrase-based statistical machine translation systems such as Moses 1 ( Koehn et al., 2007 ), the translation model is defined in terms of phrase pairs (biphrases) extracted from a bilingual"},
{"id_case":"4927","class":"3","sentence1":"For this purpose, we use the Moses toolkit to build a phrase-based statistical MT system ( Koehn et al., 2007 ), with training data from the translation task of the WMT 2013 workshop ( Bojar et al., 2"},
{"id_case":"4928","class":"2","sentence1":"We train a Support Vector Machine (SVM) ( Cortes and Vapnik, 1995 ) on the tweets provided for training."},
{"id_case":"4929","class":"2","sentence1":"The word alignment was obtained by running Giza++ ( Och and Ney, 2003 )."},
{"id_case":"4930","class":"3","sentence1":"For training SVM classifiers we used LIBSVM package ( Chang and Lin, 2001 )."},
{"id_case":"4931","class":"3","sentence1":"We use the standard alignment tool Giza++ ( Och and Ney, 2003 ) to word align the parallel data."},
{"id_case":"4932","class":"3","sentence1":"We use the standard alignment tool Giza++ ( Och and Ney, 2003 ) to word align the parallel data."},
{"id_case":"4933","class":"2","sentence1":"In the Penn Discourse TreeBank 2.0 ( Prasad et al., 2008 ), the sense is called CHOSEN ALTERNATIVE."},
{"id_case":"4934","class":"2","sentence1":"Weka ( Hall et al., 2009 ) was used to apply learning methods to extracted features."},
{"id_case":"4935","class":"3","sentence1":"For our purposes we used the Maximum Entropy algorithm implemented as part of the Mallet machine learning package ( McCallum, 2002 ) for its competitive training time and performance tradeoff."},
{"id_case":"4936","class":"3","sentence1":"Additionally, we train phrase-based machine translation models with our alignments using the popular Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"4937","class":"3","sentence1":"We used the Moses toolkit ( Koehn et al., 2007 ) with its default settings."},
{"id_case":"4938","class":"3","sentence1":"We use the standard alignment tool Giza++ ( Och and Ney, 2003 ) to word align the parallel data."},
{"id_case":"4939","class":"3","sentence1":"We used the GIZA++ software ( Och and Ney, 2003 ) to do the word alignments."},
{"id_case":"4940","class":"2","sentence1":"The language model used is the 5-gram corpus from Google Books ( Michel et al., 2011 )."},
{"id_case":"4941","class":"2","sentence1":"English annotations were all produced using the Stanford Core- NLP toolkit ( Manning et al., 2014 )."},
{"id_case":"4942","class":"2","sentence1":"For example, (Turian et al, 2010) compared Brown clusters, C&W embeddings and HLBL embeddings in NER and chunking tasks."},
{"id_case":"4944","class":"2","sentence1":"(Li et al, 2013)  use crowdsourcing to build plot graphs."},
{"id_case":"4945","class":"3","sentence1":"We use the standard alignment tool Giza++ ( Och and Ney, 2003 ) to word align the parallel data."},
{"id_case":"4946","class":"2","sentence1":"The formalism that is used to represent the semantics in the DELPH-IN grammars is Minimal Recursion Semantics (MRS) ( Copestake et al., 2005 )."},
{"id_case":"4947","class":"3","sentence1":"For French, Hungarian, Polish and Swedish we used Europarl Corpus 1 ( Koehn, 2005 )."},
{"id_case":"4948","class":"2","sentence1":"The training data of the shared task is the NUCLE corpus ( Dahlmeier et al., 2013 ), which contains essays written by learners of English (we also refer to it as learner data or shared task training d"},
{"id_case":"4950","class":"2","sentence1":"The English text was tokenized using the word tokenize routine from NLTK ( Bird et al., 2009 )."},
{"id_case":"4951","class":"3","sentence1":"We use the MOSES decoder ( Koehn et al., 2007 ) as a representative SMT decoder inside the system described below."},
{"id_case":"4952","class":"2","sentence1":"Specifically, the sentence compression dataset ( Clarke and Lapata, 2008 ) (referred as CL08) is used for subtree deletion model training ( arc )."},
{"id_case":"4953","class":"2","sentence1":"use the Stanford Parser ( de Marneffe et al., 2006 ) to extract a set of dependencies from each comment."},
{"id_case":"4954","class":"3","sentence1":"We conducted baseline experiments for phrasebased machine translation using the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"4955","class":"2","sentence1":"In practice, the decoder has to employ beam search to make it tractable ( Koehn, 2004 )."},
{"id_case":"4956","class":"2","sentence1":"The source of bilingual data used in the experiments is the Europarl collection ( Koehn, 2005 )."},
{"id_case":"4957","class":"2","sentence1":"The tectogrammatical annotation layer is based on the Functional Generative Description theory ( Sgall et al., 1986 )."},
{"id_case":"4958","class":"3","sentence1":"In this paper, we use the subjectivity corpus by Pang et al. (2002) ."},
{"id_case":"4959","class":"3","sentence1":"We conducted baseline experiments for phrasebased machine translation using the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"4960","class":"3","sentence1":"All our systems are contrasted with a standard phrase-based system built with Moses ( Koehn et al., 2007 )."},
{"id_case":"4961","class":"2","sentence1":"A common approach to computing similarity is to count the number of common words ( Lesk, 1986 )."},
{"id_case":"4962","class":"3","sentence1":"We conducted baseline experiments for phrasebased machine translation using the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"4963","class":"2","sentence1":"The Stanford dependency parser ( De Marneffe et al., 2006 ) is used for extracting features from the dependency parse trees."},
{"id_case":"4964","class":"3","sentence1":"We tokenize and frequent-case the data with the standard scripts from the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"4965","class":"3","sentence1":"We conducted baseline experiments for phrasebased machine translation using the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"4966","class":"3","sentence1":"We conducted baseline experiments for phrasebased machine translation using the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"4967","class":"3","sentence1":"For training the translation model and for decoding we used the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"4968","class":"3","sentence1":"We tokenize and frequent-case the data with the standard scripts from the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"4969","class":"3","sentence1":"We conducted baseline experiments for phrasebased machine translation using the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"4970","class":"3","sentence1":"We use the Moses software package 5 to train a PBMT model ( Koehn et al., 2007 )."},
{"id_case":"4971","class":"2","sentence1":"The corpora are tokenised and truecased using scripts from the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"4972","class":"2","sentence1":"In contrast, (McClosky et al, 2006) focus on large seeds and exploit a reranking-parser."},
{"id_case":"4973","class":"3","sentence1":"We use logistic regression with L2 regularization, implemented using the Scikit-learn toolkit ( Pedregosa et al., 2011 )."},
{"id_case":"4974","class":"2","sentence1":"Since the phrase table contains lemmas, the Wikipedia corpus was lemmatised using the TreeTagger ( Schmid, 1994 )."},
{"id_case":"4975","class":"3","sentence1":"We use logistic regression with L2 regularization, implemented using the Scikit-learn toolkit ( Pedregosa et al., 2011 )."},
{"id_case":"4976","class":"2","sentence1":"The corpora are tokenised and truecased using scripts from the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"4977","class":"3","sentence1":"For training the translation model and for decoding we used the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"4978","class":"3","sentence1":"We use the Moses software package 5 to train a PBMT model ( Koehn et al., 2007 )."},
{"id_case":"4979","class":"3","sentence1":"We perform this task on the data used by Barzilay and Lapata (2008) , a corpus collected originally by Barzilay and Elhadad (2003) from the Encyclopedia Britannica and its version for children, the Br"},
{"id_case":"4980","class":"3","sentence1":"We use the Moses software package 5 to train a PBMT model ( Koehn et al., 2007 )."},
{"id_case":"4981","class":"2","sentence1":"Europarl ( Koehn, 2005 ) is a multilingual parallel corpus extracted from the proceedings of the European Parliament."},
{"id_case":"4982","class":"2","sentence1":"For all syntactic parsers, we used the \" basic \" Stanford dependency representation ( de Marneffe et al., 2006 )."},
{"id_case":"4983","class":"2","sentence1":"The Stanford dependency parser ( De Marneffe et al., 2006 ) is used for extracting features from the dependency parse trees."},
{"id_case":"4984","class":"3","sentence1":"We built phrase-based machine translation systems using the open software toolkit Moses ( Koehn et al., 2007 )."},
{"id_case":"4985","class":"3","sentence1":"We use the cross-entropy loss function and minibatch AdaGrad ( Duchi et al., 2011 ) to optimize parameters ."},
{"id_case":"4986","class":"3","sentence1":"We use the cross-entropy loss function and minibatch AdaGrad ( Duchi et al., 2011 ) to optimize parameters ."},
{"id_case":"4987","class":"3","sentence1":"We use the AdaGrad method ( Duchi et al., 2011 ) to automatically update the learning rate for each parameter."},
{"id_case":"4988","class":"2","sentence1":"Socher et al. (2011)  explored using recursive autoencoders (RAEs) and dynamic pooling for paraphrase detection."},
{"id_case":"4989","class":"2","sentence1":"2 Translation Model Moses is a state-of-the-art toolkit for phrase-based SMT systems ( Koehn et al., 2007 )."},
{"id_case":"4990","class":"2","sentence1":"2 Translation Model Moses is a state-of-the-art toolkit for phrase-based SMT systems ( Koehn et al., 2007 )."},
{"id_case":"4991","class":"3","sentence1":"For training the translation model and for decoding we used the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"4992","class":"3","sentence1":"All our systems are contrasted with a standard phrase-based system built with Moses ( Koehn et al., 2007 )."},
{"id_case":"4993","class":"2","sentence1":"For all syntactic parsers, we used the \" basic \" Stanford dependency representation ( de Marneffe et al., 2006 )."},
{"id_case":"4994","class":"2","sentence1":"can be evaluated by maximising the pseudo-likelihood on a training corpus ( Malouf, 2002 )."},
{"id_case":"4995","class":"2","sentence1":"The baseline systems are built with the opensource phrase-based SMT toolkit Moses ( Koehn et al., 2007 )."},
{"id_case":"4996","class":"2","sentence1":"2 Translation Model Moses is a state-of-the-art toolkit for phrase-based SMT systems ( Koehn et al., 2007 )."},
{"id_case":"4997","class":"3","sentence1":"In this approach we used the NERsuite software based on the CRFsuite implementation ( Okazaki, 2007 )."},
{"id_case":"4998","class":"2","sentence1":"2 Translation Model Moses is a state-of-the-art toolkit for phrase-based SMT systems ( Koehn et al., 2007 )."},
{"id_case":"4999","class":"2","sentence1":"These efforts focused exclusively on the meronymy relation as used in WordNet ( Miller et al., 1990 )."},
{"id_case":"5000","class":"2","sentence1":"Special forms of relatedness are represented in the lexical entries of the WordNet lexical database ( Miller et al., 1990 )."},
{"id_case":"5501","class":"3","sentence1":"We train our model on a subset of the WaCkypedia EN 6 corpus ( Baroni et al., 2009 )."},
{"id_case":"5502","class":"2","sentence1":"Since the first shared task on Recognising Textual Entailment (RTE) ( Dagan et al., 2005 ) was organised in 2005, much research has been done on how one can detect entailment between natural language "},
{"id_case":"5503","class":"3","sentence1":"We used k-best batch MIRA ( Cherry and Foster, 2012 ) for tuning."},
{"id_case":"5504","class":"3","sentence1":"We used the Mallet toolkit ( McCallum, 2002 ) for learning maximum entropy models with Gaussian priors for all our experiments."},
{"id_case":"5505","class":"3","sentence1":"We use the implementation provided by CRFsuite 7 ( Okazaki, 2007 ) for both training and classification tasks."},
{"id_case":"5506","class":"2","sentence1":"glish source with target French by using GIZA++ ( Och and Ney, 2003 )."},
{"id_case":"5507","class":"3","sentence1":"We used ROUGE-N ( Lin, 2004 ) for evaluation of summaries."},
{"id_case":"5508","class":"2","sentence1":"1 with 2 -regularization using AdaGrad ( Duchi et al., 2011 )."},
{"id_case":"5509","class":"3","sentence1":"We used Weka ( Hall et al., 2009 ) for all our classification experiments."},
{"id_case":"5510","class":"2","sentence1":"POS tagging was performed with TreeTagger ( Schmid, 1994 )."},
{"id_case":"5511","class":"2","sentence1":"Word alignments on the parallel corpus are performed using GIZA++ ( Och and Ney, 2003 ) with the \" grow-diag-final \" refinement."},
{"id_case":"5512","class":"3","sentence1":"We used the implementation of the scikit-learn 2 module ( Pedregosa et al., 2011 )."},
{"id_case":"5513","class":"3","sentence1":"In ( Lavie and Agarwal, 2007 ), we described the process of tuning free parameters within the metric to optimize the correlation with human judgments and the extension of the metric for evaluating tra"},
{"id_case":"5514","class":"2","sentence1":"The Lexical sample data was parsed using the Clark and Curran CCG parser ( Clark and Curran, 2004 )."},
{"id_case":"5515","class":"3","sentence1":"For POS-tagging, we used the Stanford POS-tagger ( Toutanova and Manning, 2000 )."},
{"id_case":"5516","class":"3","sentence1":"We used the English side of the Europarl corpus ( Koehn, 2005 )."},
{"id_case":"5517","class":"2","sentence1":"This data is part of the NUCLE corpus ( Dahlmeier et al., 2013 )."},
{"id_case":"5518","class":"2","sentence1":"The training set is used to train the phrase-based translation model and language model for Moses ( Koehn et al., 2007 )."},
{"id_case":"5519","class":"3","sentence1":"We used the word alignment produced by Giza ( Och and Ney, 2000 ) out of an IBM model 2."},
{"id_case":"5520","class":"3","sentence1":"For this purpose we use the Europarl corpus ( Koehn, 2005 )."},
{"id_case":"5521","class":"2","sentence1":"Word alignment is performed using GIZA++ ( Och and Ney, 2003 )."},
{"id_case":"5522","class":"2","sentence1":"Word alignment was done with GIZA++ ( Och and Ney, 2003 ) for both systems."},
{"id_case":"5523","class":"3","sentence1":"We use the Moses toolkit ( Koehn et al., 2007 ) to create a statistical phrase-based machine translation model built on the best pre-processed data, as described above."},
{"id_case":"5524","class":"2","sentence1":"The evaluation results were provided by the organizers using their evaluation script in Python ( Dahlmeier and Ng, 2012 )."},
{"id_case":"5525","class":"2","sentence1":"In future work we can therefore incorporate unsupervised methods of NSW classification and expansion, along the lines of the automatic dictionary construction method presented by (Han et al, 2012) "},
{"id_case":"5526","class":"2","sentence1":"glish source with target French) by using GIZA++ ( Och and Ney, 2003 )."},
{"id_case":"5527","class":"3","sentence1":"We conducted statistical significance tests for BLEU between our best domain-adapted system, the baseline and the three third-party systems using paired bootstrap resampling ( Koehn, 2004) with 1,000 "},
{"id_case":"5528","class":"2","sentence1":"The word alignment was obtained by running Giza++ ( Och and Ney, 2003 )."},
{"id_case":"5529","class":"2","sentence1":"Word alignment was done with GIZA++ ( Och and Ney, 2003 ) for both systems."},
{"id_case":"5531","class":"3","sentence1":"For the theory of Cho & Chai (2000) to be complete, we have proposed a new type of marker and the Adjunct LP Constraint, in conjunction with their Argument LP Constraint."},
{"id_case":"5532","class":"3","sentence1":"In order to compare our method to a well understood phrase baseline, we present a method that tracts phrases by harvesting the Viterbi path from an HMM alignment model ( Vogel et al., 1996 )."},
{"id_case":"5533","class":"2","sentence1":"The Web1T corpus ( Brants and Franz, 2006 ) is a dataset consisting of the counts for n-grams obtained from 1 trillion (10 12 ) words of English Web text, subject to a minimum occurrence threshold (20"},
{"id_case":"5534","class":"3","sentence1":"We examine the quality of translations to English from Chinese and Arabic using Human-targeted Translation Edit Rates (HTER) ( Snover et al., 2006 ), which roughly captures the minimal number of edits"},
{"id_case":"5535","class":"2","sentence1":"Integer Linear Programming (ILP) has recently been applied to inference in sequential conditional random fields ( Roth and Yih, 2004 ), this has allowed the use of truly global constraints during infe"},
{"id_case":"5536","class":"3","sentence1":"For that purpose, we use the word analogy task proposed by Mikolov et al. (2013a) , which measures the accuracy on answering questions like \" what is the word that is similar to small in the same sens"},
{"id_case":"5537","class":"2","sentence1":"The corpus is first word-aligned using a word alignment heuristic ( Och and Ney, 2003 )."},
{"id_case":"5538","class":"3","sentence1":"For ranking, we use the SVM rank ranker ( Joachims, 2006 ), which learns a (sparse) weight vector that minimizes the number of swapped pairs in the training set."},
{"id_case":"5539","class":"2","sentence1":"Parameters are updated using AdaGrad ( Duchi et al., 2011 ) with a learning rate of 0.1."},
{"id_case":"5540","class":"3","sentence1":"The tagger we use is TnT ( Brants, 2000 ) , a hidden Markov trigram tagger, which was trained on the Spoken Dutch Corpus (CGN), Internal Release 6."},
{"id_case":"5541","class":"2","sentence1":"Sentiment score of the last post of the observation period We trained an L2 regularized logistic regression from LibLinear ( Fan et al., 2008 ) using the data collected from the Depression forum."},
{"id_case":"5542","class":"2","sentence1":"Motivated Figure 1 : The correlation between WQS in a sentence and its overall quality measured by : (a) BLEU, (b) TER and (c) TERp-A metrics by the idea of addressing WCE problem as a sequence labeli"},
{"id_case":"5544","class":"3","sentence1":"For German English we also have a system based on Moses ( Koehn et al., 2007 )."},
{"id_case":"5545","class":"3","sentence1":"In designing the algorithm, we took a similar approach to ( Marcu, 2000 )."},
{"id_case":"5546","class":"3","sentence1":"We use the Universal POS Tagset (UPOS) of Petrov et al. (2012) ."},
{"id_case":"5547","class":"3","sentence1":"We also considered an ensemble of our approach and that of ( Berant and Liang, 2014 )."},
{"id_case":"5548","class":"2","sentence1":"One way to solve this problem is to use a kernel function that is tailored for particular NLP applications, such as the tree kernel ( Collins and Duffy, 2001 ) for statistical parsing."},
{"id_case":"5549","class":"3","sentence1":"We use Adam ( Kingma and Ba, 2015 ) for optimisation with initial learning rate of 0.001."},
{"id_case":"5550","class":"3","sentence1":"We build our PB-SMT systems in a standard way using the Moses system (), KenLM for language modelling ( Heafield, 2011 ), and standard lexical reordering model ()."},
{"id_case":"5551","class":"3","sentence1":"The starting point for our model is the skipgram with negative sampling (SGNS) objective of Mikolov et al. (2013b) ."},
{"id_case":"5552","class":"2","sentence1":"In ( Collins and Duffy, 2001 ), the notion of a tree kernel is introduced to compute the number of common sub-trees of two parse trees."},
{"id_case":"5553","class":"2","sentence1":"We introduce a new anaphoricity detection model as the second neural model using a Long- Short Term Memory (LSTM) network ( Hochreiter and Schmidhuber, 1997 )."},
{"id_case":"5554","class":"2","sentence1":"The realisation ranking component is an SVM ranking model implemented with SVMrank, a Support Vector Machine-based learning tool ( Joachims, 2006 )."},
{"id_case":"5556","class":"2","sentence1":"WSI is generally considered as an unsupervised clustering task under the distributional hypothesis ( Harris, 1954 ) that the word meaning is reflected by the set of contexts in which it appears."},
{"id_case":"5557","class":"3","sentence1":"We run our experiments on Europarl ( Koehn, 2005 ), a multilingual parallel corpus which is described in detail in Section 3.1."},
{"id_case":"5558","class":"3","sentence1":"For the language model, we used the KenLM toolkit ( Heafield, 2011 ) to create a 5-gram language model on the target side of the Europarl corpus (v7) with approximately 54M tokens with Kneser-Ney smoo"},
{"id_case":"5559","class":"2","sentence1":"The second collection is constituted by the GENIA corpus ( Kim et al., 2003 ) 3 , which contains 2000 abstracts from Medline (a total of 18546 sentences )."},
{"id_case":"5560","class":"3","sentence1":"We introduce a new anaphoricity detection model as the second neural model using a Long- Short Term Memory (LSTM) network ( Hochreiter and Schmidhuber, 1997 )."},
{"id_case":"5561","class":"2","sentence1":"Baseline word alignments were obtained by running GIZA++ in both directions and symmetrizing using the grow-diag-final-and heuristic ( Och and Ney, 2003; )."},
{"id_case":"5562","class":"2","sentence1":"A system using the parsing techniques presented in this paper was entered in the CoNLL 2007 shared task competition ( Nivre et al., 2007 )."},
{"id_case":"5564","class":"3","sentence1":"All the Language Models (LM) used in our experiments are 5-grams modified Kneser-Ney smoothed LMs trained using KenLM ( Heafield et al., 2013 )."},
{"id_case":"5565","class":"2","sentence1":"They are based on Distributional Hypothesis which works under the assumption that similar words occur in similar contexts ( Harris, 1954 )."},
{"id_case":"5566","class":"3","sentence1":"Finally, we consider the Europarl corpus v7 ( Koehn, 2005 ), given it is widely used in the MT community, for Spanish English."},
{"id_case":"5567","class":"1","sentence1":"(=2b) So far, we have provided a simpler explanation for the alleged counterexamples to the analysis of Cho & Chai (2000) such as sentence (2), by postulating the Adjunct Constraint and the new type m"},
{"id_case":"5568","class":"3","sentence1":"For the contextual check we use the Google Web 1T 5-gram Corpus ( Brants and Franz, 2006 ) which contains counts for n-grams from unigrams through to five-grams obtained from over 1 trillion word toke"},
{"id_case":"5569","class":"1","sentence1":"By imposing constraints on the possible word reorderings similar to that described in (Berger et al, 1996) , the DP-based approach becomes more effective: when the constraints are applied, the number "},
{"id_case":"5571","class":"2","sentence1":"Surdeanu et al. (2012) propose a two-layer multi-instance multi-label (MIML) framework to capture the dependencies among relations."},
{"id_case":"5572","class":"3","sentence1":"We have theoretically suggested that based on Cho & Chai (2000) , our theory can be a complete theory of scrambling phenomenon by providing the new type marker and the Adjunct LP Constraint."},
{"id_case":"5573","class":"2","sentence1":"The other is from Jeffrey Pennington et al. (2014) , the dimension of word embedding is 100."},
{"id_case":"5574","class":"3","sentence1":"For the contextual check we use the Google Web 1T 5-gram Corpus ( Brants and Franz, 2006 ) which contains counts for n-grams from unigrams through to five-grams obtained from over 1 trillion word toke"},
{"id_case":"5575","class":"2","sentence1":"ROUGE ( Lin, 2004 ) is a set of evaluation metrics used for automatic summarization."},
{"id_case":"5576","class":"2","sentence1":"Here we review the parameters of the standard phrase-based translation model ( Koehn et al., 2007 )."},
{"id_case":"5577","class":"2","sentence1":"The Spanish-English (S2E) training corpus was drawn from the Europarl collection ( Koehn, 2005 )."},
{"id_case":"5580","class":"3","sentence1":"We use KenLM 3 ( Heafield, 2011 ) for computing the target language model score."},
{"id_case":"5581","class":"2","sentence1":"The most famous example would probably be the Europarl corpus ( Koehn, 2005 )."},
{"id_case":"5582","class":"3","sentence1":"We use the Stanford parser with Stanford dependencies ( de Marneffe et al., 2006 )."},
{"id_case":"5583","class":"3","sentence1":"Additionally, we train phrase-based machine translation models with our alignments using the popular Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"5584","class":"2","sentence1":"The training set is used to train the phrase-based translation model and language model for Moses ( Koehn et al., 2007 )."},
{"id_case":"5585","class":"3","sentence1":"The morpho syntactically annotated corpus we used is a variant of the French TreeBank or FTB, ( Abeille et al., 2003 )."},
{"id_case":"5586","class":"2","sentence1":"There has been a large amount of work on sentiment analysis at various levels of granularity ( Pang and Lee, 2008 )."},
{"id_case":"5587","class":"2","sentence1":"This task setup is further described in the task description paper ( Rosenthal et al., 2014 )."},
{"id_case":"5588","class":"2","sentence1":"The baseline will be created by the Moses SMT toolkit ( Koehn et al., 2007 )."},
{"id_case":"5590","class":"3","sentence1":"Additionally, we train phrase-based machine translation models with our alignments using the popular Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"5591","class":"2","sentence1":"Discourse structure in summarization Rhetorical Structure Theory (RST) ( Mann and Thompson, 1988 ) represents the discourse in a document in the form of a tree ( Figure 1 )."},
{"id_case":"5592","class":"2","sentence1":"It builds on the C&C CCG parser ( Clark and Curran, 2004 )."},
{"id_case":"5593","class":"3","sentence1":"We use Boxer ( Bos et al., 2004 ) to parse natural language into a logical form."},
{"id_case":"5594","class":"2","sentence1":"The training set is used to train the phrase-based translation model and language model for Moses ( Koehn et al., 2007 )."},
{"id_case":"5595","class":"3","sentence1":"We perform bootstrap resampling with bounds estimation as described in ( Koehn, 2004 )."},
{"id_case":"5596","class":"2","sentence1":"The major part of data comes from current and upcoming full releases of the Europarl data set ( Koehn, 2005 )."},
{"id_case":"5598","class":"3","sentence1":"For Italian, we use the word2vec to train word embeddings on the Europarl Italian corpus ( Koehn, 2005 ) 2 ."}
]
}
]
