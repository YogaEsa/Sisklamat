[
{"type":"header","version":"4.9.0.1","comment":"Export to JSON plugin for PHPMyAdmin"},
{"type":"database","name":"parafras_dataset_corpus"},
{"type":"table","name":"view_hasil_yoga","database":"parafras_dataset_corpus","data":
[
{"id_case":"1","class":"0","sentence1":"Most of existing lexical-semantic networks have been built by hand (like for instance WordNet ( Miller et al., 1990) ) and, despite that assisting tools are generally designed for consistency checking"},
{"id_case":"2","class":"0","sentence1":"( Petrov and McDonald, 2012 ), which includes the top ranked system, this indicates that self-training is already an established technique to improve the accuracy of constituency parsing on English."},
{"id_case":"3","class":"0","sentence1":"For example, Traxler et al. (2002) combine metonymic and non-metonymic verbs with entity-denoting and event-denoting nouns (The boy [started\/saw] V [the puzzle\/fight] N P ) and report significantly."},
{"id_case":"4","class":"1","sentence1":"Our system uses the architecture from ( Lee et al., 2016 ) where a character-level neural MT model maps the source character sequence to the target character sequence."},
{"id_case":"5","class":"0","sentence1":"Finite state morphology ( Beesley and Karttunen, 2003 ) is the state-of-the-art in writing morphological analysers for natural languages of the whole range of typologically varying morphological feature."},
{"id_case":"6","class":"0","sentence1":"A framework for human error analysis and error classification has been proposed in ( Vilar et al., 2006 ) and a detailed analysis of the obtained results has been carried out."},
{"id_case":"9","class":"1","sentence1":"Training is done with the Adam optimisation algorithm ( Kingma and Ba, 2014 ) with learning rate of 10 ?4 ."},
{"id_case":"10","class":"1","sentence1":"We use the Support Vector Machines implementation in the scikit-learn toolkit ( Pedregosa et al., 2011 ) to perform regression (SVR) on each feature set with either RBF kernels and parameters optimise"},
{"id_case":"12","class":"1","sentence1":"In order to assess the reliability of such results, we computed pairwise improvement intervals as described in ( Koehn, 2004 ), by means of bootstrapping with 1000 bootstrap iterations."},
{"id_case":"13","class":"1","sentence1":"It is used to support semantic analyses in HPSG English grammar ERG ( Copestake and Flickinger, 2000 ), but also in other grammar formalisms like LFG."},
{"id_case":"15","class":"1","sentence1":"Significance is marked withfor 95% confidence and for 99% confidence, and is measured with the bootstrap resampling method as described in ( Koehn, 2004 )."},
{"id_case":"16","class":"0","sentence1":"More recently, ( Pasha et al., 2014 ) created MADAMIRA, a system for morphological analysis and disambiguation of Arabic, this system can be used to improve the accuracy of spelling checking system."},
{"id_case":"17","class":"1","sentence1":"We optimize the feature weights using a modified version of averaged perceptron learning as described by Collins (2002) ."},
{"id_case":"18","class":"0","sentence1":"Another approach that uses the polarity of the local context for computing word polarity is the one presented by ( Popescu and Etzioni, 2005 ), who use a weighting function of the words around."},
{"id_case":"21","class":"1","sentence1":"It was built with the Moses toolkit ( Koehn et al., 2007 ) using the 14 standard core features including a 5-gram language model."},
{"id_case":"22","class":"3","sentence1":"The common phrasebased translation systems, such as ( Och et al., 1999; Koehn, 2004 ), do not use an explicit sentence length model."},
{"id_case":"23","class":"3","sentence1":"The common phrasebased translation systems, such as ( Och et al., 1999; Koehn, 2004 ), do not use an explicit sentence length model."},
{"id_case":"24","class":"1","sentence1":"After creating the training instances, we train a 14-class classifier on them using SVM multiclass ( Tsochantaridis et al., 2004 )."},
{"id_case":"25","class":"1","sentence1":"POS tagging was performed with TreeTagger ( Schmid, 1994 )."},
{"id_case":"26","class":"0","sentence1":"The first one is the WS- 353 3 dataset ( Finkelstein et al., 2001 ) containing 353 pairs of English words that have been assigned similarity ratings by humans."},
{"id_case":"27","class":"0","sentence1":"Web-based methods, instead, extract text snippets from Web pages which match pre-defined lexical patterns, such as X is a Y, along the lines of Hearst (1992) ."},
{"id_case":"28","class":"1","sentence1":"Given the semantic objects defined in the previous section, we design a convolution kernel in a way similar to the parse-tree kernel proposed in ( Collins and Duffy, 2002 )."},
{"id_case":"29","class":"0","sentence1":"The modeling of Process as a kind of event that is continuous and homogeneous in nature, follows the frame semantic analysis used for generating the FRAMENET data ( Baker et al., 1998 )."},
{"id_case":"31","class":"1","sentence1":"The SSN uses standard methods ( Bishop, 1995 ) to estimate a probability distribution over the set of possible next decisions d i given these representations ."},
{"id_case":"32","class":"0","sentence1":"Recently, there has been a successful attempt to harmonize the linguistic principles behind the coding systems MSD and KR ( Farkas et al., 2010 )."},
{"id_case":"33","class":"0","sentence1":"The second algorithm, denoted GloTr, is the Chu-Liu- Edmonds algorithm for maximal spanning tree implemented in the MSTParser ( McDonald, 2006 )."},
{"id_case":"34","class":"0","sentence1":"The first one is the WS-353 dataset ( Finkelstein et al., 2001 ) containing 353 pairs of English words that have been assigned similarity ratings by humans."},
{"id_case":"35","class":"0","sentence1":"Abstract Meaning Representation (AMR) ( Banarescu et al., 2013 ) is a semantic formalism where the meaning of a sentence is encoded as a rooted, directed graph."},
{"id_case":"36","class":"1","sentence1":"We perform bootstrap resampling with bounds estimation as described in ( Koehn, 2004 )."},
{"id_case":"37","class":"1","sentence1":"It is used to support semantic analyses in HPSG English Resource  grammar ERG ( Copestake and Flickinger, 2000 ), but also in other grammar formalisms like LFG."},
{"id_case":"38","class":"1","sentence1":"It is used to support semantic analyses in the English HPSG grammar ERG ( Copestake and Flickinger, 2000 ), but also in other grammar formalisms like LFG."},
{"id_case":"39","class":"1","sentence1":"It is used to support semantic analyses in HPSG English grammar ERG ( Copestake and Flickinger, 2000 ), but also in other grammar formalisms like LFG."},
{"id_case":"40","class":"1","sentence1":"It is used to support semantic analyses in the English HPSG grammar ERG ( Copestake and Flickinger, 2000 ), but also in other grammar formalisms like LFG."},
{"id_case":"41","class":"3","sentence1":"Table 2 : The size of annotated data from CoNLL ( Buchholz and Marsi, 2006 ), and the number of tags included and missing for 8 languages ."},
{"id_case":"43","class":"1","sentence1":"We build upon our previous Markov Logic based approach for joint concept disambiguation and clustering ( Fahrni and Strube, 2012 )."},
{"id_case":"44","class":"0","sentence1":"Details about SVM and KFD can be found in ( Taylor and Cristianini, 2004 )."},
{"id_case":"45","class":"2","sentence1":"Therefore, POS taggers for English tweets have been developed such as ARK, T-Pos and GATE TwitIE which reaches 92.8%, 88.4% and 89.37% accuracy respectively ( Derczynski et al., 2013 )."},
{"id_case":"46","class":"1","sentence1":"We learn the parameters using a quasi-Newton procedure with L 1 (lasso) regularization ( Andrew and Gao, 2007 )."},
{"id_case":"47","class":"1","sentence1":"We use the SCFG decoder cdec ( Dyer et al., 2010 ) 4 and build grammars using its implementation of the suffix array extraction method described in Lopez (2007)."},
{"id_case":"48","class":"0","sentence1":"This is known as the Distributional Hypothesis ( Harris, 1968 )."},
{"id_case":"49","class":"1","sentence1":"All our models , as well as the parser described in ( Henderson, 2003 ), are run only once."},
{"id_case":"50","class":"0","sentence1":"Word alignment is performed by GIZA++ ( Och and Ney, 2000 ) in both directions with the default setting."},
{"id_case":"51","class":"0","sentence1":"For strings, many such kernel functions exist with various applications in computational biology and computational linguistics ( Taylor and Cristianini, 2004 )."},
{"id_case":"52","class":"0","sentence1":"is an example of SRL annotation from the Prop- Bank corpus ( Palmer et al., 2005 ), where the subscripted information maps the semantic roles A0, A1 and A2 to arguments for the predicate sell as def"},
{"id_case":"53","class":"0","sentence1":"The estimation of the semantically Smoothed Partial Tree Kernel (SPTK) is made available by an extended version of SVM-LightTK software 5 ( Moschitti, 2006 )."},
{"id_case":"54","class":"0","sentence1":"In the i2b2 2012 temporal challenge, all top performing teams used a combination of supervised classification and rule-based methods for extracting temporal information and relations ( Sun et al., 2013 )."},
{"id_case":"55","class":"1","sentence1":"We train with the Adam optimizer ( Kingma and Ba, 2015 ), a learning rate of 0.0001, batch size of 50, and dropout with probability 0.2 applied to the hidden layer."},
{"id_case":"56","class":"1","sentence1":"In our experimental study, we use the freely available implementations in Weka ( Witten and Frank, 2005 )."},
{"id_case":"57","class":"1","sentence1":"The phrase translation probabilities are smoothed with Good-Turing smoothing ( Foster et al., 2006 )."},
{"id_case":"58","class":"0","sentence1":"More recently, ( Carpineto and Romano, 2010 ) showed that the characteristics of the outputs returned by SRC algorithms suggest the adoption of a meta clustering approach."},
{"id_case":"59","class":"1","sentence1":"They are based on the distributional hypothesis ( Harris, 1968 ) and by looking at a set of event expressions whose argument fillers have a similar distribution, they try to recognize synonymous event."},
{"id_case":"60","class":"0","sentence1":"Word alignment is performed using GIZA++ ( Och and Ney, 2003 )."},
{"id_case":"61","class":"0","sentence1":"Word alignment is performed using GIZA++ ( Och and Ney, 2003 )."},
{"id_case":"62","class":"1","sentence1":"We used Mallet software ( McCallum, 2002 ) for CRF experiments."},
{"id_case":"63","class":"0","sentence1":"The thesaurus consists of a hierarchy of 2,710 semantic classes, defined for over 264,312 nouns, with a maximum depth of twelve ( Ikehara et al., 1997 )."},
{"id_case":"64","class":"0","sentence1":"The detailed discussion is provided in the longer version of the paper ( Kim et al., 2013 )."},
{"id_case":"65","class":"0","sentence1":"can be evaluated by maximizing the pseudo-likelihood on a training corpus see ( Malouf, 2002 )."},
{"id_case":"66","class":"0","sentence1":"The first one is the WS- 353 3 dataset ( Finkelstein et al., 2001 ) containing 353 pairs of English words that have been assigned similarity ratings by humans."},
{"id_case":"67","class":"1","sentence1":"Following Blitzer et al. (2006) , we consider pivot features that appear more than 50 times in all the domains for SCL and mDA."},
{"id_case":"68","class":"1","sentence1":"A framework for human error analysis and error classification has been proposed in ( Vilar et al., 2006 ), but like human evaluation , this is also a time consuming task."},
{"id_case":"69","class":"1","sentence1":"MaxEnt classifier is a good example of this group ( Mani et al., 2006 )."},
{"id_case":"70","class":"0","sentence1":"Among these media, blog is one of the communicative and informative repository of text based emotional contents in the Web 2.0 ( Lin et al., 2007 )."},
{"id_case":"71","class":"1","sentence1":"For the gold preprocessing and all 5k settings, we refer the reader to the Shared Task overview paper ( Seddah et al., 2013 )."},
{"id_case":"73","class":"1","sentence1":"For preprocessing, we used MADA (Morphological Analysis and Disambiguation for Arabic) ( Habash et al., 2009 ) which is one of the most accurate Arabic preprocessing toolkits."},
{"id_case":"74","class":"1","sentence1":"We trained a 5-gram language model on the Xinhua section of the English Gigaword corpus (306 million words) using the SRILM toolkit ( Stolcke, 2002) with the modified Kneser-Ney smoothing."},
{"id_case":"75","class":"1","sentence1":"To determine semantic type and subtype, we train two SVM multiclass classifiers using SVM multiclass ( Tsochantaridis et al., 2004 )."},
{"id_case":"76","class":"1","sentence1":"We use the scikit implementation of Random Forest ( Pedregosa et al., 2011 )."},
{"id_case":"78","class":"1","sentence1":"Each term in the input text will be represented by its stem and POS tag, in the following format (stem:POS) using Buckwalter transliteration ( Buckwalter, 2002 )."},
{"id_case":"79","class":"1","sentence1":"An algorithm, the Kuhn-Munkres method ( Kuhn, 1955 ), can find solutions to the optimum assignment problem in polynomial time."},
{"id_case":"81","class":"1","sentence1":"We use Collapsed Gibbs Sampling ( Griffiths and Steyvers, 2004 ) to infer the parameters of the model and the latent violent categories and topics assignments for tweets, given observed data D. Gibbs."},
{"id_case":"82","class":"1","sentence1":"We use the Stanford dependency parser ( Marneffe et al., 2006 )."},
{"id_case":"83","class":"1","sentence1":"Filter weights are initialized using Glorot-Bengio strategy ( Glorot and Bengio, 2010 )."},
{"id_case":"85","class":"0","sentence1":"Automatic sentence alignment of the training data was provided by Ulrich German, and the hand alignments of the words in the test data were created by Franz Och and Hermann Ney ( Och and Ney, 2003 )."},
{"id_case":"86","class":"1","sentence1":"We use the AdaGrad optimizer ( Duchi et al., 2011)  with initial learning rate set to 0.1."},
{"id_case":"87","class":"1","sentence1":"Then we did word alignment using GIZA++ ( Och and Ney, 2003 ) with the default grow-diagfinal-and alignment symmetrization method."},
{"id_case":"88","class":"1","sentence1":"For example, DIRT ( Lin and Pantel, 2001 ) aims to discover different representations of the same semantic relation, i.e. similar dependency paths."},
{"id_case":"89","class":"1","sentence1":"The annotation was performed manually using the brat annotation tool ( Stenetorp et al., 2012 )."},
{"id_case":"90","class":"0","sentence1":"As explained in more detail in ( Henderson, 2003b )."},
{"id_case":"91","class":"0","sentence1":"System proposed by ( Li et al., 2006 ), uses a semantic-vector approach to measure sentence similarity."},
{"id_case":"93","class":"0","sentence1":"All modules take as input the corpus documents preprocessed with a part-of-speech tagger 4 and shallow parser 5 ( Punyakanok and Roth, 2001 )."},
{"id_case":"94","class":"2","sentence1":"It is worth noting that even if the above equations define a kernel function similar to the one proposed in ( Collins and Duffy, 2002 )"},
{"id_case":"95","class":"1","sentence1":"From the pioneering work of ( Rapp, 1995 ), contextual similarity has been used for BLE for a long time."},
{"id_case":"96","class":"0","sentence1":"Europarl ( Koehn, 2005 ) is a multilingual parallel corpus extracted from the proceedings of the European Parliament."},
{"id_case":"97","class":"0","sentence1":"The four test sets are: WS-353 ( Finkelstein et al., 2002 ) is a set of 353 word pairs."},
{"id_case":"98","class":"1","sentence1":"We use the F1 measure according to the Parseval metric ( Black et al., 1991 ) for the evaluation."},
{"id_case":"99","class":"0","sentence1":"TESLA (Translation Evaluation of Sentences with Linear-programming-based Analysis) was first proposed in Liu et al. (2010) ."},
{"id_case":"100","class":"1","sentence1":"For building the baseline SMT system, we used the open-source SMT toolkit Moses ( Koehn et al., 2007 ), in its standard setup."},
{"id_case":"102","class":"0","sentence1":"Rhetorical Structure Theory (RST) ( Mann and Thompson, 1988 ), one of the most influential theories of discourse, represents texts by labeled hierarchical structures, called Discourse Trees (DTs)."},
{"id_case":"103","class":"1","sentence1":"In addition, the fix-discount method in ( Foster et al., 2006 ) for phrase table smoothing is also used."},
{"id_case":"104","class":"0","sentence1":"Memory-based language processing (Daelemans and van den Bosch, 2005 ) is based on the idea that NLP problems can be solved by reuse of solved examples of the problem stored in memory."},
{"id_case":"105","class":"1","sentence1":"We calculate statistical significance of performance differences using stratified shuffling ( Yeh, 2000 )."},
{"id_case":"106","class":"0","sentence1":"For instance, machine translation (MT) systems can benefit from training on sentences extracted from parallel or comparable documents retrieved from the Web ( Munteanu and Marcu, 2005 )."},
{"id_case":"108","class":"0","sentence1":"In PB-SMT, the posterior probability P(e I 1 |f J 1 ) is directly modelled as a (log-linear) combination of features ( Och and Ney, 2002 ), that usually comprise M translational features"},
{"id_case":"109","class":"1","sentence1":"1 with 2 -regularization using AdaGrad ( Duchi et al., 2011 )."},
{"id_case":"111","class":"2","sentence1":"Why does the lr model outperform Berkeley 13 The MUC ( Vilain et al., 1995 ) score is the minimum number of links between mentions to be inserted or deleted when mapping the output to a gold standard."},
{"id_case":"112","class":"1","sentence1":"In the context of this paper we will be focusing on the SubSet Tree (SST) kernel described in ( Collins and Duffy, 2002 ), which relies on a fragment definition that does not allow to break production."},
{"id_case":"113","class":"0","sentence1":"Europarl ( Koehn, 2005 ) is a multilingual parallel corpus extracted from the proceedings of the European Parliament."},
{"id_case":"114","class":"1","sentence1":"We have used Foma, a free software tool to specify finite-state automata and transducers ( Hulden, 2009 )."},
{"id_case":"116","class":"1","sentence1":"We used the same test set used in Li et al. (2004) for our testing 5 ."},
{"id_case":"117","class":"0","sentence1":"The Penn Discourse Treebank (PDTB, Prasad et al., 2008 ) is a large corpus annotated with discourse relations, covering the Wall Street Journal part of the Penn Treebank."},
{"id_case":"118","class":"1","sentence1":"We used the implementation of the scikit-learn 2 module ( Pedregosa et al., 2011 )."},
{"id_case":"119","class":"1","sentence1":"Since the commonly used word similarity datasets contain a small number of word pairs we also use the MEN dataset ( Bruni et al., 2012 ) of 3,000 word pairs sampled from words that occur at least 700 times in a large web corpus."},
{"id_case":"120","class":"0","sentence1":"The training data of the shared task is the NUCLE corpus ( Dahlmeier et al., 2013 ), which contains essays written by learners of English ."},
{"id_case":"121","class":"0","sentence1":"RTE is instead the task of deciding whether a long text T entails a shorter text, typically a single sentence, called hypothesis H. It has been often seen as a classification task  ( Dagan et al., 2013 )."},
{"id_case":"122","class":"0","sentence1":"All system implementation was done using Python and the open-source machine learning toolkit scikit-learn ( Pedregosa et al., 2011 )."},
{"id_case":"123","class":"0","sentence1":"It has been shown that a diverse set of predictions can be used to help improve decoder accuracy for various problems in NLP ( Henderson and Brill, 1999 )."},
{"id_case":"124","class":"1","sentence1":"In the 2013 system, we had used SentiStrength lexicon ( Thelwall et al., 2010 )."},
{"id_case":"125","class":"2","sentence1":"Finally, we also compare the quality of the candidate phrase embeddings with word embeddings ( Dhillon et al., 2011 ) by adding them as features in a CRF based sequence tagger."},
{"id_case":"126","class":"0","sentence1":"These methods are based on the distributional hypothesis which states that words appearing in the same contexts tend to have similar meaning ( Harris, 1954 )."},
{"id_case":"127","class":"0","sentence1":"All annotations were done using the BRAT rapid annotation tool ( Stenetorp et al., 2012 )."},
{"id_case":"128","class":"3","sentence1":"Compared to WordNet (Fellbaum, 1998 ), there are similarities but also significant differences."},
{"id_case":"131","class":"1","sentence1":"For training, we use Adam ( Kingma and Ba, 2015 ) for optimization with an initial learning rate of 0.001."},
{"id_case":"132","class":"1","sentence1":"For our classifier, we use SVMs, specifically the LIBLINEAR SVM software package ( Fan et al., 2008 ), which is well-suited to text classification tasks with large numbers of features and large number"},
{"id_case":"133","class":"0","sentence1":"The first two experiments concern the prediction of the sentiment of movie reviews in the Stanford Sentiment Treebank ( Socher et al., 2013)."},
{"id_case":"134","class":"0","sentence1":"Concept-to-text generation broadly refers to the task of automatically producing textual output from nonlinguistic input ( Reiter and Dale, 2000 )."},
{"id_case":"136","class":"1","sentence1":"We used only the non-ensembled left-to-right run (i.e. no right-to-left rescoring as done by Sennrich et al., 2016a ) with beam size of 5, 5 taking just the single-best output."},
{"id_case":"137","class":"1","sentence1":"We used only the non-ensembled left-to-right run (Sennrich et al., 2016a ) with beam size of 5, 5 taking just the single-best output."},
{"id_case":"138","class":"0","sentence1":"The MSD morphological coding system was developed for a bunch of languages including Hungarian ( Erjavec, 2004 )."},
{"id_case":"139","class":"0","sentence1":"The phrase tables were generated by means of symmetrised word alignments obtained with GIZA++ ( Och and Ney, 2003 )."},
{"id_case":"140","class":"0","sentence1":"Word alignment is performed using GIZA++ ( Och and Ney, 2003 )."},
{"id_case":"141","class":"1","sentence1":"We experimented with several levels of cluster granularity using development data, and following Koo et al. (2008)."},
{"id_case":"143","class":"0","sentence1":"(Raghavan et al. (2007) ) measure the benefit from feature feedback as the gain in the learning speed with feature feedback."},
{"id_case":"144","class":"1","sentence1":"We built a modified Kneser-Ney smoothed 5-gram language model using the English side of the training data and performed querying with KenLM ( Heafield, 2011 )."},
{"id_case":"145","class":"0","sentence1":"( Kneser and Ney, 1995 )."},
{"id_case":"146","class":"0","sentence1":"A formal PAC-style analysis can be found in ( Ando and Zhang, 2004 )."},
{"id_case":"147","class":"1","sentence1":"The first model we introduce is based on the recurrent neural network language model of Mikolov et al. (2010) ."},
{"id_case":"148","class":"0","sentence1":"The SMT systems were built using the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"149","class":"0","sentence1":"The classifier experiments were carried out using the SVM-light software ( Joachims, 1999 ) available at http:\/\/svmlight.joachims.org\/ with a polynomial kernel 2 (degree=3)."},
{"id_case":"150","class":"0","sentence1":"The training data of the shared task is the NUCLE corpus ( Dahlmeier et al., 2013 ), which contains essays written by learners of English."},
{"id_case":"151","class":"0","sentence1":"SALDO ( Borin et al., 2013 ) is the largest freely available lexical resource for Swedish."},
{"id_case":"152","class":"1","sentence1":"For the phrase-based SMT system, we adopted the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"153","class":"0","sentence1":"However, those string-to-tree systems run slowly in cubic time ( Huang et al., 2006 )."},
{"id_case":"154","class":"0","sentence1":"The 5-gram target language model was trained using KenLM ( Heafield, 2011 )."},
{"id_case":"155","class":"1","sentence1":"For all experiments, we used the Moses SMT system ( Koehn et al., 2007 )."},
{"id_case":"156","class":"1","sentence1":"We evaluate our method on the following data sets: @BULLET OntoNotes-Dev: Development set of the OntoNotes data provided by the CoNLL2012 shared task ( Pradhan et al., 2012 )."},
{"id_case":"157","class":"1","sentence1":"We use the Moses phrase-based translation system ( Koehn et al., 2007 ) to implement our models."},
{"id_case":"158","class":"1","sentence1":"But we randomly selected 90% of the training data used in Li et al. (2004) as our training data and the remainder as the development data, as shown in Table 5 ."},
{"id_case":"159","class":"3","sentence1":"The BLEU score measures the precision of n-grams (over all n to 4 in our case) with respect to a reference translation with a penalty for short translations ( Papineni et al., 2001 )."},
{"id_case":"160","class":"0","sentence1":"The training data of the shared task is the NUCLE corpus ( Dahlmeier et al., 2013 ), which contains essays written by learners of English."},
{"id_case":"161","class":"0","sentence1":"Test data was drawn from the Open American National Corpus ( Ide and Suderman, 2004 , OANC) across a variety of genres and from both the spoken and written portions of the corpus."},
{"id_case":"162","class":"1","sentence1":"We measure statistical significance using 95% confidence intervals computed with paired bootstrap resampling ( Koehn, 2004 )."},
{"id_case":"163","class":"1","sentence1":"We use the English portion of the ACE 2005 relation extraction dataset ( Walker et al., 2006 )."},
{"id_case":"164","class":"0","sentence1":"This data was collected for the 2014 SemEval competition ( Marelli et al., 2014 ) and consists of 9,927 sentence pairs, with 4,500 for training, 500 as a development set."},
{"id_case":"165","class":"0","sentence1":"The parsing model used for intra-sentential parsing is a Dynamic Conditional Random Field (DCRF) ( Sutton et al., 2007 ) shown in Figure 7 ."},
{"id_case":"166","class":"0","sentence1":"Latent Dirichlet Allocation (LDA) is a generative model which considers a document model ( Salton, 1989 ) as a mixture probability of latent topics ."},
{"id_case":"167","class":"1","sentence1":"Distributional hypothesis theory ( Harris, 1954 ) indicates that words that occur in the same context tend to have similar meanings."},
{"id_case":"168","class":"1","sentence1":"Distributional hypothesis theory ( Harris, 1954 ) indicates that words that occur in the same context tend to have similar meanings."},
{"id_case":"169","class":"0","sentence1":"In 2009, Yefang Wang ( Wang et al., 2009 ) used cascading classifiers on manually annotated data which fetched F-score of 0.832."},
{"id_case":"170","class":"1","sentence1":"We built a 5-gram language model on the English side of QCA-train using KenLM ( Heafield, 2011 )."},
{"id_case":"171","class":"0","sentence1":"The remaining three models are all Naive Bayes classifiers trained on the Google Web 1T 5-gram corpus (henceforth, Google corpus , ( Brants and Franz, 2006 ))."},
{"id_case":"172","class":"1","sentence1":"We apply bootstrapping ( Kozareva et al., 2008 ) on the word graphs by manually selecting 10 seeds for concrete and abstract words (see Table 10 )."},
{"id_case":"173","class":"0","sentence1":"These methods are based on the distributional hypothesis which states that words appearing in the same contexts tend to have similar meaning ( Harris, 1954 )."},
{"id_case":"174","class":"1","sentence1":"These results verify the benefit of using LTAG based features and confirm the hypothesis that LTAG based features provide a novel set of abstract features that complement the hand selected features ( Collins, 2000 )."},
{"id_case":"175","class":"0","sentence1":"RG-65: ( Rubenstein and Goodenough, 1965 ) has 65 word pairs."},
{"id_case":"176","class":"1","sentence1":"The significance tests were performed using the bootstrap resampling method ( Koehn, 2004 )."},
{"id_case":"177","class":"0","sentence1":"The default Phrasal search algorithm is cube pruning ( Huang and Chiang, 2007 )."},
{"id_case":"178","class":"0","sentence1":"In-domain data is mainly used to solve the problem of data sparseness ( Sun and Xu, 2011 )."},
{"id_case":"179","class":"1","sentence1":"All experiments were carried out using the open-source SMT toolkit Moses ( Koehn et al., 2007 )."},
{"id_case":"180","class":"1","sentence1":"First, we apply heuristics to determine number and gender based on word lists, WordNet ( Miller, 1990 ) and part-of-speech tags."},
{"id_case":"181","class":"1","sentence1":"Rank SVM ( Joachims, 2002 ) is a method based on Support Vector Machines (SVMs) for which we use only linear kernels to keep complexity low."},
{"id_case":"182","class":"0","sentence1":"A more detailed description of the task can be found in ( Nakov et al., 2017 )."},
{"id_case":"184","class":"0","sentence1":"A Tree Kernel function is a convolution kernel ( Haussler, 1999 ) defined over pairs of trees."},
{"id_case":"185","class":"1","sentence1":"We build upon our previous approach for joint concept disambiguation and clustering ( Fahrni and Strube, 2012 )."},
{"id_case":"186","class":"0","sentence1":"ROUGE-2 metric ( Lin, 2004 ) is used for the evaluation ."},
{"id_case":"187","class":"1","sentence1":"We used Mallet software ( McCallum, 2002 ) for CRF experiments."},
{"id_case":"188","class":"1","sentence1":"We exploit a transition-based framework with global learning and beam-search decoding to implement the joint model ( Zhang and Clark, 2011 )."},
{"id_case":"189","class":"0","sentence1":"The annotation was performed using the BRAT 2 tool ( Stenetorp et al., 2012 )."},
{"id_case":"190","class":"1","sentence1":"For building the word alignment models we use MGIZA++ ( Gao and Vogel, 2008 )."},
{"id_case":"191","class":"0","sentence1":"The reliability of the annotation was evaluated using the kappa statistic ( Carletta, 1996 )."},
{"id_case":"192","class":"1","sentence1":"It therefore follows the distributional hypothesis ( Harris, 1954 ) which states that words that occur in the same contexts tend to have similar meanings."},
{"id_case":"194","class":"0","sentence1":"The significance tests were performed using the bootstrap resampling method ( Koehn, 2004 )."},
{"id_case":"195","class":"1","sentence1":"We use the standard alignment tool Giza++ ( Och and Ney, 2003 ) to word align the parallel data."},
{"id_case":"196","class":"1","sentence1":"We use the standard alignment tool Giza++ ( Och and Ney, 2003 ) to word align the parallel data."},
{"id_case":"198","class":"0","sentence1":"The kernels are combined using Gaussian process regression (GPR) ( Rasmussen and Williams, 2006 )."},
{"id_case":"200","class":"1","sentence1":"We measure statistical significance using 95% confidence intervals computed with paired bootstrap resampling ( Koehn, 2004 )."},
{"id_case":"701","class":"1","sentence1":"We use the Stanford dependency parser ( Chen and Manning, 2014 ) at this stage, and have not experimented with alternatives ."},
{"id_case":"702","class":"0","sentence1":"Next, a tweet was tokenized and fed into MADAMIRA ( Pasha et al., 2014 ), a morphological analysis tool for Arabic text."},
{"id_case":"703","class":"0","sentence1":"(Yarowsky ,(1995) ) has proposed a bootstrapping method for word sense disambiguation ."},
{"id_case":"704","class":"1","sentence1":"We also list the previous state-of-the-art performance from a conventional SMT system ( Durrani et al., 2014) with the BLEU of 37.0."},
{"id_case":"705","class":"1","sentence1":"We used Adam ( Kingma and Ba, 2014 ) with a learning rate of 0.0002."},
{"id_case":"706","class":"0","sentence1":"Distributional semantics is based on the idea that ( Firth, 1957) in other words, the meaning of a word is related to the contexts it appears in."},
{"id_case":"707","class":"0","sentence1":"In order to estimate the basic lexical similarity function employed in the SUM, SSC and SPTK operators, a co-occurrence Word Space is acquired through the distributional analysis of the UkWaC corpus (Baroni et al., 2009 )."},
{"id_case":"709","class":"1","sentence1":"We used the implementation of the scikit-learn 2 module ( Pedregosa et al., 2011 )."},
{"id_case":"710","class":"0","sentence1":"The translation model was trained by GIZA++ ( Och and Ney, 2003 ), and the trigram was trained by the CMU-Cambridge Statistical Language Modeling Toolkit v2 ( Clarkson and Rosenfeld, 1997 )."},
{"id_case":"711","class":"0","sentence1":"This is a generalization of the operator Id in ( Kaplan and Kay, 1994 )."},
{"id_case":"712","class":"1","sentence1":"We used standard classifiers available in scikit-learn package ( Pedregosa et al., 2011 )."},
{"id_case":"714","class":"1","sentence1":"We used the implementation of the scikit-learn 2 module ( Pedregosa et al., 2011 )."},
{"id_case":"715","class":"1","sentence1":"For the phrase-based SMT system, we adopted the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"716","class":"0","sentence1":"4 Word alignments are created by aligning the data in both directions with GIZA++ 5 and symmetrizing the two trained alignments ( Och and Ney, 2003 )."},
{"id_case":"717","class":"3","sentence1":"The perplexity achieved by the 6- gram NN LM in the Spanish news-test08 development set was 116, versus 94 obtained with a standard 6-gram language model with interpolation and Kneser-Ney smoothing ( Kneser and Ney, 1995 )."},
{"id_case":"718","class":"1","sentence1":"We used standard classifiers available in scikit-learn package ( Pedregosa et al., 2011 )."},
{"id_case":"719","class":"0","sentence1":"Statistical machine translation is typically performed using phrase-based systems ( Koehn et al., 2007 )."},
{"id_case":"720","class":"0","sentence1":"WordNet ( Miller et al., 1990 ) is an on-line hierarchical lexical database which contains semantic information about English words."},
{"id_case":"721","class":"1","sentence1":"We use the scikit implementation of Random Forest ( Pedregosa et al., 2011 )."},
{"id_case":"724","class":"1","sentence1":"We lemmatise the head of each constituent with TreeTagger ( Schmid, 1994 )."},
{"id_case":"725","class":"1","sentence1":"Our text processing uses the Natural Language Toolkit (NLTK) ( Bird et al., 2009 )."},
{"id_case":"726","class":"0","sentence1":"They used the Web-based annotation tool brat ( Stenetorp et al., 2012 ) for the annotation ."},
{"id_case":"728","class":"2","sentence1":"The log-likelihood ratio test ( Lin and Hovy, 2000 ) compares the distribution of a word in the input with that in a large background corpus to identify topic words."},
{"id_case":"729","class":"0","sentence1":"Our system participated in SemEval-2013 Task 2: Sentiment Analysis in Twitter ( Wilson et al., 2013 )."},
{"id_case":"730","class":"0","sentence1":"The English text was tokenized using the word tokenize routine from NLTK ( Bird et al., 2009 )."},
{"id_case":"731","class":"0","sentence1":"The webpages were parsed using the Stanford CoreNLP software ( Manning et al., 2014 )."},
{"id_case":"732","class":"0","sentence1":"Statistical machine translation is typically performed using phrase-based systems ( Koehn et al., 2007 )."},
{"id_case":"733","class":"0","sentence1":"The English side was tokenized using the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"734","class":"1","sentence1":"We trained an English 5-gram language model using KenLM ( Heafield, 2011 )."},
{"id_case":"735","class":"0","sentence1":"All of the text data from Reddit was tokenized using the NLTK tokenizer ( Bird et al., 2009 )."},
{"id_case":"736","class":"1","sentence1":"Our machine translation systems are trained using Moses 3 ( Koehn et al., 2007 )."},
{"id_case":"737","class":"1","sentence1":"For the phrase-based SMT system, we adopted the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"738","class":"2","sentence1":"We build upon our previous approach for joint concept disambiguation and clustering ( Fahrni and Strube, 2012 )."},
{"id_case":"739","class":"1","sentence1":"We used Adam ( Kingma and Ba, 2014 ) with a learning rate of 0.0002 ()."},
{"id_case":"740","class":"1","sentence1":"For all experiments, we used the Moses SMT system ( Koehn et al., 2007 )."},
{"id_case":"741","class":"0","sentence1":"The SMT systems were built using the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"742","class":"1","sentence1":"For training the translation model and for decoding we used the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"743","class":"1","sentence1":"For training the translation model and for decoding we used the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"744","class":"1","sentence1":"For the phrase-based SMT system, we adopted the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"745","class":"1","sentence1":"We develop translation models using the phrase-based Moses ( Koehn et al., 2007 ) SMT system."},
{"id_case":"746","class":"1","sentence1":"We used TnT ( Brants, 2000 ), trained on the Negra training set."},
{"id_case":"747","class":"0","sentence1":"The webpages were parsed using the Stanford CoreNLP software ( Manning et al., 2014 )."},
{"id_case":"748","class":"1","sentence1":"We develop translation models using the phrase-based Moses ( Koehn et al., 2007 ) SMT system."},
{"id_case":"749","class":"2","sentence1":"POS Features: ( Pak and Paroubek, 2010 ) found that subjective texts often contain more adjectives or adverbs and less nouns than objective texts."},
{"id_case":"750","class":"2","sentence1":"The term frequency count is normalized with the inverse document frequency in the test collection ( Salton and Buckley, 1988 )."},
{"id_case":"751","class":"2","sentence1":"We assessed the statistical significance of differences in score with an approximate randomization test 8 ( Noreen, 1989 ), indicating a significant impact in bold font."},
{"id_case":"752","class":"0","sentence1":"A framework for human error analysis and error classification has been proposed in ( Vilar et al., 2006 ) and a detailed analysis of the obtained results has been carried out."},
{"id_case":"753","class":"3","sentence1":"For example, Chang et al. (2009) found that the probability of held-out documents is not always a good predictor of human judgments."},
{"id_case":"754","class":"1","sentence1":"On the Chinese side, we used the morphological analyzer described in ( Kruengkrai et al., 2009 ) trained on the training data of CTB tp to perform word segmentation and POS tagging and used the first"},
{"id_case":"755","class":"0","sentence1":"conducted using the Moses phrase-based decoder ( Koehn et al., 2007 )."},
{"id_case":"756","class":"0","sentence1":"conducted using the Moses phrase-based decoder ( Koehn et al., 2007 )."},
{"id_case":"757","class":"1","sentence1":"We conducted baseline experiments for phrasebased machine translation using the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"759","class":"0","sentence1":"Then the processed data was performed for tokenization, POS tagging, parsing, stemming and lemmatization using Stanford CoreNLP ( Manning et al., 2014 )."},
{"id_case":"760","class":"1","sentence1":"We applied bootstrap resampling ( Koehn, 2004 ) to measure statistical significance , p < 0.05, of our models compared to a baseline."},
{"id_case":"761","class":"0","sentence1":"This system uses the attentional encoder-decoder architecture described by (Bahdanau et al. (2015)) , building on work by (Sutskever et al. (2014)) ."},
{"id_case":"762","class":"0","sentence1":"The language model is a 5-gram KenLM ( Heafield, 2011 ) model, trained using lmplz, with modified Kneser-Ney smoothing and no pruning."},
{"id_case":"763","class":"1","sentence1":"Weighted Finite State Transducers (FSTs) used in our model are constructed with OpenFst ( Allauzen et al., 2007 )."},
{"id_case":"764","class":"1","sentence1":"Weighted Finite State Transducers (FSTs) used in our model are constructed with OpenFst ( Allauzen et al., 2007 )."},
{"id_case":"765","class":"0","sentence1":"Then the processed data was performed for tokenization, POS tagging, parsing, stemming and lemmatization using Stanford CoreNLP ( Manning et al., 2014 )."},
{"id_case":"766","class":"0","sentence1":"It is a modification of the model proposed by Mintz et al. (2009) ."},
{"id_case":"767","class":"1","sentence1":"We use Scikit-learn ( Pedregosa et al., 2011 ), the machine learning library for Python, for implementing the different approaches."},
{"id_case":"768","class":"1","sentence1":"We use the Universal POS Tagset (UPOS) of Petrov et al. (2012) ."},
{"id_case":"770","class":"1","sentence1":"For language modeling, we trained a separate 5-gram Kneser-Ney smoothed LM model on the target (i.e. English) side of the training bi-text using KenLM ( Heafield, 2011 )."},
{"id_case":"771","class":"0","sentence1":"with the training script of the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"772","class":"0","sentence1":"The word alignment was trained using GIZA++ ( Och and Ney, 2003 ) with the configuration grow-diag-final-and alignment symmetrization method."},
{"id_case":"773","class":"1","sentence1":"We use linear SVMs from LIBLINEAR and SVMs with RBF kernel from LIBSVM ( Chang and Lin, 2011 )."},
{"id_case":"774","class":"0","sentence1":"We specify the hierarchical aligner in terms of a deduction system ( Shieber et al., 1995 )."},
{"id_case":"775","class":"1","sentence1":"We use Stanford parser ( de Marneffe et al., 2006 ) to obtain parse trees and dependency relations."},
{"id_case":"776","class":"1","sentence1":"In this work, we use the Stanford neural dependency parser ( Chen and Manning, 2014 )."},
{"id_case":"778","class":"1","sentence1":"We use Stanford parser ( de Marneffe et al., 2006 ) to obtain parse trees and dependency relations."},
{"id_case":"779","class":"1","sentence1":"The learning algorithm used in our coreference engine is C4.5 ( Quinlan, 1993 )."},
{"id_case":"780","class":"1","sentence1":"We use Stanford parser ( de Marneffe et al., 2006 ) to obtain parse trees and dependency relations."},
{"id_case":"781","class":"0","sentence1":"Distributional semantics (see Cohen and Widdows (2009) for an overview is based on the observation that words that occur in similar contexts tend to be semantically related ( Harris, 1954 )."},
{"id_case":"782","class":"0","sentence1":"Their work is part of the state-of-the-art Arabic morphological tagger MADAMIRA ( Pasha et al., 2014 )."},
{"id_case":"783","class":"1","sentence1":"We use Stanford parser ( de Marneffe et al., 2006 ) to obtain parse trees and dependency relations."},
{"id_case":"784","class":"0","sentence1":"Phrasal follows the log-linear approach to phrase-based translation ( Och and Ney, 2004 ) in which the decision rule has the familiar linear form = arg max e w ?(e, f ) (1)"},
{"id_case":"785","class":"0","sentence1":"Training data are based on a concatenation of 18 POS-tagged English corpora 2 from the CHILDES database ( MacWhinney, 2000 )."},
{"id_case":"786","class":"1","sentence1":"In-domain SMT: we used the parallel corpus (Section 3) to train an en-pt phrase-based SMT system using the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"787","class":"0","sentence1":"5-gram language models of Turkish and English were trained using KenLM ( Heafield, 2011 )."},
{"id_case":"788","class":"1","sentence1":"We used the relation classification dataset of the SemEval 2010 task 8 ( Hendrickx et al., 2010 )."},
{"id_case":"789","class":"1","sentence1":"We then run word alignment with GIZA++ ( Och and Ney, 2003 ) in both directions, with the default parameters used in Moses."},
{"id_case":"790","class":"1","sentence1":"In-domain SMT: we used the parallel corpus (Section 3) to train an en-pt phrase-based SMT system using the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"791","class":"0","sentence1":"The significance tests were performed using the bootstrap resampling method ( Koehn, 2004 )."},
{"id_case":"792","class":"0","sentence1":"All experiments were carried out using the open-source SMT toolkit Moses ( Koehn et al., 2007 ), in its standard non-monotonic configuration ."},
{"id_case":"793","class":"0","sentence1":"These sentences have then be fed into an efficient HPSG parser (PET; ( Callmeier, 2000 )) with ERG loaded."},
{"id_case":"794","class":"0","sentence1":"The language model is a 5-gram KenLM ( Heafield, 2011 ) model, trained using lmplz, with modified Kneser-Ney smoothing and no pruning."},
{"id_case":"795","class":"3","sentence1":"Cohen et al. (2012)  present a spectral algorithm for L-PCFG estimation, but the transformation of the L-PCFG model and its spectral algorithm to R-HMMs is awkward and opaque."},
{"id_case":"796","class":"0","sentence1":"Distributional hypothesis theory ( Harris, 1954 ) indicates that words that occur in the same context tend to have similar meanings."},
{"id_case":"797","class":"0","sentence1":"Distributional hypothesis theory ( Harris, 1954 ) indicates that words that occur in the same context tend to have similar meanings."},
{"id_case":"799","class":"1","sentence1":"We use the Stanford dependency parser ( Marneffe et al., 2006 )."},
{"id_case":"800","class":"1","sentence1":"We also replicated the experiment of Holmqvist et al. (2012) on this dataset."},
{"id_case":"1301","class":"1","sentence1":"We develop translation models using the phrase-based Moses ( Koehn et al., 2007 ) SMT system."},
{"id_case":"1302","class":"0","sentence1":"POS tagging was performed with TreeTagger ( Schmid, 1994 )."},
{"id_case":"1303","class":"1","sentence1":"We used the MaltParser ( Nivre et al., 2007 ) for parsing experiments."},
{"id_case":"1304","class":"0","sentence1":"Classification uses the scikit-learn Python package ( Pedregosa et al., 2011 )."},
{"id_case":"1305","class":"1","sentence1":"We use the Stanford dependency parser ( Marneffe et al., 2006 )."},
{"id_case":"1306","class":"1","sentence1":"We used Adam ( Kingma and Ba, 2014 ) with a learning rate of 0.0002."},
{"id_case":"1307","class":"0","sentence1":"Word alignment is performed using GIZA++ ( Och and Ney, 2003 )."},
{"id_case":"1308","class":"0","sentence1":"The test set was tagged with the French TreeTagger ( Schmid, 1994 )."},
{"id_case":"1309","class":"1","sentence1":"For the phrase-based SMT system, we adopted the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"1310","class":"0","sentence1":"POS tagging was performed with TreeTagger ( Schmid, 1994 )."},
{"id_case":"1311","class":"1","sentence1":"For building the word alignment models we use MGIZA++ ( Gao and Vogel, 2008 )."},
{"id_case":"1313","class":"0","sentence1":"The Polish data is taken from the EUROPARL corpus ( Koehn, 2005 )."},
{"id_case":"1314","class":"0","sentence1":"The German-to-English corpus is Europarl v7 ( Koehn, 2005 )."},
{"id_case":"1315","class":"0","sentence1":"Distributional models of meaning follow the distributional hypothesis ( Harris, 1954 ), which states that two words that occur in similar contexts have similar meanings."},
{"id_case":"1316","class":"0","sentence1":"conducted using the Moses phrase-based decoder ( Koehn et al., 2007 )."},
{"id_case":"1317","class":"0","sentence1":"Europarl 2 ( Koehn, 2005 ): it is a corpus of parallel texts in 11 languages from the proceedings of the European Parliament ."},
{"id_case":"1318","class":"1","sentence1":"We conducted statistical significance tests for BLEU between our best domain-adapted system, the baseline and the three third-party systems using paired bootstrap resampling ( Koehn, 2004) with 1,000 "},
{"id_case":"1319","class":"1","sentence1":"The phrase-based SMT framework which we used is based on the log-linear model ( Och and Ney, 2002 ), where the decision rule is expressed as follow: argmax e P (e|f ) = argmax e M ? m=1 ? m h m (e, f "},
{"id_case":"1320","class":"0","sentence1":"The English side was tokenized using the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"1321","class":"1","sentence1":"We develop translation models using the phrase-based Moses ( Koehn et al., 2007 ) SMT system."},
{"id_case":"1322","class":"0","sentence1":"The baseline will be created by the Moses SMT toolkit ( Koehn et al., 2007 )."},
{"id_case":"1323","class":"0","sentence1":"The resulting matrix is weighted using pointwise mutual information ( Church and Hanks, 1990 )."},
{"id_case":"1324","class":"0","sentence1":"The SMT systems were built using the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"1325","class":"0","sentence1":"The morpho-syntactic tagging has been made with the Tree Tagger ( Schmid, 1994 )."},
{"id_case":"1326","class":"1","sentence1":"For the determination of POS tags we use the Stuttgart TreeTagger ( Schmid, 1994 )."},
{"id_case":"1327","class":"0","sentence1":"The Polish data is taken from the EUROPARL corpus ( Koehn, 2005 )."},
{"id_case":"1328","class":"0","sentence1":"All annotations were done using the brat rapid annotation tool ( Stenetorp et al., 2012 )."},
{"id_case":"1329","class":"0","sentence1":"The Spanish-English (S2E) training corpus was drawn from the Europarl collection ( Koehn, 2005 )."},
{"id_case":"1330","class":"1","sentence1":"We used TnT ( Brants, 2000 ), trained on the Negra training set."},
{"id_case":"1331","class":"1","sentence1":"For all experiments, we used the Moses SMT system ( Koehn et al., 2007 )."},
{"id_case":"1332","class":"0","sentence1":"Two baseNP data sets have been put forward by ( Ramshaw and Marcus, 1995 )."},
{"id_case":"1333","class":"1","sentence1":"Both of our systems were based on the Moses decoder ( Koehn et al., 2007 )."},
{"id_case":"1334","class":"1","sentence1":"For the phrase-based SMT system, we adopted the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"1335","class":"0","sentence1":"Corpus-based meaning representations rely on the distributional hypothesis, which assumes that words occurring in a similar set of contexts are also similar in meaning ( Harris, 1954 )."},
{"id_case":"1336","class":"0","sentence1":"Recently, Naim et al. (2014)  proposed an unsupervised learning algorithm for automatically aligning sentences in a document with corresponding video segments."},
{"id_case":"1337","class":"0","sentence1":"A distributional similarity model is constructed based on the distributional hypothesis ( Harris, 1954 ): words that occur in the same contexts tend to share similar meanings."},
{"id_case":"1338","class":"0","sentence1":"This paper describes the details of our system that participated in the subtask A of Semeval-2014 Task 9: Sentiment Analysis in Twitter ( Rosenthal et al., 2014 )."},
{"id_case":"1339","class":"1","sentence1":"We used the phrase-based model Moses ( Koehn et al., 2007 ) for the experiments with all the standard settings, including a lexicalized reordering model, and a 5-gram language model."},
{"id_case":"1340","class":"0","sentence1":"Combinatory Categorial grammar (CCG) is a linguistic formalism that represents both the syntax and semantics of language ( Steedman, 1996 )."},
{"id_case":"1341","class":"1","sentence1":"We used the Random Forests implementation of scikit-learn toolkit ( Pedregosa et al., 2011 ) with 50 estimators."},
{"id_case":"1342","class":"0","sentence1":"The proposed model extends the LDA framework of Blei et al. (2003) ."},
{"id_case":"1343","class":"1","sentence1":"We used the training section of the dataset from Gimpel et al. (2011) ."},
{"id_case":"1344","class":"0","sentence1":"5-gram language models of Turkish and English were trained using KenLM ( Heafield, 2011 )."},
{"id_case":"1345","class":"0","sentence1":"The corpora are first tokenized and lowercased using the Moses scripts, then lemmatized and tagged by part-of-speech (PoS) using the TreeTagger ( Schmid, 1994 )."},
{"id_case":"1346","class":"0","sentence1":"The statistical significance test is also carried out with paired bootstrap resampling method with p < 0.001 intervals ( Koehn, 2004 )."},
{"id_case":"1347","class":"0","sentence1":"Li et al. (2012) used the dictionary from Wiktionary, 1 a crowd-sourced dictionary."},
{"id_case":"1348","class":"0","sentence1":"The realisation ranking component is an SVM ranking model implemented with SVMrank, a Support Vector Machine-based learning tool ( Joachims, 2006 )."},
{"id_case":"1351","class":"0","sentence1":"The statistical significance test is also carried out with paired bootstrap resampling method with p < 0.001 intervals ( Koehn, 2004 )."},
{"id_case":"1352","class":"1","sentence1":"We use the NLTK toolkit ( Loper and Bird, 2002 ) to extract the numerical quantity from each sentence."},
{"id_case":"1357","class":"1","sentence1":"We mark the source tokens to which each target unk symbol is most aligned with the method of (Luong et al. (2015)) ."},
{"id_case":"1358","class":"1","sentence1":"We rely on the hybrid aligned lexical semantic resource proposed by ( Faralli et al. 2016 ) to perform WSD."},
{"id_case":"1359","class":"0","sentence1":"To help improve the information extraction tools, a corpus , called BioScope, has been annotated for speculation, negation and its linguistic scopes in biomedical texts ( Szarvas et al., 2008 )."},
{"id_case":"1362","class":"0","sentence1":"The module of coreference resolution included in the IXA pipeline is loosely based on the Stanford Multi Sieve Pass system ( Lee et al., 2013 )."},
{"id_case":"1363","class":"1","sentence1":"We run our experiments on Europarl ( Koehn, 2005 ), a multilingual parallel corpus extracted from the proceedings of the European Parliament."},
{"id_case":"1364","class":"1","sentence1":"We used the scikit-learn ( Pedregosa et al., 2011 ) implementation of these classifiers with their default parameter settings for our experiments."},
{"id_case":"1365","class":"1","sentence1":"We used the scikit-learn ( Pedregosa et al., 2011 ) implementation of these classifiers with their default parameter settings for our experiments."},
{"id_case":"1366","class":"0","sentence1":"Automatic Multi-Document Summarization (MDS) aims at selecting the relevant information from multiple documents on the same topic to produce a summary ( Mani, 2001 )."},
{"id_case":"1367","class":"1","sentence1":"Additionally, we train phrase-based machine translation models with our alignments using the popular Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"1368","class":"1","sentence1":"We used the scikit-learn ( Pedregosa et al., 2011 ) implementation of these classifiers with their default parameter settings for our experiments."},
{"id_case":"1369","class":"1","sentence1":"Additionally, we train phrase-based machine translation models with our alignments using the popular Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"1370","class":"1","sentence1":"We train classifiers for each of the above feature types and for the full feature set on the training set of each corpus using the default configuration of the naive Bayes implementation of Weka  (Hall et al., 2009 )."},
{"id_case":"1371","class":"0","sentence1":"In addition, the corpus was lemmatised using the TreeTagger lemmatizer ( Schmid, 1994 )."},
{"id_case":"1372","class":"0","sentence1":"They are based on Distributional Hypothesis which works under the assumption that similar words occur in similar contexts ( Harris, 1954 )."},
{"id_case":"1373","class":"1","sentence1":"We used the same test set used in (Li et al. (2004)) for our testing 5 ."},
{"id_case":"1374","class":"1","sentence1":"We develop translation models using the phrase-based Moses ( Koehn et al., 2007 ) SMT system."},
{"id_case":"1375","class":"1","sentence1":"We used non-local features based on Finkel et al. (2005) ."},
{"id_case":"1376","class":"1","sentence1":"For building the baseline SMT system, we used the open-source SMT toolkit Moses ( Koehn et al., 2007 ), in its standard setup."},
{"id_case":"1377","class":"1","sentence1":"For translation, we use Moses ( Koehn et al., 2007 ) with lexicalized reordering (step), and the proposed model with latent derivations (lader)."},
{"id_case":"1378","class":"1","sentence1":"To recognize explicit connectives, we construct a list of existing connectives labeled in the Penn Discourse Treebank ( Prasad et al., 2008a )."},
{"id_case":"1379","class":"1","sentence1":"To construct language models and measure perplexity , we use SRILM (Stolcke, 2002) with interpolated modified Kneser-Ney discounting ( Chen and Goodman, 1996 ) and with a fixed vocabulary ."},
{"id_case":"1380","class":"1","sentence1":"We use Gibbs sampling to estimate the distributions of N and M , integrating out the multinomial parameters ( Griffiths and Steyvers, 2004 )."},
{"id_case":"1381","class":"2","sentence1":"Arabizi is not a letter-based transliteration from the Arabic script as is, for example, the Buckwalter transliteration ( Buckwalter, 2004 )."},
{"id_case":"1382","class":"1","sentence1":"To construct language models and measure perplexity , we use SRILM (Stolcke, 2002) with interpolated modified Kneser-Ney discounting ( Chen and Goodman, 1996 ) and with a fixed vocabulary ."},
{"id_case":"1384","class":"1","sentence1":"We build upon our previous Markov Logic based approach for joint concept disambiguation and clustering ( Fahrni and Strube, 2012 )."},
{"id_case":"1385","class":"1","sentence1":"We use the feedforward neural probabilistic language model architecture of (Vaswani et al. (2013)) , as shown in Figure 4 ."},
{"id_case":"1386","class":"0","sentence1":"The SMT systems were built using the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"1387","class":"1","sentence1":"For building the baseline SMT system, we used the open-source SMT toolkit Moses ( Koehn et al., 2007 ), in its standard setup."},
{"id_case":"1388","class":"1","sentence1":"In-domain SMT: we used the parallel corpus (Section 3) to train an en-pt phrase-based SMT system using the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"1389","class":"0","sentence1":"Following resource collection and construction a SMT model for English\/Brazilian-Portuguese was trained using the Moses toolkit ( Koehn et al., 2007 ) using its baseline settings."},
{"id_case":"1390","class":"0","sentence1":"One semiautomatic approach to evaluation is ROUGE ( Lin and Hovy, 2003 ), which is primarily based on ngram co-occurrence between automatic and human summaries."},
{"id_case":"1391","class":"1","sentence1":"We use the Web 1T 5-gram corpus ( Brants and Franz, 2006 ) to compute the language model score for a sentence."},
{"id_case":"1392","class":"0","sentence1":"The SemEval-2015 Aspect Based Sentiment Analysis task is a continuation of SemEval-2014 Task 4 ( Pontiki et al., 2014 )."},
{"id_case":"1393","class":"0","sentence1":"The 2009 Bio NLP shared task ( Kim et al., 2009 ) aimed at extracting biological \"events\", where one of the event types was gene expression."},
{"id_case":"1394","class":"0","sentence1":"The constituent context model (CCM) for inducing constituency parses ( Klein and Manning, 2002 ) was the first unsupervised approach to surpass a right-branching baseline ."},
{"id_case":"1395","class":"3","sentence1":"The CCM is a generative model for the unsupervised induction of binary constituency parses over sequences of part-of-speech (POS) tags ( Klein and Manning, 2002 )."},
{"id_case":"1397","class":"1","sentence1":"We use the implementation provided by Tai et al. (2015) , changing only the dependency parses that are fed to their model."},
{"id_case":"1398","class":"0","sentence1":"Europarl ( Koehn, 2005 ) is a multilingual parallel corpus extracted from the proceedings of the European Parliament."},
{"id_case":"1399","class":"0","sentence1":"The Stanford dependency parser ( De Marneffe et al., 2006 ) is used for extracting features from the dependency parse trees."},
{"id_case":"1400","class":"0","sentence1":"The Stanford dependency parser ( De Marneffe et al., 2006 ) is used for extracting features from the dependency parse trees."},
{"id_case":"1901","class":"0","sentence1":"The major part of data comes from current and upcoming full releases of the Europarl data set ( Koehn, 2005 )."},
{"id_case":"1902","class":"0","sentence1":"This is an extension of UBK algorithm ( Agirre and Soroa, 2009 ), the first application of personalized PageRank to the WSD problem."},
{"id_case":"1903","class":"2","sentence1":"We compare the proposed model to our implementation of the IOBES-based model described in Collobert et al. (2011) , applied to MWE tagging ."},
{"id_case":"1904","class":"0","sentence1":"It is a phrase-based system built using the Moses toolkit ( Koehn et al., 2007 ) and trained\/tuned using only the preprocessed (tokenised, lower-cased) parallel data provided for the shared task."},
{"id_case":"1905","class":"1","sentence1":"In this section, we first discuss the hybrid tree model of (Lu et al. (2008)) , and introduce a novel extension."},
{"id_case":"1906","class":"0","sentence1":"The Europarl corpus ( Koehn, 2005 ) is built from the proceedings of the European Parliament."},
{"id_case":"1907","class":"0","sentence1":"The Europarl corpus ( Koehn, 2005 ) is built from the proceedings of the European Parliament."},
{"id_case":"1908","class":"1","sentence1":"We use Adam ( Kingma and Ba, 2015 ) for optimisation with initial learning rate of 0.001."},
{"id_case":"1909","class":"1","sentence1":"We use the open-source Moses toolkit ( Koehn et al., 2007 ) to build a standard phrase-based SMT system which extracts up to 8 words phrases in the Moses phrase table."},
{"id_case":"1910","class":"1","sentence1":"We used the mkcls tool in GIZA++ ( Och and Ney, 2003 ) to learn the word classes."},
{"id_case":"1911","class":"1","sentence1":"We then use the phrase extraction utility in the Moses statistical machine translation system ( Koehn et al., 2007 ) to extract a phrase table which operates over characters ."},
{"id_case":"1912","class":"0","sentence1":"Phrase pairs are extracted from IBM4 alignments obtained with GIZA++( Och and Ney, 2003 )."},
{"id_case":"1914","class":"1","sentence1":"Specifically, we build off the Bayesian block HMMs used by (Ritter et al. (2010)) for modeling Twitter conversations, which will be our primary baseline."},
{"id_case":"1915","class":"1","sentence1":"Like (Cho & Chai (2000)) , our analysis also provides the same explanation for various scrambled sentences such as the Double Accusative Construction (DAC) in Korean."},
{"id_case":"1916","class":"0","sentence1":"In PB-SMT, the posterior probability P(e I 1 |f J 1 ) is directly modelled as a (log-linear) combination of features ( Och and Ney, 2002 ), that usually comprise M translational features, and the lang"},
{"id_case":"1917","class":"1","sentence1":"We propose a relatively simple and nuanced unsupervised model inspired by the monolingual weighted matrix factorization (WMF) model proposed in ( Guo and Diab, 2012 ), which we extend to the cross-lin"},
{"id_case":"1918","class":"1","sentence1":"Our work is also related to ( Bunescu and Mooney, 2005 ), where the similarity between the words on the path connecting two entities in the dependency graph is used to devise a Kernel function."},
{"id_case":"1919","class":"1","sentence1":"Motivated by previous work, we include a frequency count of 17 discourse markers which were found to be the most common across the ARGUE corpus ( Abbott et al., 2011 )."},
{"id_case":"1920","class":"0","sentence1":"The highest performance levels were achieved using a sequential minimal optimization algorithm for training a support vector classifier using polynomial kernels ( Platt, 1998 )."},
{"id_case":"1922","class":"0","sentence1":"In principle, classifiers trained on PDTB data can be applied directly to label connectives over the English side of the Europarl corpus ( Koehn, 2005 ) used for training and testing SMT."},
{"id_case":"1923","class":"0","sentence1":"The Shared Task on Language Identification in Code- Switched Data held in 2014 ( Solorio et al., 2014 ) is another related competition, where the focus was on tweets in which users were mixing two or "},
{"id_case":"1924","class":"0","sentence1":"The corpora are tokenised and truecased using scripts from the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"1925","class":"0","sentence1":"The HMM classifier is similar to the one described in ( Bikel et al., 1999 )."},
{"id_case":"1926","class":"1","sentence1":"In the next section we briefly review modeling of transition probabilities in a conventional HMM alignment model ( Vogel et al., 1996, Och and Ney, 2000a )."},
{"id_case":"1927","class":"0","sentence1":"This result is statistically significant at p = 0.05 according to Bootstrap Resampling Test ( Koehn, 2004 )."},
{"id_case":"1928","class":"0","sentence1":"Starting with TextRank ( Mihalcea and Tarau, 2004 ), graph-based ranking methods are becoming the most widely used unsupervised approach for keyphrase extraction."},
{"id_case":"1929","class":"1","sentence1":"Because of our experience with the Weka package ( Hall et al., 2009 ) we chose this tool for implementation ."},
{"id_case":"1930","class":"3","sentence1":"Dropout ( Srivastava et al., 2014 ) is implemented with a dropout rate of 0.2 to prevent the model from overfitting."},
{"id_case":"1931","class":"1","sentence1":"In-domain SMT: we used the parallel corpus (Section 3) to train an en-pt phrase-based SMT system using the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"1932","class":"1","sentence1":"As mentioned in Section 3, we obtained dependencies from the output of the Stanford parser ( de Marneffe and Manning, 2008 )."},
{"id_case":"1933","class":"1","sentence1":"We tokenize and truecase all of the corpora using code released with Moses ( Koehn et al., 2007 )."},
{"id_case":"1935","class":"1","sentence1":"To obtain these we use the Stanford dependency parser ( de Marneffe et al., 2006 ) and the forced alignment from Section 3.9."},
{"id_case":"1936","class":"0","sentence1":"The NMT models are trained using Adam optimizer ( Kingma and Ba, 2014 ) with an initial learning rate of 0.0001."},
{"id_case":"1937","class":"3","sentence1":"The improved alignments gave a gain of Table 8 : Hierarchical lexicalized reordering model ( Galley and Manning, 2008 )."},
{"id_case":"1938","class":"0","sentence1":"The statistical significance tests using 95% confidence interval are measured with paired bootstrap resampling ( Koehn, 2004 )."},
{"id_case":"1939","class":"1","sentence1":"To obtain these we use the Stanford dependency parser ( de Marneffe et al., 2006 ) and the forced alignment from Section 3.9."},
{"id_case":"1940","class":"1","sentence1":"For all experiments, we used the Moses SMT system ( Koehn et al., 2007 )."},
{"id_case":"1941","class":"1","sentence1":"We use the AdaGrad algorithm ( Duchi et al., 2011 ) to optimize the conditional, marginal log-likelihood of the data."},
{"id_case":"1942","class":"2","sentence1":"The other is from (Jeffrey Pennington et al. (2014)) , the dimension of word embedding is 100."},
{"id_case":"1943","class":"1","sentence1":"We use the AdaGrad optimizer ( Duchi et al., 2011)  with initial learning rate set to 0.1."},
{"id_case":"1944","class":"1","sentence1":"We experiment with the phrase-based statistical machine translation toolkit Moses ( Koehn et al., 2007 ) in order to train a Japanese -English system and to show the influence of the expanded parallel"},
{"id_case":"1945","class":"1","sentence1":"In-domain SMT: we used the parallel corpus (Section 3) to train an en-pt phrase-based SMT system using the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"1946","class":"1","sentence1":"We follow the definition in Cohen et al. (2012) of L-PCFGs."},
{"id_case":"1947","class":"0","sentence1":"The language model is a 5-gram KenLM ( Heafield, 2011 ) model, trained using lmplz, with modified Kneser-Ney smoothing and no pruning."},
{"id_case":"1949","class":"1","sentence1":"As mentioned in Section 3, we obtained dependencies from the output of the Stanford parser ( de Marneffe and Manning, 2008 )."},
{"id_case":"1950","class":"2","sentence1":"Also, we evaluate on the RTE part of the SICK dataset ( Marelli et al., 2014 ) and show that our approach leads to improvements."},
{"id_case":"1951","class":"0","sentence1":"Our model has a \" Siamese \" structure ( Bromley et al., 1993 ) with two subnetworks each processing a sentence in parallel."},
{"id_case":"1952","class":"1","sentence1":"For building the word alignment models we use MGIZA++ ( Gao and Vogel, 2008 )."},
{"id_case":"1953","class":"0","sentence1":"The dialogue act labelling of the corpus follows the DATE tagging scheme ( Walker et al., 2001 )."},
{"id_case":"1954","class":"1","sentence1":"We also obtain the dependency parse of the sentences using the Stanford parser ( De Marneffe et al., 2006 )."},
{"id_case":"1955","class":"0","sentence1":"The resulting matrix is weighted using pointwise mutual information ( Church and Hanks, 1990 )."},
{"id_case":"1956","class":"1","sentence1":"We extract structured facts using two methods: Clausie (Del Corro and Gemulla, 2013 ) and Sedona ( detailed later in Sec 4); also see Fig 1."},
{"id_case":"1957","class":"0","sentence1":"Dropout ( Srivastava et al., 2014 ) is implemented with a dropout rate of 0.2 to prevent the model from overfitting."},
{"id_case":"1958","class":"0","sentence1":"The population distribution was estimated by the bootstrap method ( Cohen, 1995 )."},
{"id_case":"1959","class":"0","sentence1":"The statistical significance test is also carried out with paired bootstrap resampling method with p < 0.001 intervals ( Koehn, 2004 )."},
{"id_case":"1960","class":"0","sentence1":"The bootstrap sampling method provides a way for artificially establishing a sampling distribution for a statistic, when the distribution is not known ( Cohen, 1995 )."},
{"id_case":"1961","class":"1","sentence1":"For medical we use the biomedical data from EMEA ( Tiedemann, 2009 )."},
{"id_case":"1962","class":"0","sentence1":"The MT experiments were carried out using the standard log-linear phrase-based SMT toolkit MOSES ( Koehn et al., 2007 )."},
{"id_case":"1963","class":"2","sentence1":"For the theory of Cho & Chai (2000) to be complete, we have proposed a new type of marker and the Adjunct LP Constraint, in conjunction with their Argument LP Constraint."},
{"id_case":"1964","class":"3","sentence1":"3 With these trees fixed, the partial derivatives with respect to parameters are computed via the backpropagation through structures algorithm ( Goller and Kuchler, 1996 )."},
{"id_case":"1965","class":"0","sentence1":"Many researchers have considered generating paraphrases by mining the Web guided by the distributional hypothesis, which states that words occurring in similar contexts tend to have similar meanings ( Harris, 1954 )."},
{"id_case":"1966","class":"1","sentence1":"We used Weka ( Hall et al., 2009 ) for all our classification experiments."},
{"id_case":"1967","class":"1","sentence1":"Bilingual Corpora The corpus used in the following experiments is the Basic Travel Expression Corpus ( Takezawa et al., 2002 )."},
{"id_case":"1968","class":"1","sentence1":"First, we used the Moses toolkit ( Koehn et al., 2007 ) for statistical machine translation."},
{"id_case":"1969","class":"2","sentence1":"Another parallel corpus is the JRC-Acquis Multilingual Parallel Corpus ( Steinberger et al., 2006 )."},
{"id_case":"1970","class":"0","sentence1":"The parallel corpus is wordaligned using GIZA++ ( Och and Ney, 2003 )."},
{"id_case":"1971","class":"0","sentence1":"It is similar to ( Och and Ney, 2000a )."},
{"id_case":"1972","class":"1","sentence1":"We thus cast MSC as a semantic sentence classification task in a CNN architecture, adopting the one-layer CNN model of Kim (2014), a variant of Collobert et al. (2011) ."},
{"id_case":"1973","class":"0","sentence1":"All these features are inherited from Moses ( Koehn et al., 2007 )."},
{"id_case":"1974","class":"1","sentence1":"For this purpose we use the Europarl corpus ( Koehn, 2005 )."},
{"id_case":"1975","class":"0","sentence1":"The texts were first automatically segmented and tokenized 10 and then they were part-of-speech tagged by TnT tagger ( Brants, 2000 ), which was trained on the respective CoNLL training data (the file"},
{"id_case":"1976","class":"2","sentence1":"Secondly, Holmqvist et al. (2012) reordered source words based on word alignment, whereas we suggest reordering source chunks."},
{"id_case":"1977","class":"1","sentence1":"For language modeling, we use the English Gigaword corpus with 5-gram LM implemented with the KenLM toolkit ( Heafield, 2011 )."},
{"id_case":"1978","class":"1","sentence1":"For our LDA implementations, we use MALLET ( McCallum, 2002 )."},
{"id_case":"1979","class":"1","sentence1":"We use the standard Stanford-style set of dependency labels ( de Marneffe et al., 2006 )."},
{"id_case":"1980","class":"1","sentence1":"Next we evaluate how well the complexity measures proposed in  (Raghavan et al. 2007 ) correlate with improvement in performance and improvement in learning rate."},
{"id_case":"1981","class":"0","sentence1":"Syntax in EPEC is annotated following the dependency based formalism used in the Prague Dependency Treebank, which was also used in the German NEGRA corpus ( Skut et al., 1997 )."},
{"id_case":"1982","class":"0","sentence1":"In addition, the corpus was lemmatised using the TreeTagger lemmatiser ( Schmid, 1994 )."},
{"id_case":"1983","class":"1","sentence1":"We used the scikit-learn ( Pedregosa et al., 2011 ) implementation of SVRs and the SKLL toolkit."},
{"id_case":"1984","class":"1","sentence1":"We used the scikit-learn toolkit to train our classifiers ( Pedregosa et al., 2011 )."},
{"id_case":"1985","class":"0","sentence1":"The training set is used to train the phrase-based translation model and language model for Moses ( Koehn et al., 2007 )."},
{"id_case":"1986","class":"0","sentence1":"These features were obtained using the Stanford parser 2 ( Marneffe et al., 2006 )."},
{"id_case":"1987","class":"1","sentence1":"We built a 5-gram language model on the English side of QCA-train using KenLM ( Heafield, 2011 )."},
{"id_case":"1988","class":"2","sentence1":"In the other side, the French corpus is part-of-speech (POS) tagged by using treetagger tool ( Schmid, 1994 ) for annotating text with part-of-speech and lemma information."},
{"id_case":"1989","class":"0","sentence1":"The COMLEX syntax dictionary ( Grishman et al., 1994 )."},
{"id_case":"1990","class":"1","sentence1":"We lemmatise the head of each constituent with TreeTagger ( Schmid, 1994 )."},
{"id_case":"1991","class":"0","sentence1":"with the KenLM toolkit ( Heafield, 2011 )."},
{"id_case":"1992","class":"1","sentence1":"We briefly review the HMM based word alignment models ( Vogel, 1996, Och and Ney, 2000 )."},
{"id_case":"1993","class":"1","sentence1":"We use the Stanford parser with Stanford dependencies ( Marneffe et al., 2006 )."},
{"id_case":"1995","class":"1","sentence1":"We use the Stanford parser with Stanford dependencies ( de Marneffe et al., 2006 )."},
{"id_case":"1996","class":"0","sentence1":"All the summaries are evaluated using ROUGE ( Lin, 2004 )."},
{"id_case":"1997","class":"0","sentence1":"The reported confidence intervals were estimated using bootstrap resampling ( Koehn, 2004 )."},
{"id_case":"1999","class":"1","sentence1":"We implemented CharWNN using the Theano library ( Bergstra et al., 2010 )."},
{"id_case":"2000","class":"1","sentence1":"For the linear logistic regression implementation we used scikit-learn ( Pedregosa et al., 2011 )."},
{"id_case":"2501","class":"1","sentence1":"The tagger we use is TnT ( Brants, 2000 ) , a hidden Markov trigram tagger, which was trained on the Spoken Dutch Corpus (CGN), Internal Release 6."},
{"id_case":"2502","class":"1","sentence1":"We have used the implementation described in ( Schapire and Singer, 1999 ) with decision trees of depth fixed to 3."},
{"id_case":"2503","class":"1","sentence1":"We built a 5-gram language model on the English side of QCA-train using KenLM ( Heafield, 2011 )."},
{"id_case":"2504","class":"0","sentence1":"DELPH-IN Minimal Recursion Semantics (DM) As part of the full HPSG sign, the ERG also makes available a logical-form representation of propositional semantics in the format of Minimal Recursion Semant"},
{"id_case":"2505","class":"2","sentence1":"Like the CoNLL-2006 shared task, the 2007 shared task focuses on dependency parsing and aims at comparing state-of-the-art machine learning algorithms applied to this task ( Nivre et al., 2007 )."},
{"id_case":"2506","class":"1","sentence1":"Like ( Cho & Chai 2000 ), our analysis also provides the same explanation for various scrambled sentences such as the Double Accusative Construction (DAC) in Korean."},
{"id_case":"2507","class":"0","sentence1":"The phrase table is extracted from a bilingual text aligned on the word level, using e.g. GIZA ++ ( Och and Ney, 2003 )."},
{"id_case":"2508","class":"0","sentence1":"Google Web 1T ( Brants and Franz, 2006 ) has been used to calculate term idf, which is used as a measure of the importance of the terms."},
{"id_case":"2509","class":"1","sentence1":"We first use a dependency parser ( de Marneffe et al., 2006 ) to parse each sentence and extract the set of dependency relations associated with the sentence ."},
{"id_case":"2510","class":"1","sentence1":"We first use a dependency parser ( de Marneffe et al., 2006 ) to parse each sentence and extract the set of dependency relations associated with the sentence ."},
{"id_case":"2511","class":"2","sentence1":"These analyses provide an alternative but theoretically more reasonable explanation to the findings of ( Liang et al. 2006 ) : while they blame \" unreasonable \" gold derivations for the failure of stand"},
{"id_case":"2512","class":"1","sentence1":"We experiment with the phrase-based statistical machine translation toolkit Moses ( Koehn et al., 2007 ) in order to train a Japanese -English system and to show the influence of the expanded parallel"},
{"id_case":"2513","class":"1","sentence1":"For our corpus, we randomly selected documents from the Washington section of the New York Times corpus ( Sandhaus, 2008 ) from the year 2007."},
{"id_case":"2514","class":"0","sentence1":"They used the Web-based annotation tool brat ( Stenetorp et al., 2012 ) for the annotation ."},
{"id_case":"2515","class":"0","sentence1":"All corpora were taken from the CHILDES database ( MacWhinney, 2000 )."},
{"id_case":"2516","class":"0","sentence1":"The system generated tweets were evaluated using ROUGE measures ( Lin, 2004 )."},
{"id_case":"2517","class":"0","sentence1":"The TOEFL synonym selection task is to select the semantically closest word to a target from a list of four candidates ( Landauer and Dumais, 1997 )."},
{"id_case":"2518","class":"1","sentence1":"We calculate our features using the KenLM toolkit ( Heafield, 2011 )."},
{"id_case":"2519","class":"1","sentence1":"We use the scikit implementation of Random Forest ( Pedregosa et al., 2011 )."},
{"id_case":"2521","class":"1","sentence1":"For this purpose we use the Europarl corpus ( Koehn, 2005 )."},
{"id_case":"2522","class":"0","sentence1":"The statistical significance tests using 95% confidence interval are measured with paired bootstrap resampling ( Koehn, 2004 )."},
{"id_case":"2523","class":"1","sentence1":"We then run word alignment with GIZA++ ( Och and Ney, 2003 ) in both directions, with the default parameters used in Moses."},
{"id_case":"2524","class":"1","sentence1":"We built a 5-gram language model on the English side of QCA-train using KenLM ( Heafield, 2011 )."},
{"id_case":"2525","class":"1","sentence1":"We used GIZA++ ( Och and Ney, 2003 ) to align the words in the corpus."},
{"id_case":"2526","class":"0","sentence1":"The phrase tables were generated by means of symmetrised word alignments obtained with GIZA++ ( Och and Ney, 2003 )."},
{"id_case":"2527","class":"0","sentence1":"Zhu et al. (2013) applied Kalman filter model to learn and estimate user intentions in their human-computer interactive word segmentation framework."},
{"id_case":"2528","class":"0","sentence1":"( Koo et al. 2008 )  have proposed to use word clusters as features to improve graph-based statistical dependency parsing for English and Czech."},
{"id_case":"2529","class":"0","sentence1":"More recently, ( Pasha et al., 2014 ) created MADAMIRA, a system for morphological analysis and disambiguation of Arabic, this system can be used to improve the accuracy of spelling checking system es"},
{"id_case":"2530","class":"1","sentence1":"We used the Moses toolkit ( Koehn et al., 2007 ) with its default settings."},
{"id_case":"2531","class":"1","sentence1":"@BULLET Naive Bayes(NB): We use Binomial variant with Laplace smoothing parameter = 1 ( Pedregosa et al., 2011 )."},
{"id_case":"2532","class":"1","sentence1":"We use AdaGrad ( Duchi et al., 2011)  with the initial learning rate set to ? = 0.5."},
{"id_case":"2533","class":"0","sentence1":"The MT experiments were carried out using the standard log-linear phrase-based SMT toolkit MOSES ( Koehn et al., 2007 )."},
{"id_case":"2534","class":"1","sentence1":"We use the AdaGrad method ( Duchi et al., 2011 ) to automatically update the learning rate for each parameter."},
{"id_case":"2535","class":"0","sentence1":"Among the existing sense-tagged corpora, the SEMCOR corpus ( Miller et al., 1994 ) is one of the most widely used."},
{"id_case":"2536","class":"1","sentence1":"We use the Liblinear Support Vector Machine (SVM) ( Chang and Lin, 2011 ) classifier for training and run 5-fold cross-validation for evaluation."},
{"id_case":"2537","class":"1","sentence1":"We build a state of the art phrase-based SMT system using Moses ( Koehn et al., 2007 )."},
{"id_case":"2538","class":"1","sentence1":"We built a source-to-target PB-SMT model from the bilingual domain corpus using the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"2539","class":"0","sentence1":"The dictionaries are automatically generated via word alignment using GIZA++ ( Och and Ney, 2000 ) on parallel corpora."},
{"id_case":"2540","class":"0","sentence1":"There are two main approaches to processing non-standard data: normalization and domain adaptation ( Eisenstein, 2013 )."},
{"id_case":"2541","class":"1","sentence1":"We conducted baseline experiments for phrasebased machine translation using the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"2542","class":"0","sentence1":"The task of identifying mentions to medical concepts in free text and mapping these mentions to a knowledge base was recently proposed in ShARe\/CLEF eHealth Evaluation Lab 2013 ( Suominen et al., 2013 )"},
{"id_case":"2543","class":"1","sentence1":"We built a source-to-target PB-SMT model from the bilingual domain corpus using the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"2545","class":"1","sentence1":"We also use MADA+TOKAN ( Habash et al., 2009 ) to preprocess and tokenize the Arabic side of the corpus."},
{"id_case":"2546","class":"2","sentence1":"Both training and testing data consist of PubMed abstracts extracted from the GENIA corpus ( Kim et al., 2008 )."},
{"id_case":"2547","class":"1","sentence1":"We conducted baseline experiments for phrasebased machine translation using the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"2548","class":"1","sentence1":"To extract our part-of-speech (POS) features, we first tag the transcripts using the NLTK POS tagger ( Bird et al., 2009 )."},
{"id_case":"2549","class":"0","sentence1":"The corpora are tokenised and truecased using scripts from the Moses toolkit ( Koehn et al., 2007 )."},
{"id_case":"2550","class":"1","sentence1":"We trained a model using Moses toolkit ( Koehn et al., 2007 ) on the training data as our baseline system."},
{"id_case":"2551","class":"1","sentence1":"For seed and test paradigms we used verbal inflectional paradigms from the CELEX morphological database ( Baayen et al., 1995 )."},
{"id_case":"2552","class":"1","sentence1":"Then we did word alignment using GIZA++ ( Och and Ney, 2003 ) with the default grow-diagfinal-and alignment symmetrization method."},
{"id_case":"2553","class":"0","sentence1":"( Pang et al. 2002) have reported the effectiveness of applying machine learning techniques to the p\/n classification."}
]
}
]
